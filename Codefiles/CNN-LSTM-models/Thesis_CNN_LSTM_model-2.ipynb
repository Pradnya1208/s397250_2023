{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0FhNLZl6qDn",
        "outputId": "8bcf4dd1-eb4f-4c63-fd18-9368792cffe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tslearn in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.2.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from tslearn) (0.56.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.3.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->tslearn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->tslearn) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tslearn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tslearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_7UkxQEYCao"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tslearn.datasets import UCR_UEA_datasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler, TimeSeriesScalerMinMax\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2tQgmpwYzk0"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use('fivethirtyeight')\n",
        "from sklearn import linear_model, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier as XGBC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier,VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve,recall_score, confusion_matrix, precision_score, fbeta_score, f1_score, accuracy_score, classification_report\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n"
      ],
      "metadata": {
        "id": "Fyg4GKleDdQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU4wOxbpYzin"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "all_sheets_dict = pd.read_excel(\"/content/drive/MyDrive/Scattering_data.xlsx\", sheet_name=None,  header=None, index_col=None)\n",
        "day1_df = all_sheets_dict['Day1']\n",
        "day2_df = all_sheets_dict['Day2']\n",
        "day3_df = all_sheets_dict['Day3']\n",
        "day4_df = all_sheets_dict['Day4']\n",
        "day5_df = all_sheets_dict['Day5']\n",
        "day6_df = all_sheets_dict['Day6']\n",
        "day7_df = all_sheets_dict['Day7']\n",
        "day8_df = all_sheets_dict['Day8']\n",
        "day9_df = all_sheets_dict['Day9']\n",
        "day10_df = all_sheets_dict['Day10']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJgE32Q06OrI",
        "outputId": "28b62bba-a4b3-48db-8556-63c1ecba57b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-0bf0b8abddd6>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_2 = day1_df.append(day2_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_3 = days_2.append(day3_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_4 = days_3.append(day4_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_5 = days_4.append(day5_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_6 = days_5.append(day6_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_7 = days_6.append(day7_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_8 = days_7.append(day8_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_9 = days_8.append(day9_df, ignore_index=True)\n",
            "<ipython-input-5-0bf0b8abddd6>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  days_10 = days_9.append(day10_df, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "days_2 = day1_df.append(day2_df, ignore_index=True)\n",
        "days_3 = days_2.append(day3_df, ignore_index=True)\n",
        "\n",
        "days_4 = days_3.append(day4_df, ignore_index=True)\n",
        "days_5 = days_4.append(day5_df, ignore_index=True)\n",
        "\n",
        "days_6 = days_5.append(day6_df, ignore_index=True)\n",
        "days_7 = days_6.append(day7_df, ignore_index=True)\n",
        "\n",
        "days_8 = days_7.append(day8_df, ignore_index=True)\n",
        "days_9 = days_8.append(day9_df, ignore_index=True)\n",
        "\n",
        "days_10 = days_9.append(day10_df, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqq-zYct-Zps",
        "outputId": "544645ec-8fdd-49aa-ae09-1623d87b6eaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
              "            ...\n",
              "            503, 504, 505, 506, 507, 508, 509, 510, 511, 512],\n",
              "           dtype='int64', length=513)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "days_10.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y56HSl9faUzZ"
      },
      "outputs": [],
      "source": [
        "x= days_10.iloc[:,:-1].copy()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "#class_labels = ['class0', 'class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'class10', 'class11']\n",
        "#label_encoder = LabelEncoder()\n",
        "#label_encoder.fit(class_labels)\n",
        "\n",
        "# Convert class labels to numeric levels using transform method of LabelEncoder\n",
        "#labels = label_encoder.transform(data_4days['Class'].values) #change\n",
        "\n",
        "y = days_10.iloc[:,-1:].values\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(x)\n",
        "x = sc.transform(x)\n",
        "x = pd.DataFrame(x)\n",
        "x.columns = days_10.iloc[:,:-1].columns.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuIjbJop_W-Y",
        "outputId": "728c10d4-7a53-459d-b59a-044a1d6f6c70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16800, 512), (16800, 1), (7200, 512), (7200, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Re6JV__W8W"
      },
      "outputs": [],
      "source": [
        "#normalize the data\n",
        "X_train = TimeSeriesScalerMinMax().fit_transform(X_train)\n",
        "X_test = TimeSeriesScalerMinMax().fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0O0aka5_W60"
      },
      "outputs": [],
      "source": [
        "# Convert the data to torch tensors\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "y_test = torch.from_numpy(y_test).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiUXORPe_W5B",
        "outputId": "c9bb1ddd-8872-4313-caae-8fa6a8278661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in y_train: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "Unique values in y_test: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
          ]
        }
      ],
      "source": [
        "print(\"Unique values in y_train:\", torch.unique(y_train))\n",
        "print(\"Unique values in y_test:\", torch.unique(y_test))\n",
        "\n",
        "#start class from 0\n",
        "#y_train = y_train - 1\n",
        "#y_test = y_test - 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4TBU28q_W3Y"
      },
      "outputs": [],
      "source": [
        "#Datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "#Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASjDvbajOQQh",
        "outputId": "2569d24d-61a0-4501-c3b3-7d88eeb16095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in y_train: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "Unique values in y_test: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
          ]
        }
      ],
      "source": [
        "print(\"Unique values in y_train:\", torch.unique(y_train))\n",
        "print(\"Unique values in y_test:\", torch.unique(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEfMC-13OROQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM3MsjOL_W0B",
        "outputId": "392b6584-860d-46dd-f393-f221237ad41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L66xPfsj_Wxt"
      },
      "outputs": [],
      "source": [
        "# model 1: CNN + LSTM\n",
        "# model 2: LSTM + CNN\n",
        "# model 3: CNN LSTM parallel\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #cnn takes input of shape (batch_size, channels, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        out = self.cnn(x)\n",
        "        # lstm takes input of shape (batch_size, seq_len, input_size)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out, _ = self.lstm(out)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdb3joIc_WtU"
      },
      "outputs": [],
      "source": [
        "class LSTM_CNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM_CNN, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=hidden_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            #flatten\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.cnn(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P8Fe9cr_h05"
      },
      "outputs": [],
      "source": [
        "# model 3: CNN LSTM parallel\n",
        "class ParallelCNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(ParallelCNNLSTMModel, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(out_features=128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc_lstm = nn.Linear(hidden_size, 128)\n",
        "        self.fc = nn.Linear(128*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #cnn takes input of shape (batch_size, channels, seq_len)\n",
        "        x_cnn = x.permute(0, 2, 1)\n",
        "        out_cnn = self.cnn(x_cnn)\n",
        "        # lstm takes input of shape (batch_size, seq_len, input_size)\n",
        "        out_lstm, _ = self.lstm(x)\n",
        "        out_lstm = self.fc_lstm(out_lstm[:, -1, :])\n",
        "        out = torch.cat([out_cnn, out_lstm], dim=1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n873i4J7_hyh",
        "outputId": "d1147706-59d0-402a-9ef4-08a5ed148cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "input_size = X_train.shape[-1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "cnn_lstm = CNN_LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "lstm_cnn = LSTM_CNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "cnn_lstm_parallel = ParallelCNNLSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WhydIqDxrgtr",
        "outputId": "69d1c76e-ddee-4c9d-db78-022343977093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [58/250], Step [200/263], Loss: 0.3502, Accuracy: 85.62%\n",
            "Epoch [58/250], Step [210/263], Loss: 0.3156, Accuracy: 85.78%\n",
            "Epoch [58/250], Step [220/263], Loss: 0.3018, Accuracy: 86.56%\n",
            "Epoch [58/250], Step [230/263], Loss: 0.2766, Accuracy: 89.06%\n",
            "Epoch [58/250], Step [240/263], Loss: 0.3040, Accuracy: 88.12%\n",
            "Epoch [58/250], Step [250/263], Loss: 0.3032, Accuracy: 88.28%\n",
            "Epoch [58/250], Step [260/263], Loss: 0.3069, Accuracy: 87.66%\n",
            "Epoch [59/250], Step [10/263], Loss: 0.2656, Accuracy: 89.84%\n",
            "Epoch [59/250], Step [20/263], Loss: 0.3049, Accuracy: 87.81%\n",
            "Epoch [59/250], Step [30/263], Loss: 0.2695, Accuracy: 89.22%\n",
            "Epoch [59/250], Step [40/263], Loss: 0.2786, Accuracy: 87.66%\n",
            "Epoch [59/250], Step [50/263], Loss: 0.2887, Accuracy: 88.28%\n",
            "Epoch [59/250], Step [60/263], Loss: 0.3039, Accuracy: 87.81%\n",
            "Epoch [59/250], Step [70/263], Loss: 0.3123, Accuracy: 86.88%\n",
            "Epoch [59/250], Step [80/263], Loss: 0.3045, Accuracy: 87.50%\n",
            "Epoch [59/250], Step [90/263], Loss: 0.2883, Accuracy: 89.38%\n",
            "Epoch [59/250], Step [100/263], Loss: 0.3122, Accuracy: 87.03%\n",
            "Epoch [59/250], Step [110/263], Loss: 0.3178, Accuracy: 87.81%\n",
            "Epoch [59/250], Step [120/263], Loss: 0.2628, Accuracy: 90.31%\n",
            "Epoch [59/250], Step [130/263], Loss: 0.2772, Accuracy: 89.38%\n",
            "Epoch [59/250], Step [140/263], Loss: 0.2929, Accuracy: 88.75%\n",
            "Epoch [59/250], Step [150/263], Loss: 0.2827, Accuracy: 88.59%\n",
            "Epoch [59/250], Step [160/263], Loss: 0.2841, Accuracy: 88.44%\n",
            "Epoch [59/250], Step [170/263], Loss: 0.3201, Accuracy: 87.19%\n",
            "Epoch [59/250], Step [180/263], Loss: 0.2672, Accuracy: 88.44%\n",
            "Epoch [59/250], Step [190/263], Loss: 0.2961, Accuracy: 88.12%\n",
            "Epoch [59/250], Step [200/263], Loss: 0.3032, Accuracy: 87.50%\n",
            "Epoch [59/250], Step [210/263], Loss: 0.2782, Accuracy: 88.59%\n",
            "Epoch [59/250], Step [220/263], Loss: 0.2663, Accuracy: 90.94%\n",
            "Epoch [59/250], Step [230/263], Loss: 0.3112, Accuracy: 86.88%\n",
            "Epoch [59/250], Step [240/263], Loss: 0.2759, Accuracy: 88.44%\n",
            "Epoch [59/250], Step [250/263], Loss: 0.2719, Accuracy: 89.53%\n",
            "Epoch [59/250], Step [260/263], Loss: 0.2816, Accuracy: 87.03%\n",
            "Epoch [60/250], Step [10/263], Loss: 0.2769, Accuracy: 88.91%\n",
            "Epoch [60/250], Step [20/263], Loss: 0.2823, Accuracy: 87.66%\n",
            "Epoch [60/250], Step [30/263], Loss: 0.2502, Accuracy: 90.31%\n",
            "Epoch [60/250], Step [40/263], Loss: 0.3109, Accuracy: 88.12%\n",
            "Epoch [60/250], Step [50/263], Loss: 0.2902, Accuracy: 88.44%\n",
            "Epoch [60/250], Step [60/263], Loss: 0.2742, Accuracy: 89.69%\n",
            "Epoch [60/250], Step [70/263], Loss: 0.2812, Accuracy: 88.59%\n",
            "Epoch [60/250], Step [80/263], Loss: 0.2590, Accuracy: 89.06%\n",
            "Epoch [60/250], Step [90/263], Loss: 0.2850, Accuracy: 87.50%\n",
            "Epoch [60/250], Step [100/263], Loss: 0.2981, Accuracy: 86.56%\n",
            "Epoch [60/250], Step [110/263], Loss: 0.3130, Accuracy: 86.56%\n",
            "Epoch [60/250], Step [120/263], Loss: 0.2967, Accuracy: 87.81%\n",
            "Epoch [60/250], Step [130/263], Loss: 0.2716, Accuracy: 89.38%\n",
            "Epoch [60/250], Step [140/263], Loss: 0.2707, Accuracy: 88.28%\n",
            "Epoch [60/250], Step [150/263], Loss: 0.2668, Accuracy: 88.44%\n",
            "Epoch [60/250], Step [160/263], Loss: 0.2988, Accuracy: 87.66%\n",
            "Epoch [60/250], Step [170/263], Loss: 0.3305, Accuracy: 85.00%\n",
            "Epoch [60/250], Step [180/263], Loss: 0.2513, Accuracy: 88.75%\n",
            "Epoch [60/250], Step [190/263], Loss: 0.3099, Accuracy: 87.66%\n",
            "Epoch [60/250], Step [200/263], Loss: 0.2878, Accuracy: 87.97%\n",
            "Epoch [60/250], Step [210/263], Loss: 0.3521, Accuracy: 85.16%\n",
            "Epoch [60/250], Step [220/263], Loss: 0.2915, Accuracy: 87.19%\n",
            "Epoch [60/250], Step [230/263], Loss: 0.3244, Accuracy: 85.00%\n",
            "Epoch [60/250], Step [240/263], Loss: 0.3030, Accuracy: 87.19%\n",
            "Epoch [60/250], Step [250/263], Loss: 0.2575, Accuracy: 90.31%\n",
            "Epoch [60/250], Step [260/263], Loss: 0.2689, Accuracy: 89.69%\n",
            "Epoch [61/250], Step [10/263], Loss: 0.3061, Accuracy: 87.34%\n",
            "Epoch [61/250], Step [20/263], Loss: 0.3021, Accuracy: 86.56%\n",
            "Epoch [61/250], Step [30/263], Loss: 0.2893, Accuracy: 88.12%\n",
            "Epoch [61/250], Step [40/263], Loss: 0.3106, Accuracy: 86.41%\n",
            "Epoch [61/250], Step [50/263], Loss: 0.2852, Accuracy: 87.66%\n",
            "Epoch [61/250], Step [60/263], Loss: 0.2834, Accuracy: 88.91%\n",
            "Epoch [61/250], Step [70/263], Loss: 0.2796, Accuracy: 88.75%\n",
            "Epoch [61/250], Step [80/263], Loss: 0.2910, Accuracy: 88.44%\n",
            "Epoch [61/250], Step [90/263], Loss: 0.2929, Accuracy: 87.66%\n",
            "Epoch [61/250], Step [100/263], Loss: 0.2637, Accuracy: 90.62%\n",
            "Epoch [61/250], Step [110/263], Loss: 0.2907, Accuracy: 88.28%\n",
            "Epoch [61/250], Step [120/263], Loss: 0.2760, Accuracy: 87.81%\n",
            "Epoch [61/250], Step [130/263], Loss: 0.2585, Accuracy: 89.69%\n",
            "Epoch [61/250], Step [140/263], Loss: 0.2729, Accuracy: 88.28%\n",
            "Epoch [61/250], Step [150/263], Loss: 0.2683, Accuracy: 89.38%\n",
            "Epoch [61/250], Step [160/263], Loss: 0.2814, Accuracy: 88.59%\n",
            "Epoch [61/250], Step [170/263], Loss: 0.2737, Accuracy: 88.28%\n",
            "Epoch [61/250], Step [180/263], Loss: 0.2897, Accuracy: 88.59%\n",
            "Epoch [61/250], Step [190/263], Loss: 0.3055, Accuracy: 86.25%\n",
            "Epoch [61/250], Step [200/263], Loss: 0.3025, Accuracy: 87.66%\n",
            "Epoch [61/250], Step [210/263], Loss: 0.3016, Accuracy: 87.03%\n",
            "Epoch [61/250], Step [220/263], Loss: 0.2627, Accuracy: 90.62%\n",
            "Epoch [61/250], Step [230/263], Loss: 0.3008, Accuracy: 87.34%\n",
            "Epoch [61/250], Step [240/263], Loss: 0.2988, Accuracy: 88.12%\n",
            "Epoch [61/250], Step [250/263], Loss: 0.2833, Accuracy: 88.59%\n",
            "Epoch [61/250], Step [260/263], Loss: 0.2780, Accuracy: 87.66%\n",
            "Epoch [62/250], Step [10/263], Loss: 0.2481, Accuracy: 89.06%\n",
            "Epoch [62/250], Step [20/263], Loss: 0.2602, Accuracy: 89.84%\n",
            "Epoch [62/250], Step [30/263], Loss: 0.2746, Accuracy: 88.12%\n",
            "Epoch [62/250], Step [40/263], Loss: 0.2179, Accuracy: 90.94%\n",
            "Epoch [62/250], Step [50/263], Loss: 0.2574, Accuracy: 89.22%\n",
            "Epoch [62/250], Step [60/263], Loss: 0.2582, Accuracy: 89.53%\n",
            "Epoch [62/250], Step [70/263], Loss: 0.2711, Accuracy: 88.91%\n",
            "Epoch [62/250], Step [80/263], Loss: 0.3145, Accuracy: 85.94%\n",
            "Epoch [62/250], Step [90/263], Loss: 0.2819, Accuracy: 88.28%\n",
            "Epoch [62/250], Step [100/263], Loss: 0.2788, Accuracy: 88.44%\n",
            "Epoch [62/250], Step [110/263], Loss: 0.2941, Accuracy: 86.41%\n",
            "Epoch [62/250], Step [120/263], Loss: 0.2604, Accuracy: 88.91%\n",
            "Epoch [62/250], Step [130/263], Loss: 0.2970, Accuracy: 87.03%\n",
            "Epoch [62/250], Step [140/263], Loss: 0.2892, Accuracy: 88.59%\n",
            "Epoch [62/250], Step [150/263], Loss: 0.2620, Accuracy: 90.00%\n",
            "Epoch [62/250], Step [160/263], Loss: 0.3185, Accuracy: 85.94%\n",
            "Epoch [62/250], Step [170/263], Loss: 0.3157, Accuracy: 87.03%\n",
            "Epoch [62/250], Step [180/263], Loss: 0.2692, Accuracy: 89.84%\n",
            "Epoch [62/250], Step [190/263], Loss: 0.2727, Accuracy: 87.34%\n",
            "Epoch [62/250], Step [200/263], Loss: 0.3255, Accuracy: 87.34%\n",
            "Epoch [62/250], Step [210/263], Loss: 0.3122, Accuracy: 88.12%\n",
            "Epoch [62/250], Step [220/263], Loss: 0.3290, Accuracy: 87.19%\n",
            "Epoch [62/250], Step [230/263], Loss: 0.3240, Accuracy: 86.09%\n",
            "Epoch [62/250], Step [240/263], Loss: 0.2921, Accuracy: 88.44%\n",
            "Epoch [62/250], Step [250/263], Loss: 0.3415, Accuracy: 85.94%\n",
            "Epoch [62/250], Step [260/263], Loss: 0.3416, Accuracy: 85.78%\n",
            "Epoch [63/250], Step [10/263], Loss: 0.2497, Accuracy: 90.47%\n",
            "Epoch [63/250], Step [20/263], Loss: 0.3278, Accuracy: 88.12%\n",
            "Epoch [63/250], Step [30/263], Loss: 0.2750, Accuracy: 88.12%\n",
            "Epoch [63/250], Step [40/263], Loss: 0.3048, Accuracy: 87.34%\n",
            "Epoch [63/250], Step [50/263], Loss: 0.3116, Accuracy: 87.97%\n",
            "Epoch [63/250], Step [60/263], Loss: 0.3067, Accuracy: 87.81%\n",
            "Epoch [63/250], Step [70/263], Loss: 0.2836, Accuracy: 87.81%\n",
            "Epoch [63/250], Step [80/263], Loss: 0.2832, Accuracy: 89.06%\n",
            "Epoch [63/250], Step [90/263], Loss: 0.2924, Accuracy: 87.19%\n",
            "Epoch [63/250], Step [100/263], Loss: 0.2719, Accuracy: 87.34%\n",
            "Epoch [63/250], Step [110/263], Loss: 0.3346, Accuracy: 86.72%\n",
            "Epoch [63/250], Step [120/263], Loss: 0.3329, Accuracy: 85.00%\n",
            "Epoch [63/250], Step [130/263], Loss: 0.3398, Accuracy: 85.94%\n",
            "Epoch [63/250], Step [140/263], Loss: 0.3946, Accuracy: 84.84%\n",
            "Epoch [63/250], Step [150/263], Loss: 0.3591, Accuracy: 85.78%\n",
            "Epoch [63/250], Step [160/263], Loss: 0.3145, Accuracy: 86.56%\n",
            "Epoch [63/250], Step [170/263], Loss: 0.2973, Accuracy: 86.88%\n",
            "Epoch [63/250], Step [180/263], Loss: 0.2820, Accuracy: 87.66%\n",
            "Epoch [63/250], Step [190/263], Loss: 0.3178, Accuracy: 86.88%\n",
            "Epoch [63/250], Step [200/263], Loss: 0.2795, Accuracy: 88.75%\n",
            "Epoch [63/250], Step [210/263], Loss: 0.2727, Accuracy: 88.28%\n",
            "Epoch [63/250], Step [220/263], Loss: 0.2626, Accuracy: 88.44%\n",
            "Epoch [63/250], Step [230/263], Loss: 0.2952, Accuracy: 87.34%\n",
            "Epoch [63/250], Step [240/263], Loss: 0.2974, Accuracy: 87.19%\n",
            "Epoch [63/250], Step [250/263], Loss: 0.2723, Accuracy: 89.53%\n",
            "Epoch [63/250], Step [260/263], Loss: 0.2734, Accuracy: 89.22%\n",
            "Epoch [64/250], Step [10/263], Loss: 0.2765, Accuracy: 89.06%\n",
            "Epoch [64/250], Step [20/263], Loss: 0.2709, Accuracy: 88.12%\n",
            "Epoch [64/250], Step [30/263], Loss: 0.2787, Accuracy: 87.19%\n",
            "Epoch [64/250], Step [40/263], Loss: 0.3215, Accuracy: 89.06%\n",
            "Epoch [64/250], Step [50/263], Loss: 0.2763, Accuracy: 88.12%\n",
            "Epoch [64/250], Step [60/263], Loss: 0.3067, Accuracy: 86.72%\n",
            "Epoch [64/250], Step [70/263], Loss: 0.2903, Accuracy: 89.06%\n",
            "Epoch [64/250], Step [80/263], Loss: 0.3059, Accuracy: 86.09%\n",
            "Epoch [64/250], Step [90/263], Loss: 0.2638, Accuracy: 88.44%\n",
            "Epoch [64/250], Step [100/263], Loss: 0.3050, Accuracy: 89.06%\n",
            "Epoch [64/250], Step [110/263], Loss: 0.2744, Accuracy: 88.91%\n",
            "Epoch [64/250], Step [120/263], Loss: 0.3034, Accuracy: 87.19%\n",
            "Epoch [64/250], Step [130/263], Loss: 0.2873, Accuracy: 88.91%\n",
            "Epoch [64/250], Step [140/263], Loss: 0.3002, Accuracy: 87.81%\n",
            "Epoch [64/250], Step [150/263], Loss: 0.3359, Accuracy: 85.31%\n",
            "Epoch [64/250], Step [160/263], Loss: 0.4065, Accuracy: 84.22%\n",
            "Epoch [64/250], Step [170/263], Loss: 0.3123, Accuracy: 86.88%\n",
            "Epoch [64/250], Step [180/263], Loss: 0.3112, Accuracy: 88.44%\n",
            "Epoch [64/250], Step [190/263], Loss: 0.3313, Accuracy: 85.94%\n",
            "Epoch [64/250], Step [200/263], Loss: 0.2850, Accuracy: 87.81%\n",
            "Epoch [64/250], Step [210/263], Loss: 0.3762, Accuracy: 85.94%\n",
            "Epoch [64/250], Step [220/263], Loss: 0.3031, Accuracy: 89.84%\n",
            "Epoch [64/250], Step [230/263], Loss: 0.2905, Accuracy: 88.44%\n",
            "Epoch [64/250], Step [240/263], Loss: 0.2993, Accuracy: 88.28%\n",
            "Epoch [64/250], Step [250/263], Loss: 0.2816, Accuracy: 89.22%\n",
            "Epoch [64/250], Step [260/263], Loss: 0.2806, Accuracy: 88.44%\n",
            "Epoch [65/250], Step [10/263], Loss: 0.2689, Accuracy: 89.22%\n",
            "Epoch [65/250], Step [20/263], Loss: 0.2728, Accuracy: 89.69%\n",
            "Epoch [65/250], Step [30/263], Loss: 0.2788, Accuracy: 89.22%\n",
            "Epoch [65/250], Step [40/263], Loss: 0.2772, Accuracy: 89.22%\n",
            "Epoch [65/250], Step [50/263], Loss: 0.2783, Accuracy: 88.59%\n",
            "Epoch [65/250], Step [60/263], Loss: 0.3205, Accuracy: 87.50%\n",
            "Epoch [65/250], Step [70/263], Loss: 0.2824, Accuracy: 87.81%\n",
            "Epoch [65/250], Step [80/263], Loss: 0.3271, Accuracy: 86.41%\n",
            "Epoch [65/250], Step [90/263], Loss: 0.2627, Accuracy: 89.22%\n",
            "Epoch [65/250], Step [100/263], Loss: 0.2964, Accuracy: 87.34%\n",
            "Epoch [65/250], Step [110/263], Loss: 0.2775, Accuracy: 86.09%\n",
            "Epoch [65/250], Step [120/263], Loss: 0.2571, Accuracy: 89.84%\n",
            "Epoch [65/250], Step [130/263], Loss: 0.2856, Accuracy: 87.97%\n",
            "Epoch [65/250], Step [140/263], Loss: 0.3162, Accuracy: 86.25%\n",
            "Epoch [65/250], Step [150/263], Loss: 0.2626, Accuracy: 88.12%\n",
            "Epoch [65/250], Step [160/263], Loss: 0.2779, Accuracy: 88.91%\n",
            "Epoch [65/250], Step [170/263], Loss: 0.2787, Accuracy: 88.28%\n",
            "Epoch [65/250], Step [180/263], Loss: 0.3240, Accuracy: 86.41%\n",
            "Epoch [65/250], Step [190/263], Loss: 0.2397, Accuracy: 88.75%\n",
            "Epoch [65/250], Step [200/263], Loss: 0.2690, Accuracy: 89.53%\n",
            "Epoch [65/250], Step [210/263], Loss: 0.2426, Accuracy: 89.06%\n",
            "Epoch [65/250], Step [220/263], Loss: 0.2979, Accuracy: 88.75%\n",
            "Epoch [65/250], Step [230/263], Loss: 0.2736, Accuracy: 88.91%\n",
            "Epoch [65/250], Step [240/263], Loss: 0.2982, Accuracy: 87.03%\n",
            "Epoch [65/250], Step [250/263], Loss: 0.2945, Accuracy: 87.19%\n",
            "Epoch [65/250], Step [260/263], Loss: 0.2217, Accuracy: 91.72%\n",
            "Epoch [66/250], Step [10/263], Loss: 0.2930, Accuracy: 87.50%\n",
            "Epoch [66/250], Step [20/263], Loss: 0.2820, Accuracy: 88.59%\n",
            "Epoch [66/250], Step [30/263], Loss: 0.2910, Accuracy: 89.22%\n",
            "Epoch [66/250], Step [40/263], Loss: 0.2592, Accuracy: 89.06%\n",
            "Epoch [66/250], Step [50/263], Loss: 0.2893, Accuracy: 87.97%\n",
            "Epoch [66/250], Step [60/263], Loss: 0.2961, Accuracy: 87.03%\n",
            "Epoch [66/250], Step [70/263], Loss: 0.3000, Accuracy: 88.28%\n",
            "Epoch [66/250], Step [80/263], Loss: 0.2847, Accuracy: 87.97%\n",
            "Epoch [66/250], Step [90/263], Loss: 0.2484, Accuracy: 90.78%\n",
            "Epoch [66/250], Step [100/263], Loss: 0.3052, Accuracy: 87.66%\n",
            "Epoch [66/250], Step [110/263], Loss: 0.2752, Accuracy: 87.97%\n",
            "Epoch [66/250], Step [120/263], Loss: 0.2817, Accuracy: 88.28%\n",
            "Epoch [66/250], Step [130/263], Loss: 0.2438, Accuracy: 90.16%\n",
            "Epoch [66/250], Step [140/263], Loss: 0.2950, Accuracy: 88.12%\n",
            "Epoch [66/250], Step [150/263], Loss: 0.2810, Accuracy: 88.28%\n",
            "Epoch [66/250], Step [160/263], Loss: 0.3154, Accuracy: 86.25%\n",
            "Epoch [66/250], Step [170/263], Loss: 0.2586, Accuracy: 89.53%\n",
            "Epoch [66/250], Step [180/263], Loss: 0.3006, Accuracy: 88.28%\n",
            "Epoch [66/250], Step [190/263], Loss: 0.2974, Accuracy: 85.94%\n",
            "Epoch [66/250], Step [200/263], Loss: 0.2827, Accuracy: 87.50%\n",
            "Epoch [66/250], Step [210/263], Loss: 0.2778, Accuracy: 88.44%\n",
            "Epoch [66/250], Step [220/263], Loss: 0.2671, Accuracy: 88.91%\n",
            "Epoch [66/250], Step [230/263], Loss: 0.2779, Accuracy: 88.91%\n",
            "Epoch [66/250], Step [240/263], Loss: 0.3048, Accuracy: 87.19%\n",
            "Epoch [66/250], Step [250/263], Loss: 0.3004, Accuracy: 87.97%\n",
            "Epoch [66/250], Step [260/263], Loss: 0.2295, Accuracy: 90.31%\n",
            "Epoch [67/250], Step [10/263], Loss: 0.2351, Accuracy: 90.31%\n",
            "Epoch [67/250], Step [20/263], Loss: 0.2500, Accuracy: 90.31%\n",
            "Epoch [67/250], Step [30/263], Loss: 0.2757, Accuracy: 87.97%\n",
            "Epoch [67/250], Step [40/263], Loss: 0.2768, Accuracy: 89.22%\n",
            "Epoch [67/250], Step [50/263], Loss: 0.2764, Accuracy: 88.91%\n",
            "Epoch [67/250], Step [60/263], Loss: 0.2456, Accuracy: 88.91%\n",
            "Epoch [67/250], Step [70/263], Loss: 0.2643, Accuracy: 87.81%\n",
            "Epoch [67/250], Step [80/263], Loss: 0.2710, Accuracy: 88.91%\n",
            "Epoch [67/250], Step [90/263], Loss: 0.3308, Accuracy: 87.03%\n",
            "Epoch [67/250], Step [100/263], Loss: 0.2639, Accuracy: 89.22%\n",
            "Epoch [67/250], Step [110/263], Loss: 0.2992, Accuracy: 87.66%\n",
            "Epoch [67/250], Step [120/263], Loss: 0.2674, Accuracy: 88.12%\n",
            "Epoch [67/250], Step [130/263], Loss: 0.2812, Accuracy: 89.53%\n",
            "Epoch [67/250], Step [140/263], Loss: 0.3519, Accuracy: 85.31%\n",
            "Epoch [67/250], Step [150/263], Loss: 0.2845, Accuracy: 88.12%\n",
            "Epoch [67/250], Step [160/263], Loss: 0.3048, Accuracy: 86.88%\n",
            "Epoch [67/250], Step [170/263], Loss: 0.2889, Accuracy: 86.88%\n",
            "Epoch [67/250], Step [180/263], Loss: 0.2541, Accuracy: 90.16%\n",
            "Epoch [67/250], Step [190/263], Loss: 0.2865, Accuracy: 87.97%\n",
            "Epoch [67/250], Step [200/263], Loss: 0.2526, Accuracy: 90.16%\n",
            "Epoch [67/250], Step [210/263], Loss: 0.2958, Accuracy: 87.50%\n",
            "Epoch [67/250], Step [220/263], Loss: 0.3491, Accuracy: 85.31%\n",
            "Epoch [67/250], Step [230/263], Loss: 0.2785, Accuracy: 89.22%\n",
            "Epoch [67/250], Step [240/263], Loss: 0.2805, Accuracy: 88.28%\n",
            "Epoch [67/250], Step [250/263], Loss: 0.2450, Accuracy: 90.16%\n",
            "Epoch [67/250], Step [260/263], Loss: 0.2841, Accuracy: 87.19%\n",
            "Epoch [68/250], Step [10/263], Loss: 0.3050, Accuracy: 86.56%\n",
            "Epoch [68/250], Step [20/263], Loss: 0.2653, Accuracy: 89.69%\n",
            "Epoch [68/250], Step [30/263], Loss: 0.3185, Accuracy: 86.09%\n",
            "Epoch [68/250], Step [40/263], Loss: 0.2712, Accuracy: 89.53%\n",
            "Epoch [68/250], Step [50/263], Loss: 0.3064, Accuracy: 88.12%\n",
            "Epoch [68/250], Step [60/263], Loss: 0.2417, Accuracy: 90.16%\n",
            "Epoch [68/250], Step [70/263], Loss: 0.2703, Accuracy: 88.59%\n",
            "Epoch [68/250], Step [80/263], Loss: 0.2921, Accuracy: 88.59%\n",
            "Epoch [68/250], Step [90/263], Loss: 0.2713, Accuracy: 90.00%\n",
            "Epoch [68/250], Step [100/263], Loss: 0.2819, Accuracy: 88.28%\n",
            "Epoch [68/250], Step [110/263], Loss: 0.2611, Accuracy: 89.06%\n",
            "Epoch [68/250], Step [120/263], Loss: 0.2457, Accuracy: 89.22%\n",
            "Epoch [68/250], Step [130/263], Loss: 0.3132, Accuracy: 86.56%\n",
            "Epoch [68/250], Step [140/263], Loss: 0.3255, Accuracy: 86.25%\n",
            "Epoch [68/250], Step [150/263], Loss: 0.3250, Accuracy: 86.88%\n",
            "Epoch [68/250], Step [160/263], Loss: 0.3233, Accuracy: 86.41%\n",
            "Epoch [68/250], Step [170/263], Loss: 0.2979, Accuracy: 87.81%\n",
            "Epoch [68/250], Step [180/263], Loss: 0.3060, Accuracy: 87.19%\n",
            "Epoch [68/250], Step [190/263], Loss: 0.2883, Accuracy: 89.06%\n",
            "Epoch [68/250], Step [200/263], Loss: 0.2344, Accuracy: 90.62%\n",
            "Epoch [68/250], Step [210/263], Loss: 0.2718, Accuracy: 88.59%\n",
            "Epoch [68/250], Step [220/263], Loss: 0.2799, Accuracy: 89.69%\n",
            "Epoch [68/250], Step [230/263], Loss: 0.2713, Accuracy: 89.69%\n",
            "Epoch [68/250], Step [240/263], Loss: 0.3262, Accuracy: 87.97%\n",
            "Epoch [68/250], Step [250/263], Loss: 0.2909, Accuracy: 87.50%\n",
            "Epoch [68/250], Step [260/263], Loss: 0.2777, Accuracy: 88.59%\n",
            "Epoch [69/250], Step [10/263], Loss: 0.2904, Accuracy: 86.72%\n",
            "Epoch [69/250], Step [20/263], Loss: 0.2575, Accuracy: 89.22%\n",
            "Epoch [69/250], Step [30/263], Loss: 0.2735, Accuracy: 89.06%\n",
            "Epoch [69/250], Step [40/263], Loss: 0.2882, Accuracy: 88.59%\n",
            "Epoch [69/250], Step [50/263], Loss: 0.2731, Accuracy: 89.69%\n",
            "Epoch [69/250], Step [60/263], Loss: 0.2571, Accuracy: 88.59%\n",
            "Epoch [69/250], Step [70/263], Loss: 0.2823, Accuracy: 88.28%\n",
            "Epoch [69/250], Step [80/263], Loss: 0.2921, Accuracy: 87.34%\n",
            "Epoch [69/250], Step [90/263], Loss: 0.2678, Accuracy: 88.28%\n",
            "Epoch [69/250], Step [100/263], Loss: 0.2697, Accuracy: 88.28%\n",
            "Epoch [69/250], Step [110/263], Loss: 0.2576, Accuracy: 89.84%\n",
            "Epoch [69/250], Step [120/263], Loss: 0.2716, Accuracy: 87.97%\n",
            "Epoch [69/250], Step [130/263], Loss: 0.3234, Accuracy: 88.12%\n",
            "Epoch [69/250], Step [140/263], Loss: 0.3602, Accuracy: 86.56%\n",
            "Epoch [69/250], Step [150/263], Loss: 0.2654, Accuracy: 89.22%\n",
            "Epoch [69/250], Step [160/263], Loss: 0.2514, Accuracy: 88.91%\n",
            "Epoch [69/250], Step [170/263], Loss: 0.2778, Accuracy: 89.69%\n",
            "Epoch [69/250], Step [180/263], Loss: 0.2595, Accuracy: 90.31%\n",
            "Epoch [69/250], Step [190/263], Loss: 0.2985, Accuracy: 88.75%\n",
            "Epoch [69/250], Step [200/263], Loss: 0.3221, Accuracy: 87.50%\n",
            "Epoch [69/250], Step [210/263], Loss: 0.2573, Accuracy: 87.81%\n",
            "Epoch [69/250], Step [220/263], Loss: 0.3015, Accuracy: 88.12%\n",
            "Epoch [69/250], Step [230/263], Loss: 0.2675, Accuracy: 88.28%\n",
            "Epoch [69/250], Step [240/263], Loss: 0.2826, Accuracy: 87.66%\n",
            "Epoch [69/250], Step [250/263], Loss: 0.2753, Accuracy: 88.59%\n",
            "Epoch [69/250], Step [260/263], Loss: 0.2686, Accuracy: 89.22%\n",
            "Epoch [70/250], Step [10/263], Loss: 0.2651, Accuracy: 87.81%\n",
            "Epoch [70/250], Step [20/263], Loss: 0.2718, Accuracy: 89.22%\n",
            "Epoch [70/250], Step [30/263], Loss: 0.2565, Accuracy: 88.91%\n",
            "Epoch [70/250], Step [40/263], Loss: 0.2552, Accuracy: 89.84%\n",
            "Epoch [70/250], Step [50/263], Loss: 0.3233, Accuracy: 85.94%\n",
            "Epoch [70/250], Step [60/263], Loss: 0.2707, Accuracy: 88.59%\n",
            "Epoch [70/250], Step [70/263], Loss: 0.2598, Accuracy: 89.84%\n",
            "Epoch [70/250], Step [80/263], Loss: 0.2709, Accuracy: 88.44%\n",
            "Epoch [70/250], Step [90/263], Loss: 0.2534, Accuracy: 90.16%\n",
            "Epoch [70/250], Step [100/263], Loss: 0.2906, Accuracy: 87.34%\n",
            "Epoch [70/250], Step [110/263], Loss: 0.2636, Accuracy: 87.50%\n",
            "Epoch [70/250], Step [120/263], Loss: 0.2594, Accuracy: 89.38%\n",
            "Epoch [70/250], Step [130/263], Loss: 0.2537, Accuracy: 89.53%\n",
            "Epoch [70/250], Step [140/263], Loss: 0.2673, Accuracy: 89.06%\n",
            "Epoch [70/250], Step [150/263], Loss: 0.3199, Accuracy: 87.81%\n",
            "Epoch [70/250], Step [160/263], Loss: 0.2996, Accuracy: 86.25%\n",
            "Epoch [70/250], Step [170/263], Loss: 0.2817, Accuracy: 88.59%\n",
            "Epoch [70/250], Step [180/263], Loss: 0.2751, Accuracy: 88.44%\n",
            "Epoch [70/250], Step [190/263], Loss: 0.2881, Accuracy: 87.50%\n",
            "Epoch [70/250], Step [200/263], Loss: 0.2596, Accuracy: 88.91%\n",
            "Epoch [70/250], Step [210/263], Loss: 0.2629, Accuracy: 89.69%\n",
            "Epoch [70/250], Step [220/263], Loss: 0.2450, Accuracy: 89.69%\n",
            "Epoch [70/250], Step [230/263], Loss: 0.2761, Accuracy: 88.91%\n",
            "Epoch [70/250], Step [240/263], Loss: 0.2761, Accuracy: 87.66%\n",
            "Epoch [70/250], Step [250/263], Loss: 0.2315, Accuracy: 91.56%\n",
            "Epoch [70/250], Step [260/263], Loss: 0.2850, Accuracy: 88.59%\n",
            "Epoch [71/250], Step [10/263], Loss: 0.2609, Accuracy: 88.75%\n",
            "Epoch [71/250], Step [20/263], Loss: 0.2929, Accuracy: 87.03%\n",
            "Epoch [71/250], Step [30/263], Loss: 0.3233, Accuracy: 87.19%\n",
            "Epoch [71/250], Step [40/263], Loss: 0.2733, Accuracy: 88.59%\n",
            "Epoch [71/250], Step [50/263], Loss: 0.2745, Accuracy: 89.69%\n",
            "Epoch [71/250], Step [60/263], Loss: 0.3042, Accuracy: 87.50%\n",
            "Epoch [71/250], Step [70/263], Loss: 0.2956, Accuracy: 88.28%\n",
            "Epoch [71/250], Step [80/263], Loss: 0.3260, Accuracy: 87.19%\n",
            "Epoch [71/250], Step [90/263], Loss: 0.2399, Accuracy: 90.47%\n",
            "Epoch [71/250], Step [100/263], Loss: 0.2469, Accuracy: 89.69%\n",
            "Epoch [71/250], Step [110/263], Loss: 0.2671, Accuracy: 89.06%\n",
            "Epoch [71/250], Step [120/263], Loss: 0.2624, Accuracy: 89.22%\n",
            "Epoch [71/250], Step [130/263], Loss: 0.2840, Accuracy: 88.12%\n",
            "Epoch [71/250], Step [140/263], Loss: 0.2549, Accuracy: 90.16%\n",
            "Epoch [71/250], Step [150/263], Loss: 0.2443, Accuracy: 90.47%\n",
            "Epoch [71/250], Step [160/263], Loss: 0.2914, Accuracy: 87.97%\n",
            "Epoch [71/250], Step [170/263], Loss: 0.2860, Accuracy: 88.12%\n",
            "Epoch [71/250], Step [180/263], Loss: 0.2694, Accuracy: 88.59%\n",
            "Epoch [71/250], Step [190/263], Loss: 0.2777, Accuracy: 90.31%\n",
            "Epoch [71/250], Step [200/263], Loss: 0.2855, Accuracy: 89.22%\n",
            "Epoch [71/250], Step [210/263], Loss: 0.2247, Accuracy: 91.41%\n",
            "Epoch [71/250], Step [220/263], Loss: 0.2882, Accuracy: 87.81%\n",
            "Epoch [71/250], Step [230/263], Loss: 0.2759, Accuracy: 89.22%\n",
            "Epoch [71/250], Step [240/263], Loss: 0.3431, Accuracy: 86.88%\n",
            "Epoch [71/250], Step [250/263], Loss: 0.2792, Accuracy: 87.97%\n",
            "Epoch [71/250], Step [260/263], Loss: 0.2747, Accuracy: 89.22%\n",
            "Epoch [72/250], Step [10/263], Loss: 0.2944, Accuracy: 88.59%\n",
            "Epoch [72/250], Step [20/263], Loss: 0.3384, Accuracy: 86.09%\n",
            "Epoch [72/250], Step [30/263], Loss: 0.2716, Accuracy: 90.62%\n",
            "Epoch [72/250], Step [40/263], Loss: 0.3227, Accuracy: 85.00%\n",
            "Epoch [72/250], Step [50/263], Loss: 0.2841, Accuracy: 87.81%\n",
            "Epoch [72/250], Step [60/263], Loss: 0.2411, Accuracy: 91.09%\n",
            "Epoch [72/250], Step [70/263], Loss: 0.2728, Accuracy: 88.59%\n",
            "Epoch [72/250], Step [80/263], Loss: 0.3112, Accuracy: 86.72%\n",
            "Epoch [72/250], Step [90/263], Loss: 0.3283, Accuracy: 86.72%\n",
            "Epoch [72/250], Step [100/263], Loss: 0.2787, Accuracy: 88.59%\n",
            "Epoch [72/250], Step [110/263], Loss: 0.3144, Accuracy: 87.19%\n",
            "Epoch [72/250], Step [120/263], Loss: 0.3401, Accuracy: 87.66%\n",
            "Epoch [72/250], Step [130/263], Loss: 0.3423, Accuracy: 85.62%\n",
            "Epoch [72/250], Step [140/263], Loss: 0.3533, Accuracy: 86.09%\n",
            "Epoch [72/250], Step [150/263], Loss: 0.3141, Accuracy: 87.19%\n",
            "Epoch [72/250], Step [160/263], Loss: 0.2799, Accuracy: 89.22%\n",
            "Epoch [72/250], Step [170/263], Loss: 0.2632, Accuracy: 88.75%\n",
            "Epoch [72/250], Step [180/263], Loss: 0.2832, Accuracy: 88.91%\n",
            "Epoch [72/250], Step [190/263], Loss: 0.2944, Accuracy: 87.81%\n",
            "Epoch [72/250], Step [200/263], Loss: 0.2345, Accuracy: 91.88%\n",
            "Epoch [72/250], Step [210/263], Loss: 0.2529, Accuracy: 89.69%\n",
            "Epoch [72/250], Step [220/263], Loss: 0.2679, Accuracy: 88.75%\n",
            "Epoch [72/250], Step [230/263], Loss: 0.2516, Accuracy: 90.00%\n",
            "Epoch [72/250], Step [240/263], Loss: 0.2901, Accuracy: 87.19%\n",
            "Epoch [72/250], Step [250/263], Loss: 0.2589, Accuracy: 89.69%\n",
            "Epoch [72/250], Step [260/263], Loss: 0.2421, Accuracy: 90.78%\n",
            "Epoch [73/250], Step [10/263], Loss: 0.2633, Accuracy: 88.59%\n",
            "Epoch [73/250], Step [20/263], Loss: 0.2446, Accuracy: 89.38%\n",
            "Epoch [73/250], Step [30/263], Loss: 0.2801, Accuracy: 88.44%\n",
            "Epoch [73/250], Step [40/263], Loss: 0.2558, Accuracy: 89.22%\n",
            "Epoch [73/250], Step [50/263], Loss: 0.2856, Accuracy: 88.91%\n",
            "Epoch [73/250], Step [60/263], Loss: 0.2416, Accuracy: 90.78%\n",
            "Epoch [73/250], Step [70/263], Loss: 0.2940, Accuracy: 87.03%\n",
            "Epoch [73/250], Step [80/263], Loss: 0.2170, Accuracy: 91.09%\n",
            "Epoch [73/250], Step [90/263], Loss: 0.2536, Accuracy: 89.69%\n",
            "Epoch [73/250], Step [100/263], Loss: 0.2624, Accuracy: 89.22%\n",
            "Epoch [73/250], Step [110/263], Loss: 0.2630, Accuracy: 87.97%\n",
            "Epoch [73/250], Step [120/263], Loss: 0.2431, Accuracy: 89.22%\n",
            "Epoch [73/250], Step [130/263], Loss: 0.2864, Accuracy: 87.50%\n",
            "Epoch [73/250], Step [140/263], Loss: 0.2827, Accuracy: 88.91%\n",
            "Epoch [73/250], Step [150/263], Loss: 0.2464, Accuracy: 89.22%\n",
            "Epoch [73/250], Step [160/263], Loss: 0.2678, Accuracy: 88.59%\n",
            "Epoch [73/250], Step [170/263], Loss: 0.2764, Accuracy: 89.38%\n",
            "Epoch [73/250], Step [180/263], Loss: 0.2606, Accuracy: 89.38%\n",
            "Epoch [73/250], Step [190/263], Loss: 0.2888, Accuracy: 88.12%\n",
            "Epoch [73/250], Step [200/263], Loss: 0.2704, Accuracy: 87.97%\n",
            "Epoch [73/250], Step [210/263], Loss: 0.2654, Accuracy: 90.00%\n",
            "Epoch [73/250], Step [220/263], Loss: 0.2633, Accuracy: 90.62%\n",
            "Epoch [73/250], Step [230/263], Loss: 0.2637, Accuracy: 89.53%\n",
            "Epoch [73/250], Step [240/263], Loss: 0.2768, Accuracy: 89.22%\n",
            "Epoch [73/250], Step [250/263], Loss: 0.2634, Accuracy: 87.81%\n",
            "Epoch [73/250], Step [260/263], Loss: 0.2565, Accuracy: 88.91%\n",
            "Epoch [74/250], Step [10/263], Loss: 0.2225, Accuracy: 91.25%\n",
            "Epoch [74/250], Step [20/263], Loss: 0.2907, Accuracy: 88.12%\n",
            "Epoch [74/250], Step [30/263], Loss: 0.2367, Accuracy: 90.31%\n",
            "Epoch [74/250], Step [40/263], Loss: 0.3143, Accuracy: 85.62%\n",
            "Epoch [74/250], Step [50/263], Loss: 0.3007, Accuracy: 87.03%\n",
            "Epoch [74/250], Step [60/263], Loss: 0.2485, Accuracy: 90.78%\n",
            "Epoch [74/250], Step [70/263], Loss: 0.2458, Accuracy: 89.22%\n",
            "Epoch [74/250], Step [80/263], Loss: 0.2727, Accuracy: 88.75%\n",
            "Epoch [74/250], Step [90/263], Loss: 0.2863, Accuracy: 87.66%\n",
            "Epoch [74/250], Step [100/263], Loss: 0.2511, Accuracy: 87.81%\n",
            "Epoch [74/250], Step [110/263], Loss: 0.2623, Accuracy: 89.06%\n",
            "Epoch [74/250], Step [120/263], Loss: 0.2542, Accuracy: 90.47%\n",
            "Epoch [74/250], Step [130/263], Loss: 0.2492, Accuracy: 89.84%\n",
            "Epoch [74/250], Step [140/263], Loss: 0.2647, Accuracy: 88.75%\n",
            "Epoch [74/250], Step [150/263], Loss: 0.2800, Accuracy: 87.97%\n",
            "Epoch [74/250], Step [160/263], Loss: 0.2635, Accuracy: 89.69%\n",
            "Epoch [74/250], Step [170/263], Loss: 0.2758, Accuracy: 89.69%\n",
            "Epoch [74/250], Step [180/263], Loss: 0.2477, Accuracy: 90.94%\n",
            "Epoch [74/250], Step [190/263], Loss: 0.3115, Accuracy: 87.50%\n",
            "Epoch [74/250], Step [200/263], Loss: 0.2905, Accuracy: 87.81%\n",
            "Epoch [74/250], Step [210/263], Loss: 0.2575, Accuracy: 89.22%\n",
            "Epoch [74/250], Step [220/263], Loss: 0.2310, Accuracy: 90.47%\n",
            "Epoch [74/250], Step [230/263], Loss: 0.2704, Accuracy: 88.44%\n",
            "Epoch [74/250], Step [240/263], Loss: 0.2684, Accuracy: 88.91%\n",
            "Epoch [74/250], Step [250/263], Loss: 0.2783, Accuracy: 88.75%\n",
            "Epoch [74/250], Step [260/263], Loss: 0.2994, Accuracy: 87.50%\n",
            "Epoch [75/250], Step [10/263], Loss: 0.2782, Accuracy: 88.44%\n",
            "Epoch [75/250], Step [20/263], Loss: 0.2719, Accuracy: 88.91%\n",
            "Epoch [75/250], Step [30/263], Loss: 0.3063, Accuracy: 88.12%\n",
            "Epoch [75/250], Step [40/263], Loss: 0.2403, Accuracy: 91.09%\n",
            "Epoch [75/250], Step [50/263], Loss: 0.2557, Accuracy: 90.62%\n",
            "Epoch [75/250], Step [60/263], Loss: 0.2983, Accuracy: 87.66%\n",
            "Epoch [75/250], Step [70/263], Loss: 0.2529, Accuracy: 88.44%\n",
            "Epoch [75/250], Step [80/263], Loss: 0.2691, Accuracy: 88.44%\n",
            "Epoch [75/250], Step [90/263], Loss: 0.2438, Accuracy: 89.84%\n",
            "Epoch [75/250], Step [100/263], Loss: 0.2927, Accuracy: 88.12%\n",
            "Epoch [75/250], Step [110/263], Loss: 0.3145, Accuracy: 86.25%\n",
            "Epoch [75/250], Step [120/263], Loss: 0.2525, Accuracy: 90.16%\n",
            "Epoch [75/250], Step [130/263], Loss: 0.2887, Accuracy: 89.22%\n",
            "Epoch [75/250], Step [140/263], Loss: 0.2837, Accuracy: 86.88%\n",
            "Epoch [75/250], Step [150/263], Loss: 0.2630, Accuracy: 88.12%\n",
            "Epoch [75/250], Step [160/263], Loss: 0.2530, Accuracy: 89.69%\n",
            "Epoch [75/250], Step [170/263], Loss: 0.2683, Accuracy: 89.69%\n",
            "Epoch [75/250], Step [180/263], Loss: 0.2785, Accuracy: 88.91%\n",
            "Epoch [75/250], Step [190/263], Loss: 0.2589, Accuracy: 89.22%\n",
            "Epoch [75/250], Step [200/263], Loss: 0.2804, Accuracy: 88.91%\n",
            "Epoch [75/250], Step [210/263], Loss: 0.2693, Accuracy: 89.69%\n",
            "Epoch [75/250], Step [220/263], Loss: 0.2653, Accuracy: 89.06%\n",
            "Epoch [75/250], Step [230/263], Loss: 0.2757, Accuracy: 86.25%\n",
            "Epoch [75/250], Step [240/263], Loss: 0.2519, Accuracy: 89.69%\n",
            "Epoch [75/250], Step [250/263], Loss: 0.3070, Accuracy: 87.97%\n",
            "Epoch [75/250], Step [260/263], Loss: 0.2605, Accuracy: 89.22%\n",
            "Epoch [76/250], Step [10/263], Loss: 0.2350, Accuracy: 90.94%\n",
            "Epoch [76/250], Step [20/263], Loss: 0.2726, Accuracy: 88.44%\n",
            "Epoch [76/250], Step [30/263], Loss: 0.2522, Accuracy: 88.28%\n",
            "Epoch [76/250], Step [40/263], Loss: 0.2569, Accuracy: 88.75%\n",
            "Epoch [76/250], Step [50/263], Loss: 0.2803, Accuracy: 88.59%\n",
            "Epoch [76/250], Step [60/263], Loss: 0.2702, Accuracy: 89.84%\n",
            "Epoch [76/250], Step [70/263], Loss: 0.3004, Accuracy: 88.12%\n",
            "Epoch [76/250], Step [80/263], Loss: 0.2592, Accuracy: 90.31%\n",
            "Epoch [76/250], Step [90/263], Loss: 0.2457, Accuracy: 90.16%\n",
            "Epoch [76/250], Step [100/263], Loss: 0.2103, Accuracy: 90.94%\n",
            "Epoch [76/250], Step [110/263], Loss: 0.2651, Accuracy: 89.84%\n",
            "Epoch [76/250], Step [120/263], Loss: 0.2625, Accuracy: 88.59%\n",
            "Epoch [76/250], Step [130/263], Loss: 0.2601, Accuracy: 89.84%\n",
            "Epoch [76/250], Step [140/263], Loss: 0.2774, Accuracy: 88.59%\n",
            "Epoch [76/250], Step [150/263], Loss: 0.3484, Accuracy: 86.41%\n",
            "Epoch [76/250], Step [160/263], Loss: 0.2604, Accuracy: 88.44%\n",
            "Epoch [76/250], Step [170/263], Loss: 0.2895, Accuracy: 88.28%\n",
            "Epoch [76/250], Step [180/263], Loss: 0.2888, Accuracy: 87.34%\n",
            "Epoch [76/250], Step [190/263], Loss: 0.2838, Accuracy: 87.97%\n",
            "Epoch [76/250], Step [200/263], Loss: 0.2563, Accuracy: 89.84%\n",
            "Epoch [76/250], Step [210/263], Loss: 0.2311, Accuracy: 91.25%\n",
            "Epoch [76/250], Step [220/263], Loss: 0.2498, Accuracy: 90.78%\n",
            "Epoch [76/250], Step [230/263], Loss: 0.2709, Accuracy: 90.00%\n",
            "Epoch [76/250], Step [240/263], Loss: 0.2855, Accuracy: 87.66%\n",
            "Epoch [76/250], Step [250/263], Loss: 0.3033, Accuracy: 88.28%\n",
            "Epoch [76/250], Step [260/263], Loss: 0.2544, Accuracy: 89.84%\n",
            "Epoch [77/250], Step [10/263], Loss: 0.2731, Accuracy: 88.12%\n",
            "Epoch [77/250], Step [20/263], Loss: 0.2238, Accuracy: 90.62%\n",
            "Epoch [77/250], Step [30/263], Loss: 0.2945, Accuracy: 87.50%\n",
            "Epoch [77/250], Step [40/263], Loss: 0.2435, Accuracy: 90.62%\n",
            "Epoch [77/250], Step [50/263], Loss: 0.2756, Accuracy: 88.28%\n",
            "Epoch [77/250], Step [60/263], Loss: 0.2506, Accuracy: 89.69%\n",
            "Epoch [77/250], Step [70/263], Loss: 0.3027, Accuracy: 87.97%\n",
            "Epoch [77/250], Step [80/263], Loss: 0.2411, Accuracy: 89.69%\n",
            "Epoch [77/250], Step [90/263], Loss: 0.2216, Accuracy: 90.16%\n",
            "Epoch [77/250], Step [100/263], Loss: 0.2490, Accuracy: 90.31%\n",
            "Epoch [77/250], Step [110/263], Loss: 0.2690, Accuracy: 88.59%\n",
            "Epoch [77/250], Step [120/263], Loss: 0.2553, Accuracy: 89.38%\n",
            "Epoch [77/250], Step [130/263], Loss: 0.2943, Accuracy: 87.19%\n",
            "Epoch [77/250], Step [140/263], Loss: 0.2340, Accuracy: 91.25%\n",
            "Epoch [77/250], Step [150/263], Loss: 0.2384, Accuracy: 91.09%\n",
            "Epoch [77/250], Step [160/263], Loss: 0.2632, Accuracy: 87.97%\n",
            "Epoch [77/250], Step [170/263], Loss: 0.2878, Accuracy: 86.88%\n",
            "Epoch [77/250], Step [180/263], Loss: 0.2729, Accuracy: 88.59%\n",
            "Epoch [77/250], Step [190/263], Loss: 0.2651, Accuracy: 89.06%\n",
            "Epoch [77/250], Step [200/263], Loss: 0.2882, Accuracy: 87.97%\n",
            "Epoch [77/250], Step [210/263], Loss: 0.2481, Accuracy: 90.00%\n",
            "Epoch [77/250], Step [220/263], Loss: 0.2237, Accuracy: 90.62%\n",
            "Epoch [77/250], Step [230/263], Loss: 0.2434, Accuracy: 89.69%\n",
            "Epoch [77/250], Step [240/263], Loss: 0.2578, Accuracy: 89.06%\n",
            "Epoch [77/250], Step [250/263], Loss: 0.2474, Accuracy: 91.25%\n",
            "Epoch [77/250], Step [260/263], Loss: 0.2327, Accuracy: 91.25%\n",
            "Epoch [78/250], Step [10/263], Loss: 0.2897, Accuracy: 88.28%\n",
            "Epoch [78/250], Step [20/263], Loss: 0.2984, Accuracy: 86.88%\n",
            "Epoch [78/250], Step [30/263], Loss: 0.2214, Accuracy: 91.09%\n",
            "Epoch [78/250], Step [40/263], Loss: 0.2285, Accuracy: 90.78%\n",
            "Epoch [78/250], Step [50/263], Loss: 0.2228, Accuracy: 91.72%\n",
            "Epoch [78/250], Step [60/263], Loss: 0.2430, Accuracy: 89.69%\n",
            "Epoch [78/250], Step [70/263], Loss: 0.2373, Accuracy: 90.62%\n",
            "Epoch [78/250], Step [80/263], Loss: 0.2185, Accuracy: 91.41%\n",
            "Epoch [78/250], Step [90/263], Loss: 0.2614, Accuracy: 88.75%\n",
            "Epoch [78/250], Step [100/263], Loss: 0.2405, Accuracy: 89.69%\n",
            "Epoch [78/250], Step [110/263], Loss: 0.2567, Accuracy: 89.53%\n",
            "Epoch [78/250], Step [120/263], Loss: 0.2682, Accuracy: 89.22%\n",
            "Epoch [78/250], Step [130/263], Loss: 0.2976, Accuracy: 87.97%\n",
            "Epoch [78/250], Step [140/263], Loss: 0.2155, Accuracy: 91.88%\n",
            "Epoch [78/250], Step [150/263], Loss: 0.2660, Accuracy: 89.38%\n",
            "Epoch [78/250], Step [160/263], Loss: 0.2437, Accuracy: 90.94%\n",
            "Epoch [78/250], Step [170/263], Loss: 0.2307, Accuracy: 90.62%\n",
            "Epoch [78/250], Step [180/263], Loss: 0.2871, Accuracy: 88.12%\n",
            "Epoch [78/250], Step [190/263], Loss: 0.2879, Accuracy: 88.44%\n",
            "Epoch [78/250], Step [200/263], Loss: 0.2842, Accuracy: 88.12%\n",
            "Epoch [78/250], Step [210/263], Loss: 0.3167, Accuracy: 86.25%\n",
            "Epoch [78/250], Step [220/263], Loss: 0.2497, Accuracy: 88.75%\n",
            "Epoch [78/250], Step [230/263], Loss: 0.2515, Accuracy: 89.53%\n",
            "Epoch [78/250], Step [240/263], Loss: 0.2464, Accuracy: 88.75%\n",
            "Epoch [78/250], Step [250/263], Loss: 0.2546, Accuracy: 89.53%\n",
            "Epoch [78/250], Step [260/263], Loss: 0.2380, Accuracy: 90.47%\n",
            "Epoch [79/250], Step [10/263], Loss: 0.2251, Accuracy: 91.72%\n",
            "Epoch [79/250], Step [20/263], Loss: 0.2598, Accuracy: 89.06%\n",
            "Epoch [79/250], Step [30/263], Loss: 0.2653, Accuracy: 88.75%\n",
            "Epoch [79/250], Step [40/263], Loss: 0.2840, Accuracy: 88.75%\n",
            "Epoch [79/250], Step [50/263], Loss: 0.2589, Accuracy: 89.69%\n",
            "Epoch [79/250], Step [60/263], Loss: 0.2649, Accuracy: 88.75%\n",
            "Epoch [79/250], Step [70/263], Loss: 0.2397, Accuracy: 89.38%\n",
            "Epoch [79/250], Step [80/263], Loss: 0.2623, Accuracy: 87.81%\n",
            "Epoch [79/250], Step [90/263], Loss: 0.3061, Accuracy: 87.03%\n",
            "Epoch [79/250], Step [100/263], Loss: 0.2426, Accuracy: 90.00%\n",
            "Epoch [79/250], Step [110/263], Loss: 0.2273, Accuracy: 90.78%\n",
            "Epoch [79/250], Step [120/263], Loss: 0.3096, Accuracy: 87.19%\n",
            "Epoch [79/250], Step [130/263], Loss: 0.2676, Accuracy: 89.84%\n",
            "Epoch [79/250], Step [140/263], Loss: 0.2578, Accuracy: 89.22%\n",
            "Epoch [79/250], Step [150/263], Loss: 0.2549, Accuracy: 90.62%\n",
            "Epoch [79/250], Step [160/263], Loss: 0.2681, Accuracy: 88.75%\n",
            "Epoch [79/250], Step [170/263], Loss: 0.2647, Accuracy: 89.53%\n",
            "Epoch [79/250], Step [180/263], Loss: 0.2876, Accuracy: 88.28%\n",
            "Epoch [79/250], Step [190/263], Loss: 0.2211, Accuracy: 90.47%\n",
            "Epoch [79/250], Step [200/263], Loss: 0.2346, Accuracy: 90.00%\n",
            "Epoch [79/250], Step [210/263], Loss: 0.2490, Accuracy: 90.94%\n",
            "Epoch [79/250], Step [220/263], Loss: 0.2384, Accuracy: 89.69%\n",
            "Epoch [79/250], Step [230/263], Loss: 0.2667, Accuracy: 89.06%\n",
            "Epoch [79/250], Step [240/263], Loss: 0.2721, Accuracy: 88.44%\n",
            "Epoch [79/250], Step [250/263], Loss: 0.2800, Accuracy: 88.28%\n",
            "Epoch [79/250], Step [260/263], Loss: 0.3012, Accuracy: 87.97%\n",
            "Epoch [80/250], Step [10/263], Loss: 0.2299, Accuracy: 91.56%\n",
            "Epoch [80/250], Step [20/263], Loss: 0.2901, Accuracy: 87.81%\n",
            "Epoch [80/250], Step [30/263], Loss: 0.2691, Accuracy: 89.69%\n",
            "Epoch [80/250], Step [40/263], Loss: 0.2658, Accuracy: 89.22%\n",
            "Epoch [80/250], Step [50/263], Loss: 0.2258, Accuracy: 89.69%\n",
            "Epoch [80/250], Step [60/263], Loss: 0.2715, Accuracy: 89.53%\n",
            "Epoch [80/250], Step [70/263], Loss: 0.2695, Accuracy: 89.53%\n",
            "Epoch [80/250], Step [80/263], Loss: 0.2887, Accuracy: 89.38%\n",
            "Epoch [80/250], Step [90/263], Loss: 0.2660, Accuracy: 89.84%\n",
            "Epoch [80/250], Step [100/263], Loss: 0.3105, Accuracy: 87.03%\n",
            "Epoch [80/250], Step [110/263], Loss: 0.6281, Accuracy: 79.69%\n",
            "Epoch [80/250], Step [120/263], Loss: 0.6669, Accuracy: 75.47%\n",
            "Epoch [80/250], Step [130/263], Loss: 0.5531, Accuracy: 79.53%\n",
            "Epoch [80/250], Step [140/263], Loss: 0.4012, Accuracy: 84.38%\n",
            "Epoch [80/250], Step [150/263], Loss: 0.4036, Accuracy: 84.38%\n",
            "Epoch [80/250], Step [160/263], Loss: 0.3951, Accuracy: 84.69%\n",
            "Epoch [80/250], Step [170/263], Loss: 0.3519, Accuracy: 87.19%\n",
            "Epoch [80/250], Step [180/263], Loss: 0.3715, Accuracy: 84.69%\n",
            "Epoch [80/250], Step [190/263], Loss: 0.3182, Accuracy: 87.50%\n",
            "Epoch [80/250], Step [200/263], Loss: 0.3610, Accuracy: 84.22%\n",
            "Epoch [80/250], Step [210/263], Loss: 0.3210, Accuracy: 85.78%\n",
            "Epoch [80/250], Step [220/263], Loss: 0.2818, Accuracy: 88.28%\n",
            "Epoch [80/250], Step [230/263], Loss: 0.3046, Accuracy: 87.19%\n",
            "Epoch [80/250], Step [240/263], Loss: 0.2554, Accuracy: 90.00%\n",
            "Epoch [80/250], Step [250/263], Loss: 0.2699, Accuracy: 89.22%\n",
            "Epoch [80/250], Step [260/263], Loss: 0.2306, Accuracy: 90.78%\n",
            "Epoch [81/250], Step [10/263], Loss: 0.2466, Accuracy: 90.94%\n",
            "Epoch [81/250], Step [20/263], Loss: 0.2316, Accuracy: 90.94%\n",
            "Epoch [81/250], Step [30/263], Loss: 0.2522, Accuracy: 90.31%\n",
            "Epoch [81/250], Step [40/263], Loss: 0.2526, Accuracy: 89.53%\n",
            "Epoch [81/250], Step [50/263], Loss: 0.2806, Accuracy: 87.66%\n",
            "Epoch [81/250], Step [60/263], Loss: 0.2167, Accuracy: 91.72%\n",
            "Epoch [81/250], Step [70/263], Loss: 0.2525, Accuracy: 90.94%\n",
            "Epoch [81/250], Step [80/263], Loss: 0.2343, Accuracy: 90.31%\n",
            "Epoch [81/250], Step [90/263], Loss: 0.2229, Accuracy: 90.62%\n",
            "Epoch [81/250], Step [100/263], Loss: 0.2569, Accuracy: 89.06%\n",
            "Epoch [81/250], Step [110/263], Loss: 0.3066, Accuracy: 86.56%\n",
            "Epoch [81/250], Step [120/263], Loss: 0.2621, Accuracy: 89.53%\n",
            "Epoch [81/250], Step [130/263], Loss: 0.2713, Accuracy: 90.00%\n",
            "Epoch [81/250], Step [140/263], Loss: 0.2570, Accuracy: 89.53%\n",
            "Epoch [81/250], Step [150/263], Loss: 0.2565, Accuracy: 88.44%\n",
            "Epoch [81/250], Step [160/263], Loss: 0.2531, Accuracy: 90.16%\n",
            "Epoch [81/250], Step [170/263], Loss: 0.3218, Accuracy: 87.97%\n",
            "Epoch [81/250], Step [180/263], Loss: 0.2720, Accuracy: 88.28%\n",
            "Epoch [81/250], Step [190/263], Loss: 0.2973, Accuracy: 88.12%\n",
            "Epoch [81/250], Step [200/263], Loss: 0.2935, Accuracy: 86.56%\n",
            "Epoch [81/250], Step [210/263], Loss: 0.2361, Accuracy: 90.62%\n",
            "Epoch [81/250], Step [220/263], Loss: 0.2443, Accuracy: 89.53%\n",
            "Epoch [81/250], Step [230/263], Loss: 0.2600, Accuracy: 87.97%\n",
            "Epoch [81/250], Step [240/263], Loss: 0.2578, Accuracy: 88.44%\n",
            "Epoch [81/250], Step [250/263], Loss: 0.2798, Accuracy: 89.06%\n",
            "Epoch [81/250], Step [260/263], Loss: 0.2789, Accuracy: 86.56%\n",
            "Epoch [82/250], Step [10/263], Loss: 0.2882, Accuracy: 86.88%\n",
            "Epoch [82/250], Step [20/263], Loss: 0.2759, Accuracy: 89.38%\n",
            "Epoch [82/250], Step [30/263], Loss: 0.2268, Accuracy: 91.25%\n",
            "Epoch [82/250], Step [40/263], Loss: 0.2285, Accuracy: 90.94%\n",
            "Epoch [82/250], Step [50/263], Loss: 0.2531, Accuracy: 90.47%\n",
            "Epoch [82/250], Step [60/263], Loss: 0.2368, Accuracy: 90.16%\n",
            "Epoch [82/250], Step [70/263], Loss: 0.2518, Accuracy: 90.47%\n",
            "Epoch [82/250], Step [80/263], Loss: 0.2479, Accuracy: 90.00%\n",
            "Epoch [82/250], Step [90/263], Loss: 0.2561, Accuracy: 90.00%\n",
            "Epoch [82/250], Step [100/263], Loss: 0.2379, Accuracy: 90.00%\n",
            "Epoch [82/250], Step [110/263], Loss: 0.2486, Accuracy: 89.53%\n",
            "Epoch [82/250], Step [120/263], Loss: 0.2590, Accuracy: 88.28%\n",
            "Epoch [82/250], Step [130/263], Loss: 0.2475, Accuracy: 90.47%\n",
            "Epoch [82/250], Step [140/263], Loss: 0.2154, Accuracy: 90.78%\n",
            "Epoch [82/250], Step [150/263], Loss: 0.3307, Accuracy: 87.03%\n",
            "Epoch [82/250], Step [160/263], Loss: 0.2572, Accuracy: 89.69%\n",
            "Epoch [82/250], Step [170/263], Loss: 0.2839, Accuracy: 86.41%\n",
            "Epoch [82/250], Step [180/263], Loss: 0.2302, Accuracy: 90.00%\n",
            "Epoch [82/250], Step [190/263], Loss: 0.2691, Accuracy: 89.22%\n",
            "Epoch [82/250], Step [200/263], Loss: 0.2240, Accuracy: 91.41%\n",
            "Epoch [82/250], Step [210/263], Loss: 0.2339, Accuracy: 90.47%\n",
            "Epoch [82/250], Step [220/263], Loss: 0.2280, Accuracy: 91.56%\n",
            "Epoch [82/250], Step [230/263], Loss: 0.2956, Accuracy: 87.03%\n",
            "Epoch [82/250], Step [240/263], Loss: 0.2395, Accuracy: 90.47%\n",
            "Epoch [82/250], Step [250/263], Loss: 0.2670, Accuracy: 89.06%\n",
            "Epoch [82/250], Step [260/263], Loss: 0.2275, Accuracy: 90.78%\n",
            "Epoch [83/250], Step [10/263], Loss: 0.2823, Accuracy: 89.53%\n",
            "Epoch [83/250], Step [20/263], Loss: 0.2375, Accuracy: 89.53%\n",
            "Epoch [83/250], Step [30/263], Loss: 0.2558, Accuracy: 89.38%\n",
            "Epoch [83/250], Step [40/263], Loss: 0.2352, Accuracy: 90.00%\n",
            "Epoch [83/250], Step [50/263], Loss: 0.2613, Accuracy: 89.38%\n",
            "Epoch [83/250], Step [60/263], Loss: 0.2494, Accuracy: 90.47%\n",
            "Epoch [83/250], Step [70/263], Loss: 0.2483, Accuracy: 88.75%\n",
            "Epoch [83/250], Step [80/263], Loss: 0.2340, Accuracy: 90.94%\n",
            "Epoch [83/250], Step [90/263], Loss: 0.2044, Accuracy: 92.03%\n",
            "Epoch [83/250], Step [100/263], Loss: 0.2683, Accuracy: 88.28%\n",
            "Epoch [83/250], Step [110/263], Loss: 0.2417, Accuracy: 89.84%\n",
            "Epoch [83/250], Step [120/263], Loss: 0.2508, Accuracy: 90.16%\n",
            "Epoch [83/250], Step [130/263], Loss: 0.2181, Accuracy: 91.25%\n",
            "Epoch [83/250], Step [140/263], Loss: 0.2896, Accuracy: 88.75%\n",
            "Epoch [83/250], Step [150/263], Loss: 0.2450, Accuracy: 90.16%\n",
            "Epoch [83/250], Step [160/263], Loss: 0.2002, Accuracy: 91.41%\n",
            "Epoch [83/250], Step [170/263], Loss: 0.2406, Accuracy: 90.47%\n",
            "Epoch [83/250], Step [180/263], Loss: 0.2556, Accuracy: 89.38%\n",
            "Epoch [83/250], Step [190/263], Loss: 0.2640, Accuracy: 90.00%\n",
            "Epoch [83/250], Step [200/263], Loss: 0.2509, Accuracy: 88.28%\n",
            "Epoch [83/250], Step [210/263], Loss: 0.2558, Accuracy: 90.16%\n",
            "Epoch [83/250], Step [220/263], Loss: 0.2446, Accuracy: 90.16%\n",
            "Epoch [83/250], Step [230/263], Loss: 0.2393, Accuracy: 90.94%\n",
            "Epoch [83/250], Step [240/263], Loss: 0.2216, Accuracy: 90.78%\n",
            "Epoch [83/250], Step [250/263], Loss: 0.2531, Accuracy: 89.84%\n",
            "Epoch [83/250], Step [260/263], Loss: 0.2316, Accuracy: 89.84%\n",
            "Epoch [84/250], Step [10/263], Loss: 0.2344, Accuracy: 90.16%\n",
            "Epoch [84/250], Step [20/263], Loss: 0.2623, Accuracy: 89.53%\n",
            "Epoch [84/250], Step [30/263], Loss: 0.2602, Accuracy: 89.38%\n",
            "Epoch [84/250], Step [40/263], Loss: 0.2024, Accuracy: 93.44%\n",
            "Epoch [84/250], Step [50/263], Loss: 0.2073, Accuracy: 91.72%\n",
            "Epoch [84/250], Step [60/263], Loss: 0.2324, Accuracy: 90.00%\n",
            "Epoch [84/250], Step [70/263], Loss: 0.2664, Accuracy: 89.84%\n",
            "Epoch [84/250], Step [80/263], Loss: 0.2472, Accuracy: 90.94%\n",
            "Epoch [84/250], Step [90/263], Loss: 0.2430, Accuracy: 90.00%\n",
            "Epoch [84/250], Step [100/263], Loss: 0.2753, Accuracy: 88.75%\n",
            "Epoch [84/250], Step [110/263], Loss: 0.2743, Accuracy: 89.53%\n",
            "Epoch [84/250], Step [120/263], Loss: 0.2726, Accuracy: 89.84%\n",
            "Epoch [84/250], Step [130/263], Loss: 0.2441, Accuracy: 90.47%\n",
            "Epoch [84/250], Step [140/263], Loss: 0.2283, Accuracy: 89.53%\n",
            "Epoch [84/250], Step [150/263], Loss: 0.2566, Accuracy: 89.06%\n",
            "Epoch [84/250], Step [160/263], Loss: 0.2642, Accuracy: 89.53%\n",
            "Epoch [84/250], Step [170/263], Loss: 0.2385, Accuracy: 90.00%\n",
            "Epoch [84/250], Step [180/263], Loss: 0.2520, Accuracy: 88.59%\n",
            "Epoch [84/250], Step [190/263], Loss: 0.2532, Accuracy: 89.69%\n",
            "Epoch [84/250], Step [200/263], Loss: 0.2730, Accuracy: 87.66%\n",
            "Epoch [84/250], Step [210/263], Loss: 0.2696, Accuracy: 88.44%\n",
            "Epoch [84/250], Step [220/263], Loss: 0.2534, Accuracy: 89.53%\n",
            "Epoch [84/250], Step [230/263], Loss: 0.2305, Accuracy: 90.62%\n",
            "Epoch [84/250], Step [240/263], Loss: 0.2774, Accuracy: 88.44%\n",
            "Epoch [84/250], Step [250/263], Loss: 0.2207, Accuracy: 91.72%\n",
            "Epoch [84/250], Step [260/263], Loss: 0.2281, Accuracy: 91.41%\n",
            "Epoch [85/250], Step [10/263], Loss: 0.2139, Accuracy: 91.41%\n",
            "Epoch [85/250], Step [20/263], Loss: 0.2347, Accuracy: 90.31%\n",
            "Epoch [85/250], Step [30/263], Loss: 0.2731, Accuracy: 87.66%\n",
            "Epoch [85/250], Step [40/263], Loss: 0.2544, Accuracy: 88.59%\n",
            "Epoch [85/250], Step [50/263], Loss: 0.2340, Accuracy: 90.47%\n",
            "Epoch [85/250], Step [60/263], Loss: 0.2493, Accuracy: 88.91%\n",
            "Epoch [85/250], Step [70/263], Loss: 0.2267, Accuracy: 91.09%\n",
            "Epoch [85/250], Step [80/263], Loss: 0.2236, Accuracy: 91.88%\n",
            "Epoch [85/250], Step [90/263], Loss: 0.2338, Accuracy: 90.00%\n",
            "Epoch [85/250], Step [100/263], Loss: 0.2141, Accuracy: 90.47%\n",
            "Epoch [85/250], Step [110/263], Loss: 0.2361, Accuracy: 90.62%\n",
            "Epoch [85/250], Step [120/263], Loss: 0.2462, Accuracy: 90.94%\n",
            "Epoch [85/250], Step [130/263], Loss: 0.2532, Accuracy: 89.22%\n",
            "Epoch [85/250], Step [140/263], Loss: 0.2823, Accuracy: 88.28%\n",
            "Epoch [85/250], Step [150/263], Loss: 0.2485, Accuracy: 89.69%\n",
            "Epoch [85/250], Step [160/263], Loss: 0.2591, Accuracy: 89.69%\n",
            "Epoch [85/250], Step [170/263], Loss: 0.2432, Accuracy: 90.62%\n",
            "Epoch [85/250], Step [180/263], Loss: 0.2680, Accuracy: 88.91%\n",
            "Epoch [85/250], Step [190/263], Loss: 0.2314, Accuracy: 90.31%\n",
            "Epoch [85/250], Step [200/263], Loss: 0.2759, Accuracy: 88.12%\n",
            "Epoch [85/250], Step [210/263], Loss: 0.2495, Accuracy: 89.22%\n",
            "Epoch [85/250], Step [220/263], Loss: 0.2898, Accuracy: 89.53%\n",
            "Epoch [85/250], Step [230/263], Loss: 0.2748, Accuracy: 88.12%\n",
            "Epoch [85/250], Step [240/263], Loss: 0.2577, Accuracy: 89.38%\n",
            "Epoch [85/250], Step [250/263], Loss: 0.2611, Accuracy: 88.12%\n",
            "Epoch [85/250], Step [260/263], Loss: 0.2494, Accuracy: 88.91%\n",
            "Epoch [86/250], Step [10/263], Loss: 0.2297, Accuracy: 92.03%\n",
            "Epoch [86/250], Step [20/263], Loss: 0.2271, Accuracy: 89.69%\n",
            "Epoch [86/250], Step [30/263], Loss: 0.2080, Accuracy: 92.34%\n",
            "Epoch [86/250], Step [40/263], Loss: 0.1815, Accuracy: 92.50%\n",
            "Epoch [86/250], Step [50/263], Loss: 0.2266, Accuracy: 90.94%\n",
            "Epoch [86/250], Step [60/263], Loss: 0.2476, Accuracy: 88.75%\n",
            "Epoch [86/250], Step [70/263], Loss: 0.2576, Accuracy: 90.31%\n",
            "Epoch [86/250], Step [80/263], Loss: 0.2397, Accuracy: 90.78%\n",
            "Epoch [86/250], Step [90/263], Loss: 0.2518, Accuracy: 89.69%\n",
            "Epoch [86/250], Step [100/263], Loss: 0.2423, Accuracy: 89.53%\n",
            "Epoch [86/250], Step [110/263], Loss: 0.2638, Accuracy: 88.44%\n",
            "Epoch [86/250], Step [120/263], Loss: 0.2076, Accuracy: 91.25%\n",
            "Epoch [86/250], Step [130/263], Loss: 0.2074, Accuracy: 92.03%\n",
            "Epoch [86/250], Step [140/263], Loss: 0.2770, Accuracy: 88.75%\n",
            "Epoch [86/250], Step [150/263], Loss: 0.2257, Accuracy: 90.78%\n",
            "Epoch [86/250], Step [160/263], Loss: 0.2887, Accuracy: 88.12%\n",
            "Epoch [86/250], Step [170/263], Loss: 0.2356, Accuracy: 90.78%\n",
            "Epoch [86/250], Step [180/263], Loss: 0.2051, Accuracy: 92.19%\n",
            "Epoch [86/250], Step [190/263], Loss: 0.2665, Accuracy: 90.31%\n",
            "Epoch [86/250], Step [200/263], Loss: 0.2533, Accuracy: 89.22%\n",
            "Epoch [86/250], Step [210/263], Loss: 0.2604, Accuracy: 88.75%\n",
            "Epoch [86/250], Step [220/263], Loss: 0.2196, Accuracy: 89.84%\n",
            "Epoch [86/250], Step [230/263], Loss: 0.2605, Accuracy: 88.59%\n",
            "Epoch [86/250], Step [240/263], Loss: 0.2315, Accuracy: 91.09%\n",
            "Epoch [86/250], Step [250/263], Loss: 0.2679, Accuracy: 89.38%\n",
            "Epoch [86/250], Step [260/263], Loss: 0.2228, Accuracy: 90.78%\n",
            "Epoch [87/250], Step [10/263], Loss: 0.2191, Accuracy: 91.09%\n",
            "Epoch [87/250], Step [20/263], Loss: 0.2514, Accuracy: 89.69%\n",
            "Epoch [87/250], Step [30/263], Loss: 0.1981, Accuracy: 92.34%\n",
            "Epoch [87/250], Step [40/263], Loss: 0.2385, Accuracy: 90.16%\n",
            "Epoch [87/250], Step [50/263], Loss: 0.1946, Accuracy: 92.66%\n",
            "Epoch [87/250], Step [60/263], Loss: 0.2116, Accuracy: 91.09%\n",
            "Epoch [87/250], Step [70/263], Loss: 0.2431, Accuracy: 90.78%\n",
            "Epoch [87/250], Step [80/263], Loss: 0.2306, Accuracy: 91.09%\n",
            "Epoch [87/250], Step [90/263], Loss: 0.2209, Accuracy: 91.25%\n",
            "Epoch [87/250], Step [100/263], Loss: 0.2513, Accuracy: 90.31%\n",
            "Epoch [87/250], Step [110/263], Loss: 0.2605, Accuracy: 88.75%\n",
            "Epoch [87/250], Step [120/263], Loss: 0.2437, Accuracy: 90.47%\n",
            "Epoch [87/250], Step [130/263], Loss: 0.2248, Accuracy: 92.19%\n",
            "Epoch [87/250], Step [140/263], Loss: 0.2574, Accuracy: 89.22%\n",
            "Epoch [87/250], Step [150/263], Loss: 0.2567, Accuracy: 88.91%\n",
            "Epoch [87/250], Step [160/263], Loss: 0.2336, Accuracy: 90.94%\n",
            "Epoch [87/250], Step [170/263], Loss: 0.2463, Accuracy: 90.62%\n",
            "Epoch [87/250], Step [180/263], Loss: 0.2925, Accuracy: 87.50%\n",
            "Epoch [87/250], Step [190/263], Loss: 0.3113, Accuracy: 87.19%\n",
            "Epoch [87/250], Step [200/263], Loss: 0.3358, Accuracy: 86.41%\n",
            "Epoch [87/250], Step [210/263], Loss: 0.3239, Accuracy: 87.03%\n",
            "Epoch [87/250], Step [220/263], Loss: 0.2956, Accuracy: 88.75%\n",
            "Epoch [87/250], Step [230/263], Loss: 0.2268, Accuracy: 90.00%\n",
            "Epoch [87/250], Step [240/263], Loss: 0.2299, Accuracy: 91.25%\n",
            "Epoch [87/250], Step [250/263], Loss: 0.2742, Accuracy: 89.22%\n",
            "Epoch [87/250], Step [260/263], Loss: 0.3304, Accuracy: 88.44%\n",
            "Epoch [88/250], Step [10/263], Loss: 0.2744, Accuracy: 88.44%\n",
            "Epoch [88/250], Step [20/263], Loss: 0.2422, Accuracy: 90.78%\n",
            "Epoch [88/250], Step [30/263], Loss: 0.2306, Accuracy: 88.91%\n",
            "Epoch [88/250], Step [40/263], Loss: 0.2157, Accuracy: 90.78%\n",
            "Epoch [88/250], Step [50/263], Loss: 0.2281, Accuracy: 91.88%\n",
            "Epoch [88/250], Step [60/263], Loss: 0.2180, Accuracy: 90.62%\n",
            "Epoch [88/250], Step [70/263], Loss: 0.2431, Accuracy: 90.47%\n",
            "Epoch [88/250], Step [80/263], Loss: 0.2286, Accuracy: 90.47%\n",
            "Epoch [88/250], Step [90/263], Loss: 0.1913, Accuracy: 92.81%\n",
            "Epoch [88/250], Step [100/263], Loss: 0.2287, Accuracy: 90.31%\n",
            "Epoch [88/250], Step [110/263], Loss: 0.2043, Accuracy: 92.66%\n",
            "Epoch [88/250], Step [120/263], Loss: 0.2475, Accuracy: 89.69%\n",
            "Epoch [88/250], Step [130/263], Loss: 0.2687, Accuracy: 89.38%\n",
            "Epoch [88/250], Step [140/263], Loss: 0.2620, Accuracy: 91.09%\n",
            "Epoch [88/250], Step [150/263], Loss: 0.2279, Accuracy: 90.78%\n",
            "Epoch [88/250], Step [160/263], Loss: 0.2649, Accuracy: 87.81%\n",
            "Epoch [88/250], Step [170/263], Loss: 0.2960, Accuracy: 88.44%\n",
            "Epoch [88/250], Step [180/263], Loss: 0.2739, Accuracy: 88.75%\n",
            "Epoch [88/250], Step [190/263], Loss: 0.2349, Accuracy: 90.31%\n",
            "Epoch [88/250], Step [200/263], Loss: 0.2338, Accuracy: 90.16%\n",
            "Epoch [88/250], Step [210/263], Loss: 0.2479, Accuracy: 90.16%\n",
            "Epoch [88/250], Step [220/263], Loss: 0.2786, Accuracy: 89.06%\n",
            "Epoch [88/250], Step [230/263], Loss: 0.2651, Accuracy: 89.84%\n",
            "Epoch [88/250], Step [240/263], Loss: 0.2432, Accuracy: 89.53%\n",
            "Epoch [88/250], Step [250/263], Loss: 0.2265, Accuracy: 91.88%\n",
            "Epoch [88/250], Step [260/263], Loss: 0.2055, Accuracy: 91.88%\n",
            "Epoch [89/250], Step [10/263], Loss: 0.2123, Accuracy: 92.19%\n",
            "Epoch [89/250], Step [20/263], Loss: 0.2273, Accuracy: 90.31%\n",
            "Epoch [89/250], Step [30/263], Loss: 0.2416, Accuracy: 90.16%\n",
            "Epoch [89/250], Step [40/263], Loss: 0.2170, Accuracy: 90.62%\n",
            "Epoch [89/250], Step [50/263], Loss: 0.2249, Accuracy: 90.47%\n",
            "Epoch [89/250], Step [60/263], Loss: 0.2059, Accuracy: 91.56%\n",
            "Epoch [89/250], Step [70/263], Loss: 0.2350, Accuracy: 90.16%\n",
            "Epoch [89/250], Step [80/263], Loss: 0.1948, Accuracy: 91.56%\n",
            "Epoch [89/250], Step [90/263], Loss: 0.2690, Accuracy: 89.84%\n",
            "Epoch [89/250], Step [100/263], Loss: 0.2322, Accuracy: 91.88%\n",
            "Epoch [89/250], Step [110/263], Loss: 0.2246, Accuracy: 91.72%\n",
            "Epoch [89/250], Step [120/263], Loss: 0.2149, Accuracy: 91.56%\n",
            "Epoch [89/250], Step [130/263], Loss: 0.2696, Accuracy: 87.81%\n",
            "Epoch [89/250], Step [140/263], Loss: 0.2164, Accuracy: 91.56%\n",
            "Epoch [89/250], Step [150/263], Loss: 0.2429, Accuracy: 91.25%\n",
            "Epoch [89/250], Step [160/263], Loss: 0.2315, Accuracy: 91.09%\n",
            "Epoch [89/250], Step [170/263], Loss: 0.2341, Accuracy: 89.53%\n",
            "Epoch [89/250], Step [180/263], Loss: 0.2355, Accuracy: 90.47%\n",
            "Epoch [89/250], Step [190/263], Loss: 0.2431, Accuracy: 90.16%\n",
            "Epoch [89/250], Step [200/263], Loss: 0.2279, Accuracy: 90.62%\n",
            "Epoch [89/250], Step [210/263], Loss: 0.2537, Accuracy: 89.38%\n",
            "Epoch [89/250], Step [220/263], Loss: 0.2507, Accuracy: 89.38%\n",
            "Epoch [89/250], Step [230/263], Loss: 0.2472, Accuracy: 89.22%\n",
            "Epoch [89/250], Step [240/263], Loss: 0.2053, Accuracy: 92.03%\n",
            "Epoch [89/250], Step [250/263], Loss: 0.2413, Accuracy: 90.47%\n",
            "Epoch [89/250], Step [260/263], Loss: 0.2523, Accuracy: 90.47%\n",
            "Epoch [90/250], Step [10/263], Loss: 0.2298, Accuracy: 91.72%\n",
            "Epoch [90/250], Step [20/263], Loss: 0.2007, Accuracy: 92.50%\n",
            "Epoch [90/250], Step [30/263], Loss: 0.2159, Accuracy: 91.56%\n",
            "Epoch [90/250], Step [40/263], Loss: 0.2389, Accuracy: 90.00%\n",
            "Epoch [90/250], Step [50/263], Loss: 0.2069, Accuracy: 91.88%\n",
            "Epoch [90/250], Step [60/263], Loss: 0.2089, Accuracy: 91.56%\n",
            "Epoch [90/250], Step [70/263], Loss: 0.2551, Accuracy: 89.38%\n",
            "Epoch [90/250], Step [80/263], Loss: 0.2314, Accuracy: 90.78%\n",
            "Epoch [90/250], Step [90/263], Loss: 0.2562, Accuracy: 88.28%\n",
            "Epoch [90/250], Step [100/263], Loss: 0.2811, Accuracy: 87.34%\n",
            "Epoch [90/250], Step [110/263], Loss: 0.2424, Accuracy: 90.16%\n",
            "Epoch [90/250], Step [120/263], Loss: 0.2842, Accuracy: 88.28%\n",
            "Epoch [90/250], Step [130/263], Loss: 0.2247, Accuracy: 90.31%\n",
            "Epoch [90/250], Step [140/263], Loss: 0.2032, Accuracy: 91.88%\n",
            "Epoch [90/250], Step [150/263], Loss: 0.1894, Accuracy: 93.44%\n",
            "Epoch [90/250], Step [160/263], Loss: 0.2143, Accuracy: 92.03%\n",
            "Epoch [90/250], Step [170/263], Loss: 0.2622, Accuracy: 89.84%\n",
            "Epoch [90/250], Step [180/263], Loss: 0.2808, Accuracy: 89.22%\n",
            "Epoch [90/250], Step [190/263], Loss: 0.2499, Accuracy: 90.31%\n",
            "Epoch [90/250], Step [200/263], Loss: 0.2272, Accuracy: 90.47%\n",
            "Epoch [90/250], Step [210/263], Loss: 0.2137, Accuracy: 90.31%\n",
            "Epoch [90/250], Step [220/263], Loss: 0.2131, Accuracy: 92.03%\n",
            "Epoch [90/250], Step [230/263], Loss: 0.2438, Accuracy: 90.31%\n",
            "Epoch [90/250], Step [240/263], Loss: 0.2235, Accuracy: 90.62%\n",
            "Epoch [90/250], Step [250/263], Loss: 0.2403, Accuracy: 91.25%\n",
            "Epoch [90/250], Step [260/263], Loss: 0.2373, Accuracy: 90.47%\n",
            "Epoch [91/250], Step [10/263], Loss: 0.2354, Accuracy: 90.00%\n",
            "Epoch [91/250], Step [20/263], Loss: 0.1990, Accuracy: 92.03%\n",
            "Epoch [91/250], Step [30/263], Loss: 0.1894, Accuracy: 92.19%\n",
            "Epoch [91/250], Step [40/263], Loss: 0.2287, Accuracy: 90.31%\n",
            "Epoch [91/250], Step [50/263], Loss: 0.1884, Accuracy: 92.50%\n",
            "Epoch [91/250], Step [60/263], Loss: 0.2279, Accuracy: 90.31%\n",
            "Epoch [91/250], Step [70/263], Loss: 0.2424, Accuracy: 89.69%\n",
            "Epoch [91/250], Step [80/263], Loss: 0.2143, Accuracy: 92.19%\n",
            "Epoch [91/250], Step [90/263], Loss: 0.2328, Accuracy: 91.09%\n",
            "Epoch [91/250], Step [100/263], Loss: 0.2219, Accuracy: 90.62%\n",
            "Epoch [91/250], Step [110/263], Loss: 0.1747, Accuracy: 92.66%\n",
            "Epoch [91/250], Step [120/263], Loss: 0.2428, Accuracy: 90.94%\n",
            "Epoch [91/250], Step [130/263], Loss: 0.2254, Accuracy: 91.72%\n",
            "Epoch [91/250], Step [140/263], Loss: 0.2472, Accuracy: 90.31%\n",
            "Epoch [91/250], Step [150/263], Loss: 0.2682, Accuracy: 89.38%\n",
            "Epoch [91/250], Step [160/263], Loss: 0.2480, Accuracy: 89.84%\n",
            "Epoch [91/250], Step [170/263], Loss: 0.2432, Accuracy: 88.59%\n",
            "Epoch [91/250], Step [180/263], Loss: 0.2200, Accuracy: 90.78%\n",
            "Epoch [91/250], Step [190/263], Loss: 0.2595, Accuracy: 89.22%\n",
            "Epoch [91/250], Step [200/263], Loss: 0.1777, Accuracy: 93.44%\n",
            "Epoch [91/250], Step [210/263], Loss: 0.2137, Accuracy: 92.03%\n",
            "Epoch [91/250], Step [220/263], Loss: 0.2249, Accuracy: 90.62%\n",
            "Epoch [91/250], Step [230/263], Loss: 0.2443, Accuracy: 91.88%\n",
            "Epoch [91/250], Step [240/263], Loss: 0.2621, Accuracy: 89.22%\n",
            "Epoch [91/250], Step [250/263], Loss: 0.2405, Accuracy: 89.69%\n",
            "Epoch [91/250], Step [260/263], Loss: 0.2463, Accuracy: 89.53%\n",
            "Epoch [92/250], Step [10/263], Loss: 0.2374, Accuracy: 90.00%\n",
            "Epoch [92/250], Step [20/263], Loss: 0.2121, Accuracy: 90.94%\n",
            "Epoch [92/250], Step [30/263], Loss: 0.1937, Accuracy: 91.41%\n",
            "Epoch [92/250], Step [40/263], Loss: 0.2030, Accuracy: 92.66%\n",
            "Epoch [92/250], Step [50/263], Loss: 0.1961, Accuracy: 92.19%\n",
            "Epoch [92/250], Step [60/263], Loss: 0.2383, Accuracy: 89.69%\n",
            "Epoch [92/250], Step [70/263], Loss: 0.2056, Accuracy: 90.94%\n",
            "Epoch [92/250], Step [80/263], Loss: 0.2412, Accuracy: 89.38%\n",
            "Epoch [92/250], Step [90/263], Loss: 0.2326, Accuracy: 89.84%\n",
            "Epoch [92/250], Step [100/263], Loss: 0.2342, Accuracy: 89.22%\n",
            "Epoch [92/250], Step [110/263], Loss: 0.2278, Accuracy: 90.31%\n",
            "Epoch [92/250], Step [120/263], Loss: 0.2817, Accuracy: 89.06%\n",
            "Epoch [92/250], Step [130/263], Loss: 0.2532, Accuracy: 89.53%\n",
            "Epoch [92/250], Step [140/263], Loss: 0.2969, Accuracy: 87.19%\n",
            "Epoch [92/250], Step [150/263], Loss: 0.2778, Accuracy: 89.06%\n",
            "Epoch [92/250], Step [160/263], Loss: 0.2749, Accuracy: 88.75%\n",
            "Epoch [92/250], Step [170/263], Loss: 0.2230, Accuracy: 91.88%\n",
            "Epoch [92/250], Step [180/263], Loss: 0.2235, Accuracy: 90.00%\n",
            "Epoch [92/250], Step [190/263], Loss: 0.2117, Accuracy: 91.41%\n",
            "Epoch [92/250], Step [200/263], Loss: 0.2436, Accuracy: 90.16%\n",
            "Epoch [92/250], Step [210/263], Loss: 0.2168, Accuracy: 90.31%\n",
            "Epoch [92/250], Step [220/263], Loss: 0.2157, Accuracy: 91.41%\n",
            "Epoch [92/250], Step [230/263], Loss: 0.2088, Accuracy: 91.09%\n",
            "Epoch [92/250], Step [240/263], Loss: 0.2250, Accuracy: 91.09%\n",
            "Epoch [92/250], Step [250/263], Loss: 0.2516, Accuracy: 90.00%\n",
            "Epoch [92/250], Step [260/263], Loss: 0.2638, Accuracy: 87.81%\n",
            "Epoch [93/250], Step [10/263], Loss: 0.2190, Accuracy: 91.41%\n",
            "Epoch [93/250], Step [20/263], Loss: 0.2258, Accuracy: 90.47%\n",
            "Epoch [93/250], Step [30/263], Loss: 0.2526, Accuracy: 90.94%\n",
            "Epoch [93/250], Step [40/263], Loss: 0.2024, Accuracy: 91.88%\n",
            "Epoch [93/250], Step [50/263], Loss: 0.2321, Accuracy: 90.16%\n",
            "Epoch [93/250], Step [60/263], Loss: 0.2138, Accuracy: 91.25%\n",
            "Epoch [93/250], Step [70/263], Loss: 0.2544, Accuracy: 89.53%\n",
            "Epoch [93/250], Step [80/263], Loss: 0.2508, Accuracy: 88.91%\n",
            "Epoch [93/250], Step [90/263], Loss: 0.2365, Accuracy: 91.25%\n",
            "Epoch [93/250], Step [100/263], Loss: 0.2136, Accuracy: 90.31%\n",
            "Epoch [93/250], Step [110/263], Loss: 0.1978, Accuracy: 91.72%\n",
            "Epoch [93/250], Step [120/263], Loss: 0.2652, Accuracy: 88.91%\n",
            "Epoch [93/250], Step [130/263], Loss: 0.1908, Accuracy: 93.12%\n",
            "Epoch [93/250], Step [140/263], Loss: 0.2287, Accuracy: 89.84%\n",
            "Epoch [93/250], Step [150/263], Loss: 0.2267, Accuracy: 91.09%\n",
            "Epoch [93/250], Step [160/263], Loss: 0.2229, Accuracy: 90.16%\n",
            "Epoch [93/250], Step [170/263], Loss: 0.2283, Accuracy: 90.94%\n",
            "Epoch [93/250], Step [180/263], Loss: 0.1943, Accuracy: 91.88%\n",
            "Epoch [93/250], Step [190/263], Loss: 0.2058, Accuracy: 91.56%\n",
            "Epoch [93/250], Step [200/263], Loss: 0.2319, Accuracy: 90.94%\n",
            "Epoch [93/250], Step [210/263], Loss: 0.2196, Accuracy: 91.56%\n",
            "Epoch [93/250], Step [220/263], Loss: 0.2474, Accuracy: 88.44%\n",
            "Epoch [93/250], Step [230/263], Loss: 0.2473, Accuracy: 89.84%\n",
            "Epoch [93/250], Step [240/263], Loss: 0.2137, Accuracy: 90.94%\n",
            "Epoch [93/250], Step [250/263], Loss: 0.2388, Accuracy: 90.78%\n",
            "Epoch [93/250], Step [260/263], Loss: 0.1944, Accuracy: 92.34%\n",
            "Epoch [94/250], Step [10/263], Loss: 0.1830, Accuracy: 92.81%\n",
            "Epoch [94/250], Step [20/263], Loss: 0.2326, Accuracy: 90.62%\n",
            "Epoch [94/250], Step [30/263], Loss: 0.2462, Accuracy: 90.00%\n",
            "Epoch [94/250], Step [40/263], Loss: 0.2366, Accuracy: 90.94%\n",
            "Epoch [94/250], Step [50/263], Loss: 0.2065, Accuracy: 92.50%\n",
            "Epoch [94/250], Step [60/263], Loss: 0.2170, Accuracy: 91.41%\n",
            "Epoch [94/250], Step [70/263], Loss: 0.2190, Accuracy: 91.25%\n",
            "Epoch [94/250], Step [80/263], Loss: 0.1876, Accuracy: 92.66%\n",
            "Epoch [94/250], Step [90/263], Loss: 0.1874, Accuracy: 92.19%\n",
            "Epoch [94/250], Step [100/263], Loss: 0.2425, Accuracy: 90.00%\n",
            "Epoch [94/250], Step [110/263], Loss: 0.2351, Accuracy: 90.31%\n",
            "Epoch [94/250], Step [120/263], Loss: 0.2173, Accuracy: 91.88%\n",
            "Epoch [94/250], Step [130/263], Loss: 0.2159, Accuracy: 90.16%\n",
            "Epoch [94/250], Step [140/263], Loss: 0.1737, Accuracy: 93.44%\n",
            "Epoch [94/250], Step [150/263], Loss: 0.2729, Accuracy: 88.28%\n",
            "Epoch [94/250], Step [160/263], Loss: 0.2316, Accuracy: 90.16%\n",
            "Epoch [94/250], Step [170/263], Loss: 0.2573, Accuracy: 90.00%\n",
            "Epoch [94/250], Step [180/263], Loss: 0.2514, Accuracy: 89.38%\n",
            "Epoch [94/250], Step [190/263], Loss: 0.2409, Accuracy: 91.09%\n",
            "Epoch [94/250], Step [200/263], Loss: 0.2389, Accuracy: 91.09%\n",
            "Epoch [94/250], Step [210/263], Loss: 0.2123, Accuracy: 91.72%\n",
            "Epoch [94/250], Step [220/263], Loss: 0.2081, Accuracy: 91.88%\n",
            "Epoch [94/250], Step [230/263], Loss: 0.2242, Accuracy: 91.41%\n",
            "Epoch [94/250], Step [240/263], Loss: 0.2693, Accuracy: 88.59%\n",
            "Epoch [94/250], Step [250/263], Loss: 0.2292, Accuracy: 90.00%\n",
            "Epoch [94/250], Step [260/263], Loss: 0.2217, Accuracy: 90.31%\n",
            "Epoch [95/250], Step [10/263], Loss: 0.2176, Accuracy: 90.16%\n",
            "Epoch [95/250], Step [20/263], Loss: 0.2420, Accuracy: 90.47%\n",
            "Epoch [95/250], Step [30/263], Loss: 0.2119, Accuracy: 90.00%\n",
            "Epoch [95/250], Step [40/263], Loss: 0.2521, Accuracy: 88.75%\n",
            "Epoch [95/250], Step [50/263], Loss: 0.2009, Accuracy: 92.03%\n",
            "Epoch [95/250], Step [60/263], Loss: 0.2220, Accuracy: 90.62%\n",
            "Epoch [95/250], Step [70/263], Loss: 0.2018, Accuracy: 91.56%\n",
            "Epoch [95/250], Step [80/263], Loss: 0.2436, Accuracy: 89.84%\n",
            "Epoch [95/250], Step [90/263], Loss: 0.2159, Accuracy: 91.25%\n",
            "Epoch [95/250], Step [100/263], Loss: 0.2040, Accuracy: 91.41%\n",
            "Epoch [95/250], Step [110/263], Loss: 0.2308, Accuracy: 90.31%\n",
            "Epoch [95/250], Step [120/263], Loss: 0.2427, Accuracy: 90.31%\n",
            "Epoch [95/250], Step [130/263], Loss: 0.2311, Accuracy: 91.25%\n",
            "Epoch [95/250], Step [140/263], Loss: 0.2215, Accuracy: 90.62%\n",
            "Epoch [95/250], Step [150/263], Loss: 0.2148, Accuracy: 91.09%\n",
            "Epoch [95/250], Step [160/263], Loss: 0.2257, Accuracy: 89.69%\n",
            "Epoch [95/250], Step [170/263], Loss: 0.2365, Accuracy: 90.62%\n",
            "Epoch [95/250], Step [180/263], Loss: 0.2300, Accuracy: 90.00%\n",
            "Epoch [95/250], Step [190/263], Loss: 0.2105, Accuracy: 91.56%\n",
            "Epoch [95/250], Step [200/263], Loss: 0.2573, Accuracy: 89.84%\n",
            "Epoch [95/250], Step [210/263], Loss: 0.2212, Accuracy: 92.03%\n",
            "Epoch [95/250], Step [220/263], Loss: 0.2419, Accuracy: 89.06%\n",
            "Epoch [95/250], Step [230/263], Loss: 0.2255, Accuracy: 91.41%\n",
            "Epoch [95/250], Step [240/263], Loss: 0.2353, Accuracy: 90.00%\n",
            "Epoch [95/250], Step [250/263], Loss: 0.2145, Accuracy: 91.41%\n",
            "Epoch [95/250], Step [260/263], Loss: 0.2203, Accuracy: 91.25%\n",
            "Epoch [96/250], Step [10/263], Loss: 0.2415, Accuracy: 89.69%\n",
            "Epoch [96/250], Step [20/263], Loss: 0.2085, Accuracy: 92.50%\n",
            "Epoch [96/250], Step [30/263], Loss: 0.2177, Accuracy: 91.25%\n",
            "Epoch [96/250], Step [40/263], Loss: 0.2487, Accuracy: 89.84%\n",
            "Epoch [96/250], Step [50/263], Loss: 0.2061, Accuracy: 92.34%\n",
            "Epoch [96/250], Step [60/263], Loss: 0.2288, Accuracy: 91.41%\n",
            "Epoch [96/250], Step [70/263], Loss: 0.2110, Accuracy: 91.72%\n",
            "Epoch [96/250], Step [80/263], Loss: 0.2234, Accuracy: 90.94%\n",
            "Epoch [96/250], Step [90/263], Loss: 0.2137, Accuracy: 91.25%\n",
            "Epoch [96/250], Step [100/263], Loss: 0.2216, Accuracy: 90.47%\n",
            "Epoch [96/250], Step [110/263], Loss: 0.2394, Accuracy: 91.09%\n",
            "Epoch [96/250], Step [120/263], Loss: 0.2033, Accuracy: 91.25%\n",
            "Epoch [96/250], Step [130/263], Loss: 0.2188, Accuracy: 91.41%\n",
            "Epoch [96/250], Step [140/263], Loss: 0.2267, Accuracy: 89.53%\n",
            "Epoch [96/250], Step [150/263], Loss: 0.2225, Accuracy: 91.09%\n",
            "Epoch [96/250], Step [160/263], Loss: 0.1898, Accuracy: 91.88%\n",
            "Epoch [96/250], Step [170/263], Loss: 0.2026, Accuracy: 91.41%\n",
            "Epoch [96/250], Step [180/263], Loss: 0.2074, Accuracy: 90.94%\n",
            "Epoch [96/250], Step [190/263], Loss: 0.2281, Accuracy: 90.62%\n",
            "Epoch [96/250], Step [200/263], Loss: 0.1891, Accuracy: 91.88%\n",
            "Epoch [96/250], Step [210/263], Loss: 0.2132, Accuracy: 91.88%\n",
            "Epoch [96/250], Step [220/263], Loss: 0.2021, Accuracy: 91.88%\n",
            "Epoch [96/250], Step [230/263], Loss: 0.1954, Accuracy: 92.66%\n",
            "Epoch [96/250], Step [240/263], Loss: 0.2135, Accuracy: 90.94%\n",
            "Epoch [96/250], Step [250/263], Loss: 0.2139, Accuracy: 90.62%\n",
            "Epoch [96/250], Step [260/263], Loss: 0.2346, Accuracy: 91.25%\n",
            "Epoch [97/250], Step [10/263], Loss: 0.2034, Accuracy: 91.56%\n",
            "Epoch [97/250], Step [20/263], Loss: 0.2039, Accuracy: 90.94%\n",
            "Epoch [97/250], Step [30/263], Loss: 0.2098, Accuracy: 90.62%\n",
            "Epoch [97/250], Step [40/263], Loss: 0.2180, Accuracy: 92.19%\n",
            "Epoch [97/250], Step [50/263], Loss: 0.1837, Accuracy: 93.59%\n",
            "Epoch [97/250], Step [60/263], Loss: 0.2062, Accuracy: 92.19%\n",
            "Epoch [97/250], Step [70/263], Loss: 0.2008, Accuracy: 91.09%\n",
            "Epoch [97/250], Step [80/263], Loss: 0.2293, Accuracy: 91.09%\n",
            "Epoch [97/250], Step [90/263], Loss: 0.2104, Accuracy: 90.78%\n",
            "Epoch [97/250], Step [100/263], Loss: 0.1938, Accuracy: 92.03%\n",
            "Epoch [97/250], Step [110/263], Loss: 0.2240, Accuracy: 91.41%\n",
            "Epoch [97/250], Step [120/263], Loss: 0.2115, Accuracy: 91.56%\n",
            "Epoch [97/250], Step [130/263], Loss: 0.2195, Accuracy: 90.16%\n",
            "Epoch [97/250], Step [140/263], Loss: 0.1958, Accuracy: 93.12%\n",
            "Epoch [97/250], Step [150/263], Loss: 0.2192, Accuracy: 91.09%\n",
            "Epoch [97/250], Step [160/263], Loss: 0.1907, Accuracy: 92.97%\n",
            "Epoch [97/250], Step [170/263], Loss: 0.1990, Accuracy: 92.34%\n",
            "Epoch [97/250], Step [180/263], Loss: 0.1967, Accuracy: 90.78%\n",
            "Epoch [97/250], Step [190/263], Loss: 0.2321, Accuracy: 90.94%\n",
            "Epoch [97/250], Step [200/263], Loss: 0.2474, Accuracy: 89.84%\n",
            "Epoch [97/250], Step [210/263], Loss: 0.2426, Accuracy: 90.94%\n",
            "Epoch [97/250], Step [220/263], Loss: 0.2160, Accuracy: 91.56%\n",
            "Epoch [97/250], Step [230/263], Loss: 0.2040, Accuracy: 90.62%\n",
            "Epoch [97/250], Step [240/263], Loss: 0.2225, Accuracy: 90.16%\n",
            "Epoch [97/250], Step [250/263], Loss: 0.2121, Accuracy: 91.09%\n",
            "Epoch [97/250], Step [260/263], Loss: 0.2566, Accuracy: 89.69%\n",
            "Epoch [98/250], Step [10/263], Loss: 0.2051, Accuracy: 91.88%\n",
            "Epoch [98/250], Step [20/263], Loss: 0.1863, Accuracy: 92.81%\n",
            "Epoch [98/250], Step [30/263], Loss: 0.1816, Accuracy: 93.28%\n",
            "Epoch [98/250], Step [40/263], Loss: 0.1851, Accuracy: 93.75%\n",
            "Epoch [98/250], Step [50/263], Loss: 0.1725, Accuracy: 93.75%\n",
            "Epoch [98/250], Step [60/263], Loss: 0.2316, Accuracy: 90.62%\n",
            "Epoch [98/250], Step [70/263], Loss: 0.1824, Accuracy: 91.72%\n",
            "Epoch [98/250], Step [80/263], Loss: 0.2114, Accuracy: 91.56%\n",
            "Epoch [98/250], Step [90/263], Loss: 0.1797, Accuracy: 92.50%\n",
            "Epoch [98/250], Step [100/263], Loss: 0.2108, Accuracy: 91.56%\n",
            "Epoch [98/250], Step [110/263], Loss: 0.2040, Accuracy: 91.88%\n",
            "Epoch [98/250], Step [120/263], Loss: 0.2054, Accuracy: 92.50%\n",
            "Epoch [98/250], Step [130/263], Loss: 0.2437, Accuracy: 89.06%\n",
            "Epoch [98/250], Step [140/263], Loss: 0.1918, Accuracy: 92.97%\n",
            "Epoch [98/250], Step [150/263], Loss: 0.1921, Accuracy: 92.81%\n",
            "Epoch [98/250], Step [160/263], Loss: 0.2052, Accuracy: 90.00%\n",
            "Epoch [98/250], Step [170/263], Loss: 0.2278, Accuracy: 90.94%\n",
            "Epoch [98/250], Step [180/263], Loss: 0.1922, Accuracy: 92.50%\n",
            "Epoch [98/250], Step [190/263], Loss: 0.2749, Accuracy: 89.06%\n",
            "Epoch [98/250], Step [200/263], Loss: 0.2475, Accuracy: 89.84%\n",
            "Epoch [98/250], Step [210/263], Loss: 0.2290, Accuracy: 90.62%\n",
            "Epoch [98/250], Step [220/263], Loss: 0.2122, Accuracy: 91.72%\n",
            "Epoch [98/250], Step [230/263], Loss: 0.2820, Accuracy: 88.12%\n",
            "Epoch [98/250], Step [240/263], Loss: 0.2267, Accuracy: 90.94%\n",
            "Epoch [98/250], Step [250/263], Loss: 0.2158, Accuracy: 92.19%\n",
            "Epoch [98/250], Step [260/263], Loss: 0.2114, Accuracy: 92.03%\n",
            "Epoch [99/250], Step [10/263], Loss: 0.1946, Accuracy: 91.56%\n",
            "Epoch [99/250], Step [20/263], Loss: 0.1859, Accuracy: 92.03%\n",
            "Epoch [99/250], Step [30/263], Loss: 0.1800, Accuracy: 93.28%\n",
            "Epoch [99/250], Step [40/263], Loss: 0.2106, Accuracy: 92.03%\n",
            "Epoch [99/250], Step [50/263], Loss: 0.1745, Accuracy: 93.75%\n",
            "Epoch [99/250], Step [60/263], Loss: 0.2030, Accuracy: 91.56%\n",
            "Epoch [99/250], Step [70/263], Loss: 0.2272, Accuracy: 92.03%\n",
            "Epoch [99/250], Step [80/263], Loss: 0.1837, Accuracy: 92.50%\n",
            "Epoch [99/250], Step [90/263], Loss: 0.1954, Accuracy: 90.94%\n",
            "Epoch [99/250], Step [100/263], Loss: 0.2012, Accuracy: 92.66%\n",
            "Epoch [99/250], Step [110/263], Loss: 0.1546, Accuracy: 94.22%\n",
            "Epoch [99/250], Step [120/263], Loss: 0.2187, Accuracy: 91.56%\n",
            "Epoch [99/250], Step [130/263], Loss: 0.2201, Accuracy: 90.47%\n",
            "Epoch [99/250], Step [140/263], Loss: 0.2200, Accuracy: 90.16%\n",
            "Epoch [99/250], Step [150/263], Loss: 0.1984, Accuracy: 92.19%\n",
            "Epoch [99/250], Step [160/263], Loss: 0.1973, Accuracy: 92.34%\n",
            "Epoch [99/250], Step [170/263], Loss: 0.1992, Accuracy: 91.88%\n",
            "Epoch [99/250], Step [180/263], Loss: 0.1951, Accuracy: 92.81%\n",
            "Epoch [99/250], Step [190/263], Loss: 0.1996, Accuracy: 91.25%\n",
            "Epoch [99/250], Step [200/263], Loss: 0.2236, Accuracy: 91.56%\n",
            "Epoch [99/250], Step [210/263], Loss: 0.2379, Accuracy: 90.16%\n",
            "Epoch [99/250], Step [220/263], Loss: 0.2344, Accuracy: 90.31%\n",
            "Epoch [99/250], Step [230/263], Loss: 0.2291, Accuracy: 90.16%\n",
            "Epoch [99/250], Step [240/263], Loss: 0.2024, Accuracy: 91.41%\n",
            "Epoch [99/250], Step [250/263], Loss: 0.1714, Accuracy: 93.28%\n",
            "Epoch [99/250], Step [260/263], Loss: 0.2373, Accuracy: 90.62%\n",
            "Epoch [100/250], Step [10/263], Loss: 0.1950, Accuracy: 91.56%\n",
            "Epoch [100/250], Step [20/263], Loss: 0.2307, Accuracy: 91.56%\n",
            "Epoch [100/250], Step [30/263], Loss: 0.1853, Accuracy: 92.81%\n",
            "Epoch [100/250], Step [40/263], Loss: 0.1989, Accuracy: 92.19%\n",
            "Epoch [100/250], Step [50/263], Loss: 0.1781, Accuracy: 92.81%\n",
            "Epoch [100/250], Step [60/263], Loss: 0.2204, Accuracy: 91.56%\n",
            "Epoch [100/250], Step [70/263], Loss: 0.2163, Accuracy: 92.19%\n",
            "Epoch [100/250], Step [80/263], Loss: 0.1983, Accuracy: 93.28%\n",
            "Epoch [100/250], Step [90/263], Loss: 0.2089, Accuracy: 91.41%\n",
            "Epoch [100/250], Step [100/263], Loss: 0.2434, Accuracy: 90.94%\n",
            "Epoch [100/250], Step [110/263], Loss: 0.2220, Accuracy: 90.62%\n",
            "Epoch [100/250], Step [120/263], Loss: 0.2221, Accuracy: 90.78%\n",
            "Epoch [100/250], Step [130/263], Loss: 0.2132, Accuracy: 90.78%\n",
            "Epoch [100/250], Step [140/263], Loss: 0.2341, Accuracy: 89.53%\n",
            "Epoch [100/250], Step [150/263], Loss: 0.2011, Accuracy: 92.19%\n",
            "Epoch [100/250], Step [160/263], Loss: 0.2030, Accuracy: 91.41%\n",
            "Epoch [100/250], Step [170/263], Loss: 0.2264, Accuracy: 91.09%\n",
            "Epoch [100/250], Step [180/263], Loss: 0.2046, Accuracy: 90.62%\n",
            "Epoch [100/250], Step [190/263], Loss: 0.2003, Accuracy: 92.66%\n",
            "Epoch [100/250], Step [200/263], Loss: 0.2007, Accuracy: 91.41%\n",
            "Epoch [100/250], Step [210/263], Loss: 0.2104, Accuracy: 91.72%\n",
            "Epoch [100/250], Step [220/263], Loss: 0.2381, Accuracy: 91.41%\n",
            "Epoch [100/250], Step [230/263], Loss: 0.2999, Accuracy: 87.66%\n",
            "Epoch [100/250], Step [240/263], Loss: 0.3260, Accuracy: 86.25%\n",
            "Epoch [100/250], Step [250/263], Loss: 0.2658, Accuracy: 90.62%\n",
            "Epoch [100/250], Step [260/263], Loss: 0.2903, Accuracy: 88.59%\n",
            "Epoch [101/250], Step [10/263], Loss: 0.2761, Accuracy: 87.81%\n",
            "Epoch [101/250], Step [20/263], Loss: 0.2292, Accuracy: 91.09%\n",
            "Epoch [101/250], Step [30/263], Loss: 0.2264, Accuracy: 90.47%\n",
            "Epoch [101/250], Step [40/263], Loss: 0.2305, Accuracy: 90.94%\n",
            "Epoch [101/250], Step [50/263], Loss: 0.2221, Accuracy: 91.25%\n",
            "Epoch [101/250], Step [60/263], Loss: 0.1952, Accuracy: 91.25%\n",
            "Epoch [101/250], Step [70/263], Loss: 0.1816, Accuracy: 92.03%\n",
            "Epoch [101/250], Step [80/263], Loss: 0.2091, Accuracy: 90.78%\n",
            "Epoch [101/250], Step [90/263], Loss: 0.1776, Accuracy: 93.44%\n",
            "Epoch [101/250], Step [100/263], Loss: 0.1860, Accuracy: 92.34%\n",
            "Epoch [101/250], Step [110/263], Loss: 0.1805, Accuracy: 92.97%\n",
            "Epoch [101/250], Step [120/263], Loss: 0.1967, Accuracy: 92.34%\n",
            "Epoch [101/250], Step [130/263], Loss: 0.1994, Accuracy: 92.50%\n",
            "Epoch [101/250], Step [140/263], Loss: 0.2247, Accuracy: 90.78%\n",
            "Epoch [101/250], Step [150/263], Loss: 0.2074, Accuracy: 91.41%\n",
            "Epoch [101/250], Step [160/263], Loss: 0.1933, Accuracy: 92.81%\n",
            "Epoch [101/250], Step [170/263], Loss: 0.2214, Accuracy: 92.19%\n",
            "Epoch [101/250], Step [180/263], Loss: 0.1856, Accuracy: 92.66%\n",
            "Epoch [101/250], Step [190/263], Loss: 0.2075, Accuracy: 91.09%\n",
            "Epoch [101/250], Step [200/263], Loss: 0.1992, Accuracy: 92.66%\n",
            "Epoch [101/250], Step [210/263], Loss: 0.2207, Accuracy: 91.25%\n",
            "Epoch [101/250], Step [220/263], Loss: 0.2315, Accuracy: 90.31%\n",
            "Epoch [101/250], Step [230/263], Loss: 0.1885, Accuracy: 92.66%\n",
            "Epoch [101/250], Step [240/263], Loss: 0.2107, Accuracy: 90.78%\n",
            "Epoch [101/250], Step [250/263], Loss: 0.1887, Accuracy: 94.06%\n",
            "Epoch [101/250], Step [260/263], Loss: 0.1933, Accuracy: 92.34%\n",
            "Epoch [102/250], Step [10/263], Loss: 0.2255, Accuracy: 91.41%\n",
            "Epoch [102/250], Step [20/263], Loss: 0.1970, Accuracy: 90.62%\n",
            "Epoch [102/250], Step [30/263], Loss: 0.2451, Accuracy: 90.16%\n",
            "Epoch [102/250], Step [40/263], Loss: 0.2041, Accuracy: 91.09%\n",
            "Epoch [102/250], Step [50/263], Loss: 0.2224, Accuracy: 90.47%\n",
            "Epoch [102/250], Step [60/263], Loss: 0.2579, Accuracy: 89.22%\n",
            "Epoch [102/250], Step [70/263], Loss: 0.2733, Accuracy: 89.22%\n",
            "Epoch [102/250], Step [80/263], Loss: 0.2468, Accuracy: 90.16%\n",
            "Epoch [102/250], Step [90/263], Loss: 0.1834, Accuracy: 93.12%\n",
            "Epoch [102/250], Step [100/263], Loss: 0.2189, Accuracy: 91.09%\n",
            "Epoch [102/250], Step [110/263], Loss: 0.2095, Accuracy: 90.78%\n",
            "Epoch [102/250], Step [120/263], Loss: 0.2043, Accuracy: 91.25%\n",
            "Epoch [102/250], Step [130/263], Loss: 0.2011, Accuracy: 92.19%\n",
            "Epoch [102/250], Step [140/263], Loss: 0.1939, Accuracy: 92.34%\n",
            "Epoch [102/250], Step [150/263], Loss: 0.1889, Accuracy: 92.66%\n",
            "Epoch [102/250], Step [160/263], Loss: 0.2583, Accuracy: 88.12%\n",
            "Epoch [102/250], Step [170/263], Loss: 0.1952, Accuracy: 92.19%\n",
            "Epoch [102/250], Step [180/263], Loss: 0.2157, Accuracy: 91.72%\n",
            "Epoch [102/250], Step [190/263], Loss: 0.2079, Accuracy: 91.72%\n",
            "Epoch [102/250], Step [200/263], Loss: 0.1962, Accuracy: 91.09%\n",
            "Epoch [102/250], Step [210/263], Loss: 0.1931, Accuracy: 92.19%\n",
            "Epoch [102/250], Step [220/263], Loss: 0.1997, Accuracy: 91.25%\n",
            "Epoch [102/250], Step [230/263], Loss: 0.2036, Accuracy: 91.25%\n",
            "Epoch [102/250], Step [240/263], Loss: 0.1966, Accuracy: 92.34%\n",
            "Epoch [102/250], Step [250/263], Loss: 0.1982, Accuracy: 92.19%\n",
            "Epoch [102/250], Step [260/263], Loss: 0.1770, Accuracy: 92.66%\n",
            "Epoch [103/250], Step [10/263], Loss: 0.1781, Accuracy: 93.28%\n",
            "Epoch [103/250], Step [20/263], Loss: 0.1580, Accuracy: 94.06%\n",
            "Epoch [103/250], Step [30/263], Loss: 0.1686, Accuracy: 93.12%\n",
            "Epoch [103/250], Step [40/263], Loss: 0.2311, Accuracy: 90.16%\n",
            "Epoch [103/250], Step [50/263], Loss: 0.1882, Accuracy: 91.72%\n",
            "Epoch [103/250], Step [60/263], Loss: 0.1594, Accuracy: 93.44%\n",
            "Epoch [103/250], Step [70/263], Loss: 0.1994, Accuracy: 91.72%\n",
            "Epoch [103/250], Step [80/263], Loss: 0.1878, Accuracy: 92.34%\n",
            "Epoch [103/250], Step [90/263], Loss: 0.2116, Accuracy: 91.72%\n",
            "Epoch [103/250], Step [100/263], Loss: 0.1896, Accuracy: 92.81%\n",
            "Epoch [103/250], Step [110/263], Loss: 0.1983, Accuracy: 91.56%\n",
            "Epoch [103/250], Step [120/263], Loss: 0.1680, Accuracy: 93.28%\n",
            "Epoch [103/250], Step [130/263], Loss: 0.1890, Accuracy: 92.50%\n",
            "Epoch [103/250], Step [140/263], Loss: 0.1552, Accuracy: 93.75%\n",
            "Epoch [103/250], Step [150/263], Loss: 0.1988, Accuracy: 90.94%\n",
            "Epoch [103/250], Step [160/263], Loss: 0.1872, Accuracy: 92.34%\n",
            "Epoch [103/250], Step [170/263], Loss: 0.2088, Accuracy: 91.72%\n",
            "Epoch [103/250], Step [180/263], Loss: 0.1892, Accuracy: 92.34%\n",
            "Epoch [103/250], Step [190/263], Loss: 0.1818, Accuracy: 92.66%\n",
            "Epoch [103/250], Step [200/263], Loss: 0.1831, Accuracy: 92.66%\n",
            "Epoch [103/250], Step [210/263], Loss: 0.1710, Accuracy: 93.44%\n",
            "Epoch [103/250], Step [220/263], Loss: 0.1586, Accuracy: 92.97%\n",
            "Epoch [103/250], Step [230/263], Loss: 0.1709, Accuracy: 93.44%\n",
            "Epoch [103/250], Step [240/263], Loss: 0.1719, Accuracy: 92.81%\n",
            "Epoch [103/250], Step [250/263], Loss: 0.1881, Accuracy: 92.81%\n",
            "Epoch [103/250], Step [260/263], Loss: 0.1736, Accuracy: 93.28%\n",
            "Epoch [104/250], Step [10/263], Loss: 0.1512, Accuracy: 94.22%\n",
            "Epoch [104/250], Step [20/263], Loss: 0.1504, Accuracy: 93.75%\n",
            "Epoch [104/250], Step [30/263], Loss: 0.1740, Accuracy: 93.12%\n",
            "Epoch [104/250], Step [40/263], Loss: 0.1906, Accuracy: 90.78%\n",
            "Epoch [104/250], Step [50/263], Loss: 0.1836, Accuracy: 92.81%\n",
            "Epoch [104/250], Step [60/263], Loss: 0.2011, Accuracy: 91.88%\n",
            "Epoch [104/250], Step [70/263], Loss: 0.1720, Accuracy: 92.19%\n",
            "Epoch [104/250], Step [80/263], Loss: 0.1530, Accuracy: 94.22%\n",
            "Epoch [104/250], Step [90/263], Loss: 0.1688, Accuracy: 93.91%\n",
            "Epoch [104/250], Step [100/263], Loss: 0.1832, Accuracy: 92.66%\n",
            "Epoch [104/250], Step [110/263], Loss: 0.1691, Accuracy: 92.97%\n",
            "Epoch [104/250], Step [120/263], Loss: 0.1805, Accuracy: 92.97%\n",
            "Epoch [104/250], Step [130/263], Loss: 0.1693, Accuracy: 93.28%\n",
            "Epoch [104/250], Step [140/263], Loss: 0.1997, Accuracy: 92.97%\n",
            "Epoch [104/250], Step [150/263], Loss: 0.1832, Accuracy: 92.50%\n",
            "Epoch [104/250], Step [160/263], Loss: 0.1935, Accuracy: 92.19%\n",
            "Epoch [104/250], Step [170/263], Loss: 0.1714, Accuracy: 92.97%\n",
            "Epoch [104/250], Step [180/263], Loss: 0.1854, Accuracy: 93.12%\n",
            "Epoch [104/250], Step [190/263], Loss: 0.2108, Accuracy: 92.50%\n",
            "Epoch [104/250], Step [200/263], Loss: 0.1705, Accuracy: 92.97%\n",
            "Epoch [104/250], Step [210/263], Loss: 0.1807, Accuracy: 92.50%\n",
            "Epoch [104/250], Step [220/263], Loss: 0.1981, Accuracy: 92.34%\n",
            "Epoch [104/250], Step [230/263], Loss: 0.2033, Accuracy: 90.94%\n",
            "Epoch [104/250], Step [240/263], Loss: 0.2399, Accuracy: 89.53%\n",
            "Epoch [104/250], Step [250/263], Loss: 0.2078, Accuracy: 91.56%\n",
            "Epoch [104/250], Step [260/263], Loss: 0.1959, Accuracy: 92.03%\n",
            "Epoch [105/250], Step [10/263], Loss: 0.1964, Accuracy: 92.19%\n",
            "Epoch [105/250], Step [20/263], Loss: 0.1794, Accuracy: 92.81%\n",
            "Epoch [105/250], Step [30/263], Loss: 0.1623, Accuracy: 93.75%\n",
            "Epoch [105/250], Step [40/263], Loss: 0.1765, Accuracy: 93.44%\n",
            "Epoch [105/250], Step [50/263], Loss: 0.1775, Accuracy: 92.81%\n",
            "Epoch [105/250], Step [60/263], Loss: 0.1661, Accuracy: 93.59%\n",
            "Epoch [105/250], Step [70/263], Loss: 0.1652, Accuracy: 93.59%\n",
            "Epoch [105/250], Step [80/263], Loss: 0.1797, Accuracy: 93.75%\n",
            "Epoch [105/250], Step [90/263], Loss: 0.1393, Accuracy: 94.69%\n",
            "Epoch [105/250], Step [100/263], Loss: 0.1526, Accuracy: 95.00%\n",
            "Epoch [105/250], Step [110/263], Loss: 0.2037, Accuracy: 92.03%\n",
            "Epoch [105/250], Step [120/263], Loss: 0.1796, Accuracy: 93.59%\n",
            "Epoch [105/250], Step [130/263], Loss: 0.1347, Accuracy: 94.84%\n",
            "Epoch [105/250], Step [140/263], Loss: 0.2235, Accuracy: 90.78%\n",
            "Epoch [105/250], Step [150/263], Loss: 0.2160, Accuracy: 90.94%\n",
            "Epoch [105/250], Step [160/263], Loss: 0.1725, Accuracy: 93.28%\n",
            "Epoch [105/250], Step [170/263], Loss: 0.1875, Accuracy: 92.34%\n",
            "Epoch [105/250], Step [180/263], Loss: 0.2041, Accuracy: 90.94%\n",
            "Epoch [105/250], Step [190/263], Loss: 0.1637, Accuracy: 93.59%\n",
            "Epoch [105/250], Step [200/263], Loss: 0.1961, Accuracy: 91.72%\n",
            "Epoch [105/250], Step [210/263], Loss: 0.1695, Accuracy: 93.12%\n",
            "Epoch [105/250], Step [220/263], Loss: 0.1685, Accuracy: 93.12%\n",
            "Epoch [105/250], Step [230/263], Loss: 0.1598, Accuracy: 94.06%\n",
            "Epoch [105/250], Step [240/263], Loss: 0.1765, Accuracy: 92.97%\n",
            "Epoch [105/250], Step [250/263], Loss: 0.2051, Accuracy: 91.25%\n",
            "Epoch [105/250], Step [260/263], Loss: 0.1862, Accuracy: 93.12%\n",
            "Epoch [106/250], Step [10/263], Loss: 0.1628, Accuracy: 94.84%\n",
            "Epoch [106/250], Step [20/263], Loss: 0.1874, Accuracy: 92.50%\n",
            "Epoch [106/250], Step [30/263], Loss: 0.1746, Accuracy: 93.91%\n",
            "Epoch [106/250], Step [40/263], Loss: 0.1624, Accuracy: 93.28%\n",
            "Epoch [106/250], Step [50/263], Loss: 0.1482, Accuracy: 94.53%\n",
            "Epoch [106/250], Step [60/263], Loss: 0.1658, Accuracy: 93.12%\n",
            "Epoch [106/250], Step [70/263], Loss: 0.1652, Accuracy: 93.44%\n",
            "Epoch [106/250], Step [80/263], Loss: 0.1562, Accuracy: 93.75%\n",
            "Epoch [106/250], Step [90/263], Loss: 0.1892, Accuracy: 92.81%\n",
            "Epoch [106/250], Step [100/263], Loss: 0.1924, Accuracy: 93.59%\n",
            "Epoch [106/250], Step [110/263], Loss: 0.1636, Accuracy: 93.75%\n",
            "Epoch [106/250], Step [120/263], Loss: 0.1926, Accuracy: 92.50%\n",
            "Epoch [106/250], Step [130/263], Loss: 0.1646, Accuracy: 94.06%\n",
            "Epoch [106/250], Step [140/263], Loss: 0.1640, Accuracy: 93.75%\n",
            "Epoch [106/250], Step [150/263], Loss: 0.1764, Accuracy: 93.28%\n",
            "Epoch [106/250], Step [160/263], Loss: 0.2202, Accuracy: 92.50%\n",
            "Epoch [106/250], Step [170/263], Loss: 0.1874, Accuracy: 93.12%\n",
            "Epoch [106/250], Step [180/263], Loss: 0.1781, Accuracy: 93.12%\n",
            "Epoch [106/250], Step [190/263], Loss: 0.1590, Accuracy: 94.06%\n",
            "Epoch [106/250], Step [200/263], Loss: 0.1279, Accuracy: 94.53%\n",
            "Epoch [106/250], Step [210/263], Loss: 0.2177, Accuracy: 90.62%\n",
            "Epoch [106/250], Step [220/263], Loss: 0.1948, Accuracy: 91.56%\n",
            "Epoch [106/250], Step [230/263], Loss: 0.1810, Accuracy: 93.12%\n",
            "Epoch [106/250], Step [240/263], Loss: 0.1838, Accuracy: 93.75%\n",
            "Epoch [106/250], Step [250/263], Loss: 0.1871, Accuracy: 92.66%\n",
            "Epoch [106/250], Step [260/263], Loss: 0.1790, Accuracy: 92.50%\n",
            "Epoch [107/250], Step [10/263], Loss: 0.1864, Accuracy: 93.91%\n",
            "Epoch [107/250], Step [20/263], Loss: 0.1640, Accuracy: 93.12%\n",
            "Epoch [107/250], Step [30/263], Loss: 0.1920, Accuracy: 91.56%\n",
            "Epoch [107/250], Step [40/263], Loss: 0.1724, Accuracy: 93.44%\n",
            "Epoch [107/250], Step [50/263], Loss: 0.2170, Accuracy: 90.94%\n",
            "Epoch [107/250], Step [60/263], Loss: 0.1843, Accuracy: 92.50%\n",
            "Epoch [107/250], Step [70/263], Loss: 0.2096, Accuracy: 91.25%\n",
            "Epoch [107/250], Step [80/263], Loss: 0.2015, Accuracy: 91.09%\n",
            "Epoch [107/250], Step [90/263], Loss: 0.1784, Accuracy: 92.50%\n",
            "Epoch [107/250], Step [100/263], Loss: 0.2200, Accuracy: 91.09%\n",
            "Epoch [107/250], Step [110/263], Loss: 0.2271, Accuracy: 91.09%\n",
            "Epoch [107/250], Step [120/263], Loss: 0.2582, Accuracy: 89.69%\n",
            "Epoch [107/250], Step [130/263], Loss: 0.2047, Accuracy: 91.41%\n",
            "Epoch [107/250], Step [140/263], Loss: 0.1812, Accuracy: 91.25%\n",
            "Epoch [107/250], Step [150/263], Loss: 0.1634, Accuracy: 93.91%\n",
            "Epoch [107/250], Step [160/263], Loss: 0.2021, Accuracy: 92.03%\n",
            "Epoch [107/250], Step [170/263], Loss: 0.1881, Accuracy: 91.56%\n",
            "Epoch [107/250], Step [180/263], Loss: 0.1744, Accuracy: 93.12%\n",
            "Epoch [107/250], Step [190/263], Loss: 0.1943, Accuracy: 92.34%\n",
            "Epoch [107/250], Step [200/263], Loss: 0.2014, Accuracy: 92.19%\n",
            "Epoch [107/250], Step [210/263], Loss: 0.2142, Accuracy: 91.88%\n",
            "Epoch [107/250], Step [220/263], Loss: 0.1961, Accuracy: 92.66%\n",
            "Epoch [107/250], Step [230/263], Loss: 0.1791, Accuracy: 91.88%\n",
            "Epoch [107/250], Step [240/263], Loss: 0.2102, Accuracy: 91.88%\n",
            "Epoch [107/250], Step [250/263], Loss: 0.1577, Accuracy: 93.59%\n",
            "Epoch [107/250], Step [260/263], Loss: 0.1371, Accuracy: 95.31%\n",
            "Epoch [108/250], Step [10/263], Loss: 0.1543, Accuracy: 95.62%\n",
            "Epoch [108/250], Step [20/263], Loss: 0.1695, Accuracy: 92.19%\n",
            "Epoch [108/250], Step [30/263], Loss: 0.1720, Accuracy: 93.12%\n",
            "Epoch [108/250], Step [40/263], Loss: 0.1509, Accuracy: 95.00%\n",
            "Epoch [108/250], Step [50/263], Loss: 0.1919, Accuracy: 92.66%\n",
            "Epoch [108/250], Step [60/263], Loss: 0.1958, Accuracy: 91.72%\n",
            "Epoch [108/250], Step [70/263], Loss: 0.1756, Accuracy: 93.12%\n",
            "Epoch [108/250], Step [80/263], Loss: 0.2043, Accuracy: 92.03%\n",
            "Epoch [108/250], Step [90/263], Loss: 0.2051, Accuracy: 92.34%\n",
            "Epoch [108/250], Step [100/263], Loss: 0.1636, Accuracy: 94.22%\n",
            "Epoch [108/250], Step [110/263], Loss: 0.1478, Accuracy: 94.69%\n",
            "Epoch [108/250], Step [120/263], Loss: 0.1982, Accuracy: 92.03%\n",
            "Epoch [108/250], Step [130/263], Loss: 0.2255, Accuracy: 90.31%\n",
            "Epoch [108/250], Step [140/263], Loss: 0.1881, Accuracy: 92.19%\n",
            "Epoch [108/250], Step [150/263], Loss: 0.2023, Accuracy: 91.72%\n",
            "Epoch [108/250], Step [160/263], Loss: 0.1767, Accuracy: 94.06%\n",
            "Epoch [108/250], Step [170/263], Loss: 0.1595, Accuracy: 93.12%\n",
            "Epoch [108/250], Step [180/263], Loss: 0.1759, Accuracy: 92.81%\n",
            "Epoch [108/250], Step [190/263], Loss: 0.1675, Accuracy: 92.34%\n",
            "Epoch [108/250], Step [200/263], Loss: 0.1903, Accuracy: 92.03%\n",
            "Epoch [108/250], Step [210/263], Loss: 0.1668, Accuracy: 93.75%\n",
            "Epoch [108/250], Step [220/263], Loss: 0.1962, Accuracy: 92.81%\n",
            "Epoch [108/250], Step [230/263], Loss: 0.1952, Accuracy: 91.56%\n",
            "Epoch [108/250], Step [240/263], Loss: 0.1860, Accuracy: 92.50%\n",
            "Epoch [108/250], Step [250/263], Loss: 0.2330, Accuracy: 90.62%\n",
            "Epoch [108/250], Step [260/263], Loss: 0.1951, Accuracy: 90.47%\n",
            "Epoch [109/250], Step [10/263], Loss: 0.2089, Accuracy: 91.72%\n",
            "Epoch [109/250], Step [20/263], Loss: 0.1879, Accuracy: 91.72%\n",
            "Epoch [109/250], Step [30/263], Loss: 0.1746, Accuracy: 92.97%\n",
            "Epoch [109/250], Step [40/263], Loss: 0.1942, Accuracy: 92.19%\n",
            "Epoch [109/250], Step [50/263], Loss: 0.1803, Accuracy: 93.59%\n",
            "Epoch [109/250], Step [60/263], Loss: 0.1858, Accuracy: 92.66%\n",
            "Epoch [109/250], Step [70/263], Loss: 0.1896, Accuracy: 92.34%\n",
            "Epoch [109/250], Step [80/263], Loss: 0.2402, Accuracy: 91.09%\n",
            "Epoch [109/250], Step [90/263], Loss: 0.1795, Accuracy: 92.97%\n",
            "Epoch [109/250], Step [100/263], Loss: 0.2145, Accuracy: 92.66%\n",
            "Epoch [109/250], Step [110/263], Loss: 0.2261, Accuracy: 90.31%\n",
            "Epoch [109/250], Step [120/263], Loss: 0.1846, Accuracy: 93.75%\n",
            "Epoch [109/250], Step [130/263], Loss: 0.1601, Accuracy: 94.38%\n",
            "Epoch [109/250], Step [140/263], Loss: 0.2011, Accuracy: 91.09%\n",
            "Epoch [109/250], Step [150/263], Loss: 0.1622, Accuracy: 92.81%\n",
            "Epoch [109/250], Step [160/263], Loss: 0.2149, Accuracy: 90.62%\n",
            "Epoch [109/250], Step [170/263], Loss: 0.1468, Accuracy: 94.06%\n",
            "Epoch [109/250], Step [180/263], Loss: 0.1854, Accuracy: 92.97%\n",
            "Epoch [109/250], Step [190/263], Loss: 0.1446, Accuracy: 94.38%\n",
            "Epoch [109/250], Step [200/263], Loss: 0.2047, Accuracy: 91.88%\n",
            "Epoch [109/250], Step [210/263], Loss: 0.1780, Accuracy: 93.91%\n",
            "Epoch [109/250], Step [220/263], Loss: 0.2118, Accuracy: 91.72%\n",
            "Epoch [109/250], Step [230/263], Loss: 0.1732, Accuracy: 92.81%\n",
            "Epoch [109/250], Step [240/263], Loss: 0.1833, Accuracy: 92.50%\n",
            "Epoch [109/250], Step [250/263], Loss: 0.1547, Accuracy: 93.75%\n",
            "Epoch [109/250], Step [260/263], Loss: 0.1502, Accuracy: 93.59%\n",
            "Epoch [110/250], Step [10/263], Loss: 0.1395, Accuracy: 95.16%\n",
            "Epoch [110/250], Step [20/263], Loss: 0.1368, Accuracy: 95.00%\n",
            "Epoch [110/250], Step [30/263], Loss: 0.1564, Accuracy: 94.53%\n",
            "Epoch [110/250], Step [40/263], Loss: 0.1479, Accuracy: 95.00%\n",
            "Epoch [110/250], Step [50/263], Loss: 0.1608, Accuracy: 93.28%\n",
            "Epoch [110/250], Step [60/263], Loss: 0.1839, Accuracy: 93.28%\n",
            "Epoch [110/250], Step [70/263], Loss: 0.1775, Accuracy: 92.97%\n",
            "Epoch [110/250], Step [80/263], Loss: 0.1647, Accuracy: 94.53%\n",
            "Epoch [110/250], Step [90/263], Loss: 0.1607, Accuracy: 94.06%\n",
            "Epoch [110/250], Step [100/263], Loss: 0.1634, Accuracy: 92.81%\n",
            "Epoch [110/250], Step [110/263], Loss: 0.1767, Accuracy: 93.91%\n",
            "Epoch [110/250], Step [120/263], Loss: 0.1505, Accuracy: 94.84%\n",
            "Epoch [110/250], Step [130/263], Loss: 0.1744, Accuracy: 93.12%\n",
            "Epoch [110/250], Step [140/263], Loss: 0.1799, Accuracy: 92.34%\n",
            "Epoch [110/250], Step [150/263], Loss: 0.1918, Accuracy: 93.28%\n",
            "Epoch [110/250], Step [160/263], Loss: 0.1559, Accuracy: 94.22%\n",
            "Epoch [110/250], Step [170/263], Loss: 0.1477, Accuracy: 95.16%\n",
            "Epoch [110/250], Step [180/263], Loss: 0.1868, Accuracy: 92.50%\n",
            "Epoch [110/250], Step [190/263], Loss: 0.1697, Accuracy: 93.59%\n",
            "Epoch [110/250], Step [200/263], Loss: 0.1859, Accuracy: 92.34%\n",
            "Epoch [110/250], Step [210/263], Loss: 0.1603, Accuracy: 92.97%\n",
            "Epoch [110/250], Step [220/263], Loss: 0.2020, Accuracy: 91.09%\n",
            "Epoch [110/250], Step [230/263], Loss: 0.1682, Accuracy: 92.19%\n",
            "Epoch [110/250], Step [240/263], Loss: 0.1548, Accuracy: 93.59%\n",
            "Epoch [110/250], Step [250/263], Loss: 0.1489, Accuracy: 94.06%\n",
            "Epoch [110/250], Step [260/263], Loss: 0.1564, Accuracy: 94.22%\n",
            "Epoch [111/250], Step [10/263], Loss: 0.1836, Accuracy: 92.66%\n",
            "Epoch [111/250], Step [20/263], Loss: 0.1314, Accuracy: 95.16%\n",
            "Epoch [111/250], Step [30/263], Loss: 0.1576, Accuracy: 93.75%\n",
            "Epoch [111/250], Step [40/263], Loss: 0.1599, Accuracy: 94.53%\n",
            "Epoch [111/250], Step [50/263], Loss: 0.1876, Accuracy: 93.12%\n",
            "Epoch [111/250], Step [60/263], Loss: 0.1513, Accuracy: 94.22%\n",
            "Epoch [111/250], Step [70/263], Loss: 0.1335, Accuracy: 94.06%\n",
            "Epoch [111/250], Step [80/263], Loss: 0.1578, Accuracy: 93.59%\n",
            "Epoch [111/250], Step [90/263], Loss: 0.1395, Accuracy: 94.22%\n",
            "Epoch [111/250], Step [100/263], Loss: 0.1192, Accuracy: 95.78%\n",
            "Epoch [111/250], Step [110/263], Loss: 0.1581, Accuracy: 94.06%\n",
            "Epoch [111/250], Step [120/263], Loss: 0.1687, Accuracy: 93.44%\n",
            "Epoch [111/250], Step [130/263], Loss: 0.1948, Accuracy: 92.19%\n",
            "Epoch [111/250], Step [140/263], Loss: 0.1542, Accuracy: 94.53%\n",
            "Epoch [111/250], Step [150/263], Loss: 0.1763, Accuracy: 92.34%\n",
            "Epoch [111/250], Step [160/263], Loss: 0.1773, Accuracy: 93.59%\n",
            "Epoch [111/250], Step [170/263], Loss: 0.1495, Accuracy: 94.22%\n",
            "Epoch [111/250], Step [180/263], Loss: 0.1277, Accuracy: 95.00%\n",
            "Epoch [111/250], Step [190/263], Loss: 0.2154, Accuracy: 91.88%\n",
            "Epoch [111/250], Step [200/263], Loss: 0.1543, Accuracy: 93.91%\n",
            "Epoch [111/250], Step [210/263], Loss: 0.1459, Accuracy: 95.16%\n",
            "Epoch [111/250], Step [220/263], Loss: 0.1535, Accuracy: 93.44%\n",
            "Epoch [111/250], Step [230/263], Loss: 0.1773, Accuracy: 92.66%\n",
            "Epoch [111/250], Step [240/263], Loss: 0.1395, Accuracy: 94.69%\n",
            "Epoch [111/250], Step [250/263], Loss: 0.1696, Accuracy: 93.91%\n",
            "Epoch [111/250], Step [260/263], Loss: 0.1597, Accuracy: 92.97%\n",
            "Epoch [112/250], Step [10/263], Loss: 0.2111, Accuracy: 91.25%\n",
            "Epoch [112/250], Step [20/263], Loss: 0.2214, Accuracy: 91.09%\n",
            "Epoch [112/250], Step [30/263], Loss: 0.2713, Accuracy: 89.22%\n",
            "Epoch [112/250], Step [40/263], Loss: 0.2122, Accuracy: 91.56%\n",
            "Epoch [112/250], Step [50/263], Loss: 0.2449, Accuracy: 92.34%\n",
            "Epoch [112/250], Step [60/263], Loss: 0.1894, Accuracy: 92.66%\n",
            "Epoch [112/250], Step [70/263], Loss: 0.3420, Accuracy: 90.16%\n",
            "Epoch [112/250], Step [80/263], Loss: 0.3059, Accuracy: 87.03%\n",
            "Epoch [112/250], Step [90/263], Loss: 0.4052, Accuracy: 84.53%\n",
            "Epoch [112/250], Step [100/263], Loss: 0.3293, Accuracy: 87.66%\n",
            "Epoch [112/250], Step [110/263], Loss: 0.2771, Accuracy: 88.59%\n",
            "Epoch [112/250], Step [120/263], Loss: 0.2451, Accuracy: 90.78%\n",
            "Epoch [112/250], Step [130/263], Loss: 0.2856, Accuracy: 88.59%\n",
            "Epoch [112/250], Step [140/263], Loss: 0.2055, Accuracy: 90.94%\n",
            "Epoch [112/250], Step [150/263], Loss: 0.2183, Accuracy: 90.47%\n",
            "Epoch [112/250], Step [160/263], Loss: 0.1544, Accuracy: 92.50%\n",
            "Epoch [112/250], Step [170/263], Loss: 0.2121, Accuracy: 90.94%\n",
            "Epoch [112/250], Step [180/263], Loss: 0.2233, Accuracy: 91.56%\n",
            "Epoch [112/250], Step [190/263], Loss: 0.1851, Accuracy: 92.81%\n",
            "Epoch [112/250], Step [200/263], Loss: 0.1905, Accuracy: 91.25%\n",
            "Epoch [112/250], Step [210/263], Loss: 0.1873, Accuracy: 91.41%\n",
            "Epoch [112/250], Step [220/263], Loss: 0.1856, Accuracy: 92.34%\n",
            "Epoch [112/250], Step [230/263], Loss: 0.1818, Accuracy: 92.66%\n",
            "Epoch [112/250], Step [240/263], Loss: 0.1670, Accuracy: 93.59%\n",
            "Epoch [112/250], Step [250/263], Loss: 0.1434, Accuracy: 95.00%\n",
            "Epoch [112/250], Step [260/263], Loss: 0.1682, Accuracy: 92.97%\n",
            "Epoch [113/250], Step [10/263], Loss: 0.1401, Accuracy: 94.84%\n",
            "Epoch [113/250], Step [20/263], Loss: 0.1824, Accuracy: 93.28%\n",
            "Epoch [113/250], Step [30/263], Loss: 0.1512, Accuracy: 93.91%\n",
            "Epoch [113/250], Step [40/263], Loss: 0.1402, Accuracy: 94.53%\n",
            "Epoch [113/250], Step [50/263], Loss: 0.1693, Accuracy: 93.91%\n",
            "Epoch [113/250], Step [60/263], Loss: 0.1638, Accuracy: 94.22%\n",
            "Epoch [113/250], Step [70/263], Loss: 0.1353, Accuracy: 94.84%\n",
            "Epoch [113/250], Step [80/263], Loss: 0.1421, Accuracy: 94.38%\n",
            "Epoch [113/250], Step [90/263], Loss: 0.1411, Accuracy: 94.06%\n",
            "Epoch [113/250], Step [100/263], Loss: 0.1423, Accuracy: 94.69%\n",
            "Epoch [113/250], Step [110/263], Loss: 0.1539, Accuracy: 93.12%\n",
            "Epoch [113/250], Step [120/263], Loss: 0.1381, Accuracy: 94.84%\n",
            "Epoch [113/250], Step [130/263], Loss: 0.1618, Accuracy: 93.75%\n",
            "Epoch [113/250], Step [140/263], Loss: 0.1908, Accuracy: 91.88%\n",
            "Epoch [113/250], Step [150/263], Loss: 0.1776, Accuracy: 93.44%\n",
            "Epoch [113/250], Step [160/263], Loss: 0.1650, Accuracy: 93.91%\n",
            "Epoch [113/250], Step [170/263], Loss: 0.1301, Accuracy: 95.16%\n",
            "Epoch [113/250], Step [180/263], Loss: 0.1444, Accuracy: 94.06%\n",
            "Epoch [113/250], Step [190/263], Loss: 0.1718, Accuracy: 93.59%\n",
            "Epoch [113/250], Step [200/263], Loss: 0.1712, Accuracy: 92.81%\n",
            "Epoch [113/250], Step [210/263], Loss: 0.1519, Accuracy: 94.53%\n",
            "Epoch [113/250], Step [220/263], Loss: 0.1506, Accuracy: 94.06%\n",
            "Epoch [113/250], Step [230/263], Loss: 0.1608, Accuracy: 92.81%\n",
            "Epoch [113/250], Step [240/263], Loss: 0.1412, Accuracy: 94.84%\n",
            "Epoch [113/250], Step [250/263], Loss: 0.1702, Accuracy: 93.28%\n",
            "Epoch [113/250], Step [260/263], Loss: 0.1494, Accuracy: 95.00%\n",
            "Epoch [114/250], Step [10/263], Loss: 0.1254, Accuracy: 96.09%\n",
            "Epoch [114/250], Step [20/263], Loss: 0.1448, Accuracy: 95.62%\n",
            "Epoch [114/250], Step [30/263], Loss: 0.1401, Accuracy: 94.38%\n",
            "Epoch [114/250], Step [40/263], Loss: 0.1340, Accuracy: 95.16%\n",
            "Epoch [114/250], Step [50/263], Loss: 0.1727, Accuracy: 92.66%\n",
            "Epoch [114/250], Step [60/263], Loss: 0.1162, Accuracy: 95.00%\n",
            "Epoch [114/250], Step [70/263], Loss: 0.1358, Accuracy: 94.38%\n",
            "Epoch [114/250], Step [80/263], Loss: 0.1528, Accuracy: 93.75%\n",
            "Epoch [114/250], Step [90/263], Loss: 0.1242, Accuracy: 95.00%\n",
            "Epoch [114/250], Step [100/263], Loss: 0.1510, Accuracy: 93.75%\n",
            "Epoch [114/250], Step [110/263], Loss: 0.1475, Accuracy: 93.75%\n",
            "Epoch [114/250], Step [120/263], Loss: 0.1426, Accuracy: 95.00%\n",
            "Epoch [114/250], Step [130/263], Loss: 0.1237, Accuracy: 95.31%\n",
            "Epoch [114/250], Step [140/263], Loss: 0.1311, Accuracy: 94.84%\n",
            "Epoch [114/250], Step [150/263], Loss: 0.1343, Accuracy: 94.53%\n",
            "Epoch [114/250], Step [160/263], Loss: 0.1280, Accuracy: 94.53%\n",
            "Epoch [114/250], Step [170/263], Loss: 0.1539, Accuracy: 94.06%\n",
            "Epoch [114/250], Step [180/263], Loss: 0.1586, Accuracy: 94.22%\n",
            "Epoch [114/250], Step [190/263], Loss: 0.1472, Accuracy: 93.75%\n",
            "Epoch [114/250], Step [200/263], Loss: 0.1614, Accuracy: 93.75%\n",
            "Epoch [114/250], Step [210/263], Loss: 0.1848, Accuracy: 92.66%\n",
            "Epoch [114/250], Step [220/263], Loss: 0.1411, Accuracy: 95.00%\n",
            "Epoch [114/250], Step [230/263], Loss: 0.1307, Accuracy: 94.69%\n",
            "Epoch [114/250], Step [240/263], Loss: 0.1299, Accuracy: 95.31%\n",
            "Epoch [114/250], Step [250/263], Loss: 0.1685, Accuracy: 93.12%\n",
            "Epoch [114/250], Step [260/263], Loss: 0.1756, Accuracy: 92.81%\n",
            "Epoch [115/250], Step [10/263], Loss: 0.1447, Accuracy: 94.84%\n",
            "Epoch [115/250], Step [20/263], Loss: 0.1319, Accuracy: 95.62%\n",
            "Epoch [115/250], Step [30/263], Loss: 0.1431, Accuracy: 94.06%\n",
            "Epoch [115/250], Step [40/263], Loss: 0.1336, Accuracy: 94.69%\n",
            "Epoch [115/250], Step [50/263], Loss: 0.1320, Accuracy: 94.84%\n",
            "Epoch [115/250], Step [60/263], Loss: 0.1361, Accuracy: 94.84%\n",
            "Epoch [115/250], Step [70/263], Loss: 0.1526, Accuracy: 94.38%\n",
            "Epoch [115/250], Step [80/263], Loss: 0.1841, Accuracy: 92.34%\n",
            "Epoch [115/250], Step [90/263], Loss: 0.1958, Accuracy: 92.34%\n",
            "Epoch [115/250], Step [100/263], Loss: 0.1260, Accuracy: 95.16%\n",
            "Epoch [115/250], Step [110/263], Loss: 0.1255, Accuracy: 96.41%\n",
            "Epoch [115/250], Step [120/263], Loss: 0.1508, Accuracy: 94.22%\n",
            "Epoch [115/250], Step [130/263], Loss: 0.1261, Accuracy: 95.62%\n",
            "Epoch [115/250], Step [140/263], Loss: 0.1524, Accuracy: 93.28%\n",
            "Epoch [115/250], Step [150/263], Loss: 0.1632, Accuracy: 93.75%\n",
            "Epoch [115/250], Step [160/263], Loss: 0.1431, Accuracy: 94.06%\n",
            "Epoch [115/250], Step [170/263], Loss: 0.1512, Accuracy: 94.84%\n",
            "Epoch [115/250], Step [180/263], Loss: 0.1199, Accuracy: 95.78%\n",
            "Epoch [115/250], Step [190/263], Loss: 0.1732, Accuracy: 92.50%\n",
            "Epoch [115/250], Step [200/263], Loss: 0.1612, Accuracy: 93.28%\n",
            "Epoch [115/250], Step [210/263], Loss: 0.1837, Accuracy: 93.59%\n",
            "Epoch [115/250], Step [220/263], Loss: 0.1569, Accuracy: 93.44%\n",
            "Epoch [115/250], Step [230/263], Loss: 0.1286, Accuracy: 94.69%\n",
            "Epoch [115/250], Step [240/263], Loss: 0.1376, Accuracy: 94.53%\n",
            "Epoch [115/250], Step [250/263], Loss: 0.1286, Accuracy: 95.16%\n",
            "Epoch [115/250], Step [260/263], Loss: 0.1845, Accuracy: 91.88%\n",
            "Epoch [116/250], Step [10/263], Loss: 0.1224, Accuracy: 95.31%\n",
            "Epoch [116/250], Step [20/263], Loss: 0.1167, Accuracy: 95.31%\n",
            "Epoch [116/250], Step [30/263], Loss: 0.1417, Accuracy: 93.91%\n",
            "Epoch [116/250], Step [40/263], Loss: 0.1653, Accuracy: 93.75%\n",
            "Epoch [116/250], Step [50/263], Loss: 0.1126, Accuracy: 95.62%\n",
            "Epoch [116/250], Step [60/263], Loss: 0.1455, Accuracy: 94.53%\n",
            "Epoch [116/250], Step [70/263], Loss: 0.1368, Accuracy: 94.69%\n",
            "Epoch [116/250], Step [80/263], Loss: 0.1299, Accuracy: 95.16%\n",
            "Epoch [116/250], Step [90/263], Loss: 0.1185, Accuracy: 95.62%\n",
            "Epoch [116/250], Step [100/263], Loss: 0.1330, Accuracy: 94.84%\n",
            "Epoch [116/250], Step [110/263], Loss: 0.1517, Accuracy: 94.38%\n",
            "Epoch [116/250], Step [120/263], Loss: 0.1434, Accuracy: 94.53%\n",
            "Epoch [116/250], Step [130/263], Loss: 0.1676, Accuracy: 92.34%\n",
            "Epoch [116/250], Step [140/263], Loss: 0.1389, Accuracy: 94.69%\n",
            "Epoch [116/250], Step [150/263], Loss: 0.1191, Accuracy: 95.94%\n",
            "Epoch [116/250], Step [160/263], Loss: 0.1440, Accuracy: 93.75%\n",
            "Epoch [116/250], Step [170/263], Loss: 0.1320, Accuracy: 95.16%\n",
            "Epoch [116/250], Step [180/263], Loss: 0.1263, Accuracy: 95.16%\n",
            "Epoch [116/250], Step [190/263], Loss: 0.1716, Accuracy: 92.97%\n",
            "Epoch [116/250], Step [200/263], Loss: 0.1815, Accuracy: 92.66%\n",
            "Epoch [116/250], Step [210/263], Loss: 0.1661, Accuracy: 94.38%\n",
            "Epoch [116/250], Step [220/263], Loss: 0.1518, Accuracy: 94.38%\n",
            "Epoch [116/250], Step [230/263], Loss: 0.1821, Accuracy: 92.81%\n",
            "Epoch [116/250], Step [240/263], Loss: 0.1561, Accuracy: 94.22%\n",
            "Epoch [116/250], Step [250/263], Loss: 0.1734, Accuracy: 93.44%\n",
            "Epoch [116/250], Step [260/263], Loss: 0.1864, Accuracy: 91.88%\n",
            "Epoch [117/250], Step [10/263], Loss: 0.1405, Accuracy: 95.16%\n",
            "Epoch [117/250], Step [20/263], Loss: 0.1489, Accuracy: 92.97%\n",
            "Epoch [117/250], Step [30/263], Loss: 0.1334, Accuracy: 95.00%\n",
            "Epoch [117/250], Step [40/263], Loss: 0.1642, Accuracy: 93.44%\n",
            "Epoch [117/250], Step [50/263], Loss: 0.1382, Accuracy: 94.06%\n",
            "Epoch [117/250], Step [60/263], Loss: 0.1669, Accuracy: 93.28%\n",
            "Epoch [117/250], Step [70/263], Loss: 0.1283, Accuracy: 95.16%\n",
            "Epoch [117/250], Step [80/263], Loss: 0.1358, Accuracy: 95.16%\n",
            "Epoch [117/250], Step [90/263], Loss: 0.1428, Accuracy: 94.69%\n",
            "Epoch [117/250], Step [100/263], Loss: 0.1243, Accuracy: 94.84%\n",
            "Epoch [117/250], Step [110/263], Loss: 0.1216, Accuracy: 95.16%\n",
            "Epoch [117/250], Step [120/263], Loss: 0.1554, Accuracy: 94.69%\n",
            "Epoch [117/250], Step [130/263], Loss: 0.1647, Accuracy: 92.97%\n",
            "Epoch [117/250], Step [140/263], Loss: 0.1783, Accuracy: 92.81%\n",
            "Epoch [117/250], Step [150/263], Loss: 0.1371, Accuracy: 95.16%\n",
            "Epoch [117/250], Step [160/263], Loss: 0.1417, Accuracy: 94.84%\n",
            "Epoch [117/250], Step [170/263], Loss: 0.1717, Accuracy: 92.97%\n",
            "Epoch [117/250], Step [180/263], Loss: 0.1807, Accuracy: 92.50%\n",
            "Epoch [117/250], Step [190/263], Loss: 0.1484, Accuracy: 93.59%\n",
            "Epoch [117/250], Step [200/263], Loss: 0.1481, Accuracy: 93.44%\n",
            "Epoch [117/250], Step [210/263], Loss: 0.1391, Accuracy: 94.22%\n",
            "Epoch [117/250], Step [220/263], Loss: 0.1614, Accuracy: 92.81%\n",
            "Epoch [117/250], Step [230/263], Loss: 0.1559, Accuracy: 94.69%\n",
            "Epoch [117/250], Step [240/263], Loss: 0.1348, Accuracy: 94.84%\n",
            "Epoch [117/250], Step [250/263], Loss: 0.1455, Accuracy: 94.22%\n",
            "Epoch [117/250], Step [260/263], Loss: 0.2013, Accuracy: 91.25%\n",
            "Epoch [118/250], Step [10/263], Loss: 0.1322, Accuracy: 95.62%\n",
            "Epoch [118/250], Step [20/263], Loss: 0.1051, Accuracy: 96.72%\n",
            "Epoch [118/250], Step [30/263], Loss: 0.1475, Accuracy: 94.06%\n",
            "Epoch [118/250], Step [40/263], Loss: 0.1547, Accuracy: 93.75%\n",
            "Epoch [118/250], Step [50/263], Loss: 0.1470, Accuracy: 94.06%\n",
            "Epoch [118/250], Step [60/263], Loss: 0.1583, Accuracy: 94.53%\n",
            "Epoch [118/250], Step [70/263], Loss: 0.1136, Accuracy: 95.78%\n",
            "Epoch [118/250], Step [80/263], Loss: 0.1392, Accuracy: 94.22%\n",
            "Epoch [118/250], Step [90/263], Loss: 0.1210, Accuracy: 95.31%\n",
            "Epoch [118/250], Step [100/263], Loss: 0.1167, Accuracy: 95.31%\n",
            "Epoch [118/250], Step [110/263], Loss: 0.1484, Accuracy: 93.91%\n",
            "Epoch [118/250], Step [120/263], Loss: 0.1421, Accuracy: 94.38%\n",
            "Epoch [118/250], Step [130/263], Loss: 0.1424, Accuracy: 94.84%\n",
            "Epoch [118/250], Step [140/263], Loss: 0.1194, Accuracy: 95.78%\n",
            "Epoch [118/250], Step [150/263], Loss: 0.1501, Accuracy: 94.53%\n",
            "Epoch [118/250], Step [160/263], Loss: 0.1041, Accuracy: 95.94%\n",
            "Epoch [118/250], Step [170/263], Loss: 0.1566, Accuracy: 93.91%\n",
            "Epoch [118/250], Step [180/263], Loss: 0.1486, Accuracy: 94.22%\n",
            "Epoch [118/250], Step [190/263], Loss: 0.1419, Accuracy: 93.91%\n",
            "Epoch [118/250], Step [200/263], Loss: 0.1487, Accuracy: 94.06%\n",
            "Epoch [118/250], Step [210/263], Loss: 0.1525, Accuracy: 93.75%\n",
            "Epoch [118/250], Step [220/263], Loss: 0.1462, Accuracy: 95.16%\n",
            "Epoch [118/250], Step [230/263], Loss: 0.1275, Accuracy: 95.31%\n",
            "Epoch [118/250], Step [240/263], Loss: 0.1702, Accuracy: 93.12%\n",
            "Epoch [118/250], Step [250/263], Loss: 0.1395, Accuracy: 94.53%\n",
            "Epoch [118/250], Step [260/263], Loss: 0.1324, Accuracy: 95.00%\n",
            "Epoch [119/250], Step [10/263], Loss: 0.1397, Accuracy: 94.84%\n",
            "Epoch [119/250], Step [20/263], Loss: 0.1104, Accuracy: 96.25%\n",
            "Epoch [119/250], Step [30/263], Loss: 0.0981, Accuracy: 96.56%\n",
            "Epoch [119/250], Step [40/263], Loss: 0.1288, Accuracy: 96.09%\n",
            "Epoch [119/250], Step [50/263], Loss: 0.1234, Accuracy: 94.84%\n",
            "Epoch [119/250], Step [60/263], Loss: 0.1257, Accuracy: 95.31%\n",
            "Epoch [119/250], Step [70/263], Loss: 0.1211, Accuracy: 96.41%\n",
            "Epoch [119/250], Step [80/263], Loss: 0.0979, Accuracy: 96.72%\n",
            "Epoch [119/250], Step [90/263], Loss: 0.1241, Accuracy: 95.00%\n",
            "Epoch [119/250], Step [100/263], Loss: 0.1136, Accuracy: 95.94%\n",
            "Epoch [119/250], Step [110/263], Loss: 0.1317, Accuracy: 95.47%\n",
            "Epoch [119/250], Step [120/263], Loss: 0.1184, Accuracy: 95.94%\n",
            "Epoch [119/250], Step [130/263], Loss: 0.1214, Accuracy: 95.31%\n",
            "Epoch [119/250], Step [140/263], Loss: 0.1266, Accuracy: 94.84%\n",
            "Epoch [119/250], Step [150/263], Loss: 0.1342, Accuracy: 95.31%\n",
            "Epoch [119/250], Step [160/263], Loss: 0.1172, Accuracy: 95.16%\n",
            "Epoch [119/250], Step [170/263], Loss: 0.1446, Accuracy: 94.84%\n",
            "Epoch [119/250], Step [180/263], Loss: 0.1363, Accuracy: 95.00%\n",
            "Epoch [119/250], Step [190/263], Loss: 0.1319, Accuracy: 95.31%\n",
            "Epoch [119/250], Step [200/263], Loss: 0.1268, Accuracy: 94.53%\n",
            "Epoch [119/250], Step [210/263], Loss: 0.1475, Accuracy: 94.06%\n",
            "Epoch [119/250], Step [220/263], Loss: 0.1647, Accuracy: 92.34%\n",
            "Epoch [119/250], Step [230/263], Loss: 0.1125, Accuracy: 95.94%\n",
            "Epoch [119/250], Step [240/263], Loss: 0.1273, Accuracy: 95.31%\n",
            "Epoch [119/250], Step [250/263], Loss: 0.1433, Accuracy: 93.59%\n",
            "Epoch [119/250], Step [260/263], Loss: 0.1477, Accuracy: 95.47%\n",
            "Epoch [120/250], Step [10/263], Loss: 0.1417, Accuracy: 94.69%\n",
            "Epoch [120/250], Step [20/263], Loss: 0.1069, Accuracy: 96.72%\n",
            "Epoch [120/250], Step [30/263], Loss: 0.0982, Accuracy: 96.25%\n",
            "Epoch [120/250], Step [40/263], Loss: 0.1366, Accuracy: 95.16%\n",
            "Epoch [120/250], Step [50/263], Loss: 0.1054, Accuracy: 96.56%\n",
            "Epoch [120/250], Step [60/263], Loss: 0.1180, Accuracy: 95.47%\n",
            "Epoch [120/250], Step [70/263], Loss: 0.1205, Accuracy: 96.09%\n",
            "Epoch [120/250], Step [80/263], Loss: 0.0954, Accuracy: 96.25%\n",
            "Epoch [120/250], Step [90/263], Loss: 0.1711, Accuracy: 92.97%\n",
            "Epoch [120/250], Step [100/263], Loss: 0.1309, Accuracy: 94.84%\n",
            "Epoch [120/250], Step [110/263], Loss: 0.1094, Accuracy: 95.31%\n",
            "Epoch [120/250], Step [120/263], Loss: 0.1206, Accuracy: 95.31%\n",
            "Epoch [120/250], Step [130/263], Loss: 0.1334, Accuracy: 95.31%\n",
            "Epoch [120/250], Step [140/263], Loss: 0.1528, Accuracy: 93.75%\n",
            "Epoch [120/250], Step [150/263], Loss: 0.1165, Accuracy: 95.31%\n",
            "Epoch [120/250], Step [160/263], Loss: 0.1796, Accuracy: 92.66%\n",
            "Epoch [120/250], Step [170/263], Loss: 0.2102, Accuracy: 91.41%\n",
            "Epoch [120/250], Step [180/263], Loss: 0.2415, Accuracy: 90.47%\n",
            "Epoch [120/250], Step [190/263], Loss: 0.2655, Accuracy: 90.94%\n",
            "Epoch [120/250], Step [200/263], Loss: 0.2650, Accuracy: 91.56%\n",
            "Epoch [120/250], Step [210/263], Loss: 0.3724, Accuracy: 88.28%\n",
            "Epoch [120/250], Step [220/263], Loss: 2.0219, Accuracy: 59.06%\n",
            "Epoch [120/250], Step [230/263], Loss: 1.3156, Accuracy: 60.62%\n",
            "Epoch [120/250], Step [240/263], Loss: 1.0414, Accuracy: 63.28%\n",
            "Epoch [120/250], Step [250/263], Loss: 0.9859, Accuracy: 68.59%\n",
            "Epoch [120/250], Step [260/263], Loss: 0.8626, Accuracy: 70.00%\n",
            "Epoch [121/250], Step [10/263], Loss: 0.8914, Accuracy: 68.75%\n",
            "Epoch [121/250], Step [20/263], Loss: 0.7190, Accuracy: 74.84%\n",
            "Epoch [121/250], Step [30/263], Loss: 1.1645, Accuracy: 65.78%\n",
            "Epoch [121/250], Step [40/263], Loss: 1.3408, Accuracy: 58.13%\n",
            "Epoch [121/250], Step [50/263], Loss: 1.1090, Accuracy: 62.03%\n",
            "Epoch [121/250], Step [60/263], Loss: 1.0490, Accuracy: 62.97%\n",
            "Epoch [121/250], Step [70/263], Loss: 0.9722, Accuracy: 66.72%\n",
            "Epoch [121/250], Step [80/263], Loss: 0.9156, Accuracy: 66.88%\n",
            "Epoch [121/250], Step [90/263], Loss: 0.8819, Accuracy: 68.44%\n",
            "Epoch [121/250], Step [100/263], Loss: 0.7816, Accuracy: 71.56%\n",
            "Epoch [121/250], Step [110/263], Loss: 0.9033, Accuracy: 67.34%\n",
            "Epoch [121/250], Step [120/263], Loss: 0.8062, Accuracy: 69.69%\n",
            "Epoch [121/250], Step [130/263], Loss: 0.8392, Accuracy: 67.81%\n",
            "Epoch [121/250], Step [140/263], Loss: 0.7303, Accuracy: 72.34%\n",
            "Epoch [121/250], Step [150/263], Loss: 0.8243, Accuracy: 70.47%\n",
            "Epoch [121/250], Step [160/263], Loss: 0.7958, Accuracy: 70.78%\n",
            "Epoch [121/250], Step [170/263], Loss: 0.7535, Accuracy: 71.88%\n",
            "Epoch [121/250], Step [180/263], Loss: 0.6921, Accuracy: 74.84%\n",
            "Epoch [121/250], Step [190/263], Loss: 0.8768, Accuracy: 70.62%\n",
            "Epoch [121/250], Step [200/263], Loss: 1.0007, Accuracy: 72.19%\n",
            "Epoch [121/250], Step [210/263], Loss: 0.9662, Accuracy: 66.09%\n",
            "Epoch [121/250], Step [220/263], Loss: 1.0196, Accuracy: 65.47%\n",
            "Epoch [121/250], Step [230/263], Loss: 0.9445, Accuracy: 65.62%\n",
            "Epoch [121/250], Step [240/263], Loss: 0.8328, Accuracy: 68.59%\n",
            "Epoch [121/250], Step [250/263], Loss: 0.8187, Accuracy: 68.28%\n",
            "Epoch [121/250], Step [260/263], Loss: 0.8846, Accuracy: 68.75%\n",
            "Epoch [122/250], Step [10/263], Loss: 0.7540, Accuracy: 72.03%\n",
            "Epoch [122/250], Step [20/263], Loss: 0.8464, Accuracy: 69.06%\n",
            "Epoch [122/250], Step [30/263], Loss: 0.7630, Accuracy: 71.09%\n",
            "Epoch [122/250], Step [40/263], Loss: 1.1358, Accuracy: 65.47%\n",
            "Epoch [122/250], Step [50/263], Loss: 0.7707, Accuracy: 70.62%\n",
            "Epoch [122/250], Step [60/263], Loss: 0.6945, Accuracy: 73.91%\n",
            "Epoch [122/250], Step [70/263], Loss: 0.5686, Accuracy: 76.09%\n",
            "Epoch [122/250], Step [80/263], Loss: 0.5585, Accuracy: 78.12%\n",
            "Epoch [122/250], Step [90/263], Loss: 0.4267, Accuracy: 83.44%\n",
            "Epoch [122/250], Step [100/263], Loss: 0.4234, Accuracy: 83.44%\n",
            "Epoch [122/250], Step [110/263], Loss: 0.4004, Accuracy: 84.38%\n",
            "Epoch [122/250], Step [120/263], Loss: 0.3908, Accuracy: 83.12%\n",
            "Epoch [122/250], Step [130/263], Loss: 0.4078, Accuracy: 82.97%\n",
            "Epoch [122/250], Step [140/263], Loss: 0.3440, Accuracy: 86.09%\n",
            "Epoch [122/250], Step [150/263], Loss: 0.3558, Accuracy: 85.62%\n",
            "Epoch [122/250], Step [160/263], Loss: 0.3580, Accuracy: 85.31%\n",
            "Epoch [122/250], Step [170/263], Loss: 0.3970, Accuracy: 83.59%\n",
            "Epoch [122/250], Step [180/263], Loss: 0.3958, Accuracy: 84.06%\n",
            "Epoch [122/250], Step [190/263], Loss: 0.5331, Accuracy: 80.78%\n",
            "Epoch [122/250], Step [200/263], Loss: 0.3988, Accuracy: 83.75%\n",
            "Epoch [122/250], Step [210/263], Loss: 0.3816, Accuracy: 85.47%\n",
            "Epoch [122/250], Step [220/263], Loss: 0.3579, Accuracy: 85.00%\n",
            "Epoch [122/250], Step [230/263], Loss: 0.3566, Accuracy: 84.53%\n",
            "Epoch [122/250], Step [240/263], Loss: 0.3693, Accuracy: 84.22%\n",
            "Epoch [122/250], Step [250/263], Loss: 0.3562, Accuracy: 86.72%\n",
            "Epoch [122/250], Step [260/263], Loss: 0.3621, Accuracy: 84.38%\n",
            "Epoch [123/250], Step [10/263], Loss: 0.3282, Accuracy: 87.03%\n",
            "Epoch [123/250], Step [20/263], Loss: 0.3448, Accuracy: 85.62%\n",
            "Epoch [123/250], Step [30/263], Loss: 0.2958, Accuracy: 88.12%\n",
            "Epoch [123/250], Step [40/263], Loss: 0.3056, Accuracy: 87.66%\n",
            "Epoch [123/250], Step [50/263], Loss: 0.3181, Accuracy: 87.50%\n",
            "Epoch [123/250], Step [60/263], Loss: 0.3435, Accuracy: 87.03%\n",
            "Epoch [123/250], Step [70/263], Loss: 0.3222, Accuracy: 86.09%\n",
            "Epoch [123/250], Step [80/263], Loss: 0.3379, Accuracy: 86.56%\n",
            "Epoch [123/250], Step [90/263], Loss: 0.3339, Accuracy: 85.62%\n",
            "Epoch [123/250], Step [100/263], Loss: 0.3730, Accuracy: 83.12%\n",
            "Epoch [123/250], Step [110/263], Loss: 0.3209, Accuracy: 87.03%\n",
            "Epoch [123/250], Step [120/263], Loss: 0.2509, Accuracy: 90.00%\n",
            "Epoch [123/250], Step [130/263], Loss: 0.3100, Accuracy: 87.50%\n",
            "Epoch [123/250], Step [140/263], Loss: 0.2945, Accuracy: 88.44%\n",
            "Epoch [123/250], Step [150/263], Loss: 0.2634, Accuracy: 88.44%\n",
            "Epoch [123/250], Step [160/263], Loss: 0.2605, Accuracy: 89.38%\n",
            "Epoch [123/250], Step [170/263], Loss: 0.3125, Accuracy: 87.50%\n",
            "Epoch [123/250], Step [180/263], Loss: 0.2737, Accuracy: 88.12%\n",
            "Epoch [123/250], Step [190/263], Loss: 0.3060, Accuracy: 87.50%\n",
            "Epoch [123/250], Step [200/263], Loss: 0.3137, Accuracy: 86.41%\n",
            "Epoch [123/250], Step [210/263], Loss: 0.2807, Accuracy: 88.44%\n",
            "Epoch [123/250], Step [220/263], Loss: 0.2752, Accuracy: 87.81%\n",
            "Epoch [123/250], Step [230/263], Loss: 0.2958, Accuracy: 88.59%\n",
            "Epoch [123/250], Step [240/263], Loss: 0.2711, Accuracy: 89.06%\n",
            "Epoch [123/250], Step [250/263], Loss: 0.3124, Accuracy: 87.50%\n",
            "Epoch [123/250], Step [260/263], Loss: 0.2616, Accuracy: 89.84%\n",
            "Epoch [124/250], Step [10/263], Loss: 0.2614, Accuracy: 89.22%\n",
            "Epoch [124/250], Step [20/263], Loss: 0.2648, Accuracy: 88.75%\n",
            "Epoch [124/250], Step [30/263], Loss: 0.2593, Accuracy: 90.31%\n",
            "Epoch [124/250], Step [40/263], Loss: 0.2323, Accuracy: 90.47%\n",
            "Epoch [124/250], Step [50/263], Loss: 0.2437, Accuracy: 90.16%\n",
            "Epoch [124/250], Step [60/263], Loss: 0.3056, Accuracy: 88.91%\n",
            "Epoch [124/250], Step [70/263], Loss: 0.2735, Accuracy: 88.75%\n",
            "Epoch [124/250], Step [80/263], Loss: 0.2419, Accuracy: 90.31%\n",
            "Epoch [124/250], Step [90/263], Loss: 0.2284, Accuracy: 90.47%\n",
            "Epoch [124/250], Step [100/263], Loss: 0.2282, Accuracy: 90.62%\n",
            "Epoch [124/250], Step [110/263], Loss: 0.2382, Accuracy: 90.62%\n",
            "Epoch [124/250], Step [120/263], Loss: 0.2407, Accuracy: 91.72%\n",
            "Epoch [124/250], Step [130/263], Loss: 0.2534, Accuracy: 89.69%\n",
            "Epoch [124/250], Step [140/263], Loss: 0.2509, Accuracy: 88.59%\n",
            "Epoch [124/250], Step [150/263], Loss: 0.2397, Accuracy: 90.31%\n",
            "Epoch [124/250], Step [160/263], Loss: 0.2530, Accuracy: 89.22%\n",
            "Epoch [124/250], Step [170/263], Loss: 0.2515, Accuracy: 90.31%\n",
            "Epoch [124/250], Step [180/263], Loss: 0.2323, Accuracy: 90.94%\n",
            "Epoch [124/250], Step [190/263], Loss: 0.2622, Accuracy: 88.91%\n",
            "Epoch [124/250], Step [200/263], Loss: 0.2503, Accuracy: 90.00%\n",
            "Epoch [124/250], Step [210/263], Loss: 0.2460, Accuracy: 88.28%\n",
            "Epoch [124/250], Step [220/263], Loss: 0.2661, Accuracy: 87.97%\n",
            "Epoch [124/250], Step [230/263], Loss: 0.2690, Accuracy: 90.00%\n",
            "Epoch [124/250], Step [240/263], Loss: 0.2431, Accuracy: 89.22%\n",
            "Epoch [124/250], Step [250/263], Loss: 0.2115, Accuracy: 91.72%\n",
            "Epoch [124/250], Step [260/263], Loss: 0.2381, Accuracy: 90.78%\n",
            "Epoch [125/250], Step [10/263], Loss: 0.2140, Accuracy: 92.03%\n",
            "Epoch [125/250], Step [20/263], Loss: 0.2102, Accuracy: 90.94%\n",
            "Epoch [125/250], Step [30/263], Loss: 0.2523, Accuracy: 89.53%\n",
            "Epoch [125/250], Step [40/263], Loss: 0.2372, Accuracy: 90.16%\n",
            "Epoch [125/250], Step [50/263], Loss: 0.1663, Accuracy: 93.44%\n",
            "Epoch [125/250], Step [60/263], Loss: 0.2123, Accuracy: 91.88%\n",
            "Epoch [125/250], Step [70/263], Loss: 0.1744, Accuracy: 93.12%\n",
            "Epoch [125/250], Step [80/263], Loss: 0.2187, Accuracy: 90.47%\n",
            "Epoch [125/250], Step [90/263], Loss: 0.2079, Accuracy: 91.88%\n",
            "Epoch [125/250], Step [100/263], Loss: 0.2206, Accuracy: 90.47%\n",
            "Epoch [125/250], Step [110/263], Loss: 0.2201, Accuracy: 90.47%\n",
            "Epoch [125/250], Step [120/263], Loss: 0.2138, Accuracy: 92.50%\n",
            "Epoch [125/250], Step [130/263], Loss: 0.2059, Accuracy: 91.88%\n",
            "Epoch [125/250], Step [140/263], Loss: 0.1932, Accuracy: 91.09%\n",
            "Epoch [125/250], Step [150/263], Loss: 0.1967, Accuracy: 91.88%\n",
            "Epoch [125/250], Step [160/263], Loss: 0.1815, Accuracy: 93.12%\n",
            "Epoch [125/250], Step [170/263], Loss: 0.2149, Accuracy: 92.50%\n",
            "Epoch [125/250], Step [180/263], Loss: 0.2363, Accuracy: 91.72%\n",
            "Epoch [125/250], Step [190/263], Loss: 0.1868, Accuracy: 92.34%\n",
            "Epoch [125/250], Step [200/263], Loss: 0.2008, Accuracy: 91.72%\n",
            "Epoch [125/250], Step [210/263], Loss: 0.2222, Accuracy: 90.31%\n",
            "Epoch [125/250], Step [220/263], Loss: 0.2245, Accuracy: 90.16%\n",
            "Epoch [125/250], Step [230/263], Loss: 0.1970, Accuracy: 92.50%\n",
            "Epoch [125/250], Step [240/263], Loss: 0.1890, Accuracy: 92.97%\n",
            "Epoch [125/250], Step [250/263], Loss: 0.2063, Accuracy: 91.41%\n",
            "Epoch [125/250], Step [260/263], Loss: 0.2193, Accuracy: 91.25%\n",
            "Epoch [126/250], Step [10/263], Loss: 0.1919, Accuracy: 92.34%\n",
            "Epoch [126/250], Step [20/263], Loss: 0.1834, Accuracy: 93.44%\n",
            "Epoch [126/250], Step [30/263], Loss: 0.1763, Accuracy: 92.97%\n",
            "Epoch [126/250], Step [40/263], Loss: 0.1650, Accuracy: 94.38%\n",
            "Epoch [126/250], Step [50/263], Loss: 0.1592, Accuracy: 94.06%\n",
            "Epoch [126/250], Step [60/263], Loss: 0.1711, Accuracy: 93.59%\n",
            "Epoch [126/250], Step [70/263], Loss: 0.1877, Accuracy: 91.72%\n",
            "Epoch [126/250], Step [80/263], Loss: 0.1703, Accuracy: 93.59%\n",
            "Epoch [126/250], Step [90/263], Loss: 0.1697, Accuracy: 93.12%\n",
            "Epoch [126/250], Step [100/263], Loss: 0.1522, Accuracy: 93.28%\n",
            "Epoch [126/250], Step [110/263], Loss: 0.2043, Accuracy: 90.31%\n",
            "Epoch [126/250], Step [120/263], Loss: 0.1939, Accuracy: 92.03%\n",
            "Epoch [126/250], Step [130/263], Loss: 0.1600, Accuracy: 93.75%\n",
            "Epoch [126/250], Step [140/263], Loss: 0.1491, Accuracy: 93.91%\n",
            "Epoch [126/250], Step [150/263], Loss: 0.1712, Accuracy: 93.28%\n",
            "Epoch [126/250], Step [160/263], Loss: 0.1864, Accuracy: 92.66%\n",
            "Epoch [126/250], Step [170/263], Loss: 0.1534, Accuracy: 93.75%\n",
            "Epoch [126/250], Step [180/263], Loss: 0.1806, Accuracy: 92.81%\n",
            "Epoch [126/250], Step [190/263], Loss: 0.2040, Accuracy: 92.34%\n",
            "Epoch [126/250], Step [200/263], Loss: 0.1899, Accuracy: 93.12%\n",
            "Epoch [126/250], Step [210/263], Loss: 0.1420, Accuracy: 94.84%\n",
            "Epoch [126/250], Step [220/263], Loss: 0.1696, Accuracy: 92.50%\n",
            "Epoch [126/250], Step [230/263], Loss: 0.1794, Accuracy: 92.50%\n",
            "Epoch [126/250], Step [240/263], Loss: 0.1628, Accuracy: 93.59%\n",
            "Epoch [126/250], Step [250/263], Loss: 0.1701, Accuracy: 92.81%\n",
            "Epoch [126/250], Step [260/263], Loss: 0.2023, Accuracy: 91.41%\n",
            "Epoch [127/250], Step [10/263], Loss: 0.1603, Accuracy: 94.38%\n",
            "Epoch [127/250], Step [20/263], Loss: 0.1680, Accuracy: 94.06%\n",
            "Epoch [127/250], Step [30/263], Loss: 0.1608, Accuracy: 93.59%\n",
            "Epoch [127/250], Step [40/263], Loss: 0.1518, Accuracy: 94.06%\n",
            "Epoch [127/250], Step [50/263], Loss: 0.1698, Accuracy: 93.75%\n",
            "Epoch [127/250], Step [60/263], Loss: 0.1377, Accuracy: 94.53%\n",
            "Epoch [127/250], Step [70/263], Loss: 0.1599, Accuracy: 92.66%\n",
            "Epoch [127/250], Step [80/263], Loss: 0.1470, Accuracy: 94.38%\n",
            "Epoch [127/250], Step [90/263], Loss: 0.1565, Accuracy: 92.97%\n",
            "Epoch [127/250], Step [100/263], Loss: 0.1538, Accuracy: 95.16%\n",
            "Epoch [127/250], Step [110/263], Loss: 0.1520, Accuracy: 94.53%\n",
            "Epoch [127/250], Step [120/263], Loss: 0.1522, Accuracy: 94.84%\n",
            "Epoch [127/250], Step [130/263], Loss: 0.1331, Accuracy: 94.69%\n",
            "Epoch [127/250], Step [140/263], Loss: 0.1326, Accuracy: 95.00%\n",
            "Epoch [127/250], Step [150/263], Loss: 0.1707, Accuracy: 94.06%\n",
            "Epoch [127/250], Step [160/263], Loss: 0.1537, Accuracy: 94.22%\n",
            "Epoch [127/250], Step [170/263], Loss: 0.1800, Accuracy: 93.59%\n",
            "Epoch [127/250], Step [180/263], Loss: 0.1294, Accuracy: 95.00%\n",
            "Epoch [127/250], Step [190/263], Loss: 0.1685, Accuracy: 92.34%\n",
            "Epoch [127/250], Step [200/263], Loss: 0.1766, Accuracy: 92.34%\n",
            "Epoch [127/250], Step [210/263], Loss: 0.1375, Accuracy: 95.62%\n",
            "Epoch [127/250], Step [220/263], Loss: 0.1471, Accuracy: 94.69%\n",
            "Epoch [127/250], Step [230/263], Loss: 0.1366, Accuracy: 95.47%\n",
            "Epoch [127/250], Step [240/263], Loss: 0.1513, Accuracy: 94.06%\n",
            "Epoch [127/250], Step [250/263], Loss: 0.1575, Accuracy: 93.75%\n",
            "Epoch [127/250], Step [260/263], Loss: 0.1825, Accuracy: 92.97%\n",
            "Epoch [128/250], Step [10/263], Loss: 0.1337, Accuracy: 95.31%\n",
            "Epoch [128/250], Step [20/263], Loss: 0.1602, Accuracy: 94.53%\n",
            "Epoch [128/250], Step [30/263], Loss: 0.1235, Accuracy: 95.94%\n",
            "Epoch [128/250], Step [40/263], Loss: 0.1538, Accuracy: 93.28%\n",
            "Epoch [128/250], Step [50/263], Loss: 0.1584, Accuracy: 93.91%\n",
            "Epoch [128/250], Step [60/263], Loss: 0.1472, Accuracy: 94.84%\n",
            "Epoch [128/250], Step [70/263], Loss: 0.1668, Accuracy: 94.06%\n",
            "Epoch [128/250], Step [80/263], Loss: 0.1409, Accuracy: 95.00%\n",
            "Epoch [128/250], Step [90/263], Loss: 0.1588, Accuracy: 94.22%\n",
            "Epoch [128/250], Step [100/263], Loss: 0.1484, Accuracy: 93.75%\n",
            "Epoch [128/250], Step [110/263], Loss: 0.1527, Accuracy: 94.06%\n",
            "Epoch [128/250], Step [120/263], Loss: 0.1318, Accuracy: 96.09%\n",
            "Epoch [128/250], Step [130/263], Loss: 0.1565, Accuracy: 94.38%\n",
            "Epoch [128/250], Step [140/263], Loss: 0.1525, Accuracy: 94.38%\n",
            "Epoch [128/250], Step [150/263], Loss: 0.1499, Accuracy: 94.38%\n",
            "Epoch [128/250], Step [160/263], Loss: 0.1517, Accuracy: 93.44%\n",
            "Epoch [128/250], Step [170/263], Loss: 0.1295, Accuracy: 95.31%\n",
            "Epoch [128/250], Step [180/263], Loss: 0.1715, Accuracy: 92.81%\n",
            "Epoch [128/250], Step [190/263], Loss: 0.1171, Accuracy: 95.62%\n",
            "Epoch [128/250], Step [200/263], Loss: 0.1465, Accuracy: 95.00%\n",
            "Epoch [128/250], Step [210/263], Loss: 0.1590, Accuracy: 93.44%\n",
            "Epoch [128/250], Step [220/263], Loss: 0.1195, Accuracy: 95.78%\n",
            "Epoch [128/250], Step [230/263], Loss: 0.1287, Accuracy: 93.75%\n",
            "Epoch [128/250], Step [240/263], Loss: 0.1535, Accuracy: 93.75%\n",
            "Epoch [128/250], Step [250/263], Loss: 0.1407, Accuracy: 94.69%\n",
            "Epoch [128/250], Step [260/263], Loss: 0.1534, Accuracy: 94.38%\n",
            "Epoch [129/250], Step [10/263], Loss: 0.1159, Accuracy: 96.72%\n",
            "Epoch [129/250], Step [20/263], Loss: 0.1373, Accuracy: 94.84%\n",
            "Epoch [129/250], Step [30/263], Loss: 0.1392, Accuracy: 94.84%\n",
            "Epoch [129/250], Step [40/263], Loss: 0.1283, Accuracy: 95.47%\n",
            "Epoch [129/250], Step [50/263], Loss: 0.1109, Accuracy: 95.78%\n",
            "Epoch [129/250], Step [60/263], Loss: 0.1137, Accuracy: 95.78%\n",
            "Epoch [129/250], Step [70/263], Loss: 0.1451, Accuracy: 93.91%\n",
            "Epoch [129/250], Step [80/263], Loss: 0.1302, Accuracy: 94.69%\n",
            "Epoch [129/250], Step [90/263], Loss: 0.1166, Accuracy: 95.16%\n",
            "Epoch [129/250], Step [100/263], Loss: 0.1610, Accuracy: 93.75%\n",
            "Epoch [129/250], Step [110/263], Loss: 0.1592, Accuracy: 93.75%\n",
            "Epoch [129/250], Step [120/263], Loss: 0.0927, Accuracy: 97.19%\n",
            "Epoch [129/250], Step [130/263], Loss: 0.1200, Accuracy: 95.94%\n",
            "Epoch [129/250], Step [140/263], Loss: 0.1383, Accuracy: 94.84%\n",
            "Epoch [129/250], Step [150/263], Loss: 0.1327, Accuracy: 94.06%\n",
            "Epoch [129/250], Step [160/263], Loss: 0.1416, Accuracy: 95.00%\n",
            "Epoch [129/250], Step [170/263], Loss: 0.1474, Accuracy: 94.06%\n",
            "Epoch [129/250], Step [180/263], Loss: 0.1624, Accuracy: 93.75%\n",
            "Epoch [129/250], Step [190/263], Loss: 0.1548, Accuracy: 94.38%\n",
            "Epoch [129/250], Step [200/263], Loss: 0.1453, Accuracy: 94.69%\n",
            "Epoch [129/250], Step [210/263], Loss: 0.1718, Accuracy: 93.91%\n",
            "Epoch [129/250], Step [220/263], Loss: 0.1640, Accuracy: 94.22%\n",
            "Epoch [129/250], Step [230/263], Loss: 0.1381, Accuracy: 95.31%\n",
            "Epoch [129/250], Step [240/263], Loss: 0.1466, Accuracy: 94.53%\n",
            "Epoch [129/250], Step [250/263], Loss: 0.1454, Accuracy: 94.06%\n",
            "Epoch [129/250], Step [260/263], Loss: 0.1357, Accuracy: 94.22%\n",
            "Epoch [130/250], Step [10/263], Loss: 0.1235, Accuracy: 95.78%\n",
            "Epoch [130/250], Step [20/263], Loss: 0.1354, Accuracy: 96.09%\n",
            "Epoch [130/250], Step [30/263], Loss: 0.1119, Accuracy: 95.47%\n",
            "Epoch [130/250], Step [40/263], Loss: 0.1019, Accuracy: 95.94%\n",
            "Epoch [130/250], Step [50/263], Loss: 0.1372, Accuracy: 94.69%\n",
            "Epoch [130/250], Step [60/263], Loss: 0.1142, Accuracy: 95.94%\n",
            "Epoch [130/250], Step [70/263], Loss: 0.1328, Accuracy: 94.53%\n",
            "Epoch [130/250], Step [80/263], Loss: 0.1004, Accuracy: 96.41%\n",
            "Epoch [130/250], Step [90/263], Loss: 0.1260, Accuracy: 94.69%\n",
            "Epoch [130/250], Step [100/263], Loss: 0.1166, Accuracy: 95.62%\n",
            "Epoch [130/250], Step [110/263], Loss: 0.1418, Accuracy: 94.38%\n",
            "Epoch [130/250], Step [120/263], Loss: 0.1306, Accuracy: 95.31%\n",
            "Epoch [130/250], Step [130/263], Loss: 0.1461, Accuracy: 94.22%\n",
            "Epoch [130/250], Step [140/263], Loss: 0.1654, Accuracy: 94.06%\n",
            "Epoch [130/250], Step [150/263], Loss: 0.1361, Accuracy: 95.31%\n",
            "Epoch [130/250], Step [160/263], Loss: 0.1283, Accuracy: 95.00%\n",
            "Epoch [130/250], Step [170/263], Loss: 0.0980, Accuracy: 96.72%\n",
            "Epoch [130/250], Step [180/263], Loss: 0.1269, Accuracy: 94.38%\n",
            "Epoch [130/250], Step [190/263], Loss: 0.1301, Accuracy: 95.62%\n",
            "Epoch [130/250], Step [200/263], Loss: 0.1400, Accuracy: 94.69%\n",
            "Epoch [130/250], Step [210/263], Loss: 0.1675, Accuracy: 92.81%\n",
            "Epoch [130/250], Step [220/263], Loss: 0.1112, Accuracy: 95.94%\n",
            "Epoch [130/250], Step [230/263], Loss: 0.1503, Accuracy: 94.22%\n",
            "Epoch [130/250], Step [240/263], Loss: 0.1389, Accuracy: 94.84%\n",
            "Epoch [130/250], Step [250/263], Loss: 0.1028, Accuracy: 96.72%\n",
            "Epoch [130/250], Step [260/263], Loss: 0.1377, Accuracy: 93.44%\n",
            "Epoch [131/250], Step [10/263], Loss: 0.1032, Accuracy: 96.88%\n",
            "Epoch [131/250], Step [20/263], Loss: 0.0846, Accuracy: 97.66%\n",
            "Epoch [131/250], Step [30/263], Loss: 0.1217, Accuracy: 94.53%\n",
            "Epoch [131/250], Step [40/263], Loss: 0.1010, Accuracy: 97.19%\n",
            "Epoch [131/250], Step [50/263], Loss: 0.1025, Accuracy: 96.09%\n",
            "Epoch [131/250], Step [60/263], Loss: 0.1316, Accuracy: 94.84%\n",
            "Epoch [131/250], Step [70/263], Loss: 0.1231, Accuracy: 95.47%\n",
            "Epoch [131/250], Step [80/263], Loss: 0.1344, Accuracy: 95.00%\n",
            "Epoch [131/250], Step [90/263], Loss: 0.1326, Accuracy: 95.62%\n",
            "Epoch [131/250], Step [100/263], Loss: 0.1101, Accuracy: 95.94%\n",
            "Epoch [131/250], Step [110/263], Loss: 0.0973, Accuracy: 96.25%\n",
            "Epoch [131/250], Step [120/263], Loss: 0.1240, Accuracy: 94.53%\n",
            "Epoch [131/250], Step [130/263], Loss: 0.1266, Accuracy: 95.16%\n",
            "Epoch [131/250], Step [140/263], Loss: 0.1224, Accuracy: 93.59%\n",
            "Epoch [131/250], Step [150/263], Loss: 0.1406, Accuracy: 94.53%\n",
            "Epoch [131/250], Step [160/263], Loss: 0.1115, Accuracy: 95.31%\n",
            "Epoch [131/250], Step [170/263], Loss: 0.1173, Accuracy: 95.62%\n",
            "Epoch [131/250], Step [180/263], Loss: 0.1322, Accuracy: 95.00%\n",
            "Epoch [131/250], Step [190/263], Loss: 0.1053, Accuracy: 95.78%\n",
            "Epoch [131/250], Step [200/263], Loss: 0.1664, Accuracy: 93.91%\n",
            "Epoch [131/250], Step [210/263], Loss: 0.1271, Accuracy: 95.31%\n",
            "Epoch [131/250], Step [220/263], Loss: 0.1200, Accuracy: 95.62%\n",
            "Epoch [131/250], Step [230/263], Loss: 0.0994, Accuracy: 95.94%\n",
            "Epoch [131/250], Step [240/263], Loss: 0.1252, Accuracy: 95.16%\n",
            "Epoch [131/250], Step [250/263], Loss: 0.1082, Accuracy: 95.94%\n",
            "Epoch [131/250], Step [260/263], Loss: 0.1348, Accuracy: 94.38%\n",
            "Epoch [132/250], Step [10/263], Loss: 0.0967, Accuracy: 95.78%\n",
            "Epoch [132/250], Step [20/263], Loss: 0.1244, Accuracy: 95.31%\n",
            "Epoch [132/250], Step [30/263], Loss: 0.1250, Accuracy: 95.78%\n",
            "Epoch [132/250], Step [40/263], Loss: 0.0744, Accuracy: 97.81%\n",
            "Epoch [132/250], Step [50/263], Loss: 0.1275, Accuracy: 95.31%\n",
            "Epoch [132/250], Step [60/263], Loss: 0.1059, Accuracy: 95.94%\n",
            "Epoch [132/250], Step [70/263], Loss: 0.1100, Accuracy: 95.31%\n",
            "Epoch [132/250], Step [80/263], Loss: 0.1247, Accuracy: 94.53%\n",
            "Epoch [132/250], Step [90/263], Loss: 0.1282, Accuracy: 95.16%\n",
            "Epoch [132/250], Step [100/263], Loss: 0.1221, Accuracy: 94.69%\n",
            "Epoch [132/250], Step [110/263], Loss: 0.1206, Accuracy: 95.47%\n",
            "Epoch [132/250], Step [120/263], Loss: 0.1132, Accuracy: 96.09%\n",
            "Epoch [132/250], Step [130/263], Loss: 0.1195, Accuracy: 96.09%\n",
            "Epoch [132/250], Step [140/263], Loss: 0.0970, Accuracy: 96.09%\n",
            "Epoch [132/250], Step [150/263], Loss: 0.0880, Accuracy: 97.03%\n",
            "Epoch [132/250], Step [160/263], Loss: 0.1134, Accuracy: 96.72%\n",
            "Epoch [132/250], Step [170/263], Loss: 0.1304, Accuracy: 94.53%\n",
            "Epoch [132/250], Step [180/263], Loss: 0.1246, Accuracy: 95.16%\n",
            "Epoch [132/250], Step [190/263], Loss: 0.1294, Accuracy: 95.00%\n",
            "Epoch [132/250], Step [200/263], Loss: 0.1235, Accuracy: 95.78%\n",
            "Epoch [132/250], Step [210/263], Loss: 0.1238, Accuracy: 95.31%\n",
            "Epoch [132/250], Step [220/263], Loss: 0.1078, Accuracy: 96.72%\n",
            "Epoch [132/250], Step [230/263], Loss: 0.1016, Accuracy: 97.03%\n",
            "Epoch [132/250], Step [240/263], Loss: 0.1166, Accuracy: 94.69%\n",
            "Epoch [132/250], Step [250/263], Loss: 0.1411, Accuracy: 95.31%\n",
            "Epoch [132/250], Step [260/263], Loss: 0.1093, Accuracy: 95.78%\n",
            "Epoch [133/250], Step [10/263], Loss: 0.1148, Accuracy: 95.78%\n",
            "Epoch [133/250], Step [20/263], Loss: 0.1020, Accuracy: 96.56%\n",
            "Epoch [133/250], Step [30/263], Loss: 0.1038, Accuracy: 96.09%\n",
            "Epoch [133/250], Step [40/263], Loss: 0.1110, Accuracy: 96.09%\n",
            "Epoch [133/250], Step [50/263], Loss: 0.1147, Accuracy: 95.31%\n",
            "Epoch [133/250], Step [60/263], Loss: 0.0947, Accuracy: 96.25%\n",
            "Epoch [133/250], Step [70/263], Loss: 0.1045, Accuracy: 95.94%\n",
            "Epoch [133/250], Step [80/263], Loss: 0.0982, Accuracy: 96.25%\n",
            "Epoch [133/250], Step [90/263], Loss: 0.0993, Accuracy: 96.56%\n",
            "Epoch [133/250], Step [100/263], Loss: 0.0883, Accuracy: 97.34%\n",
            "Epoch [133/250], Step [110/263], Loss: 0.1198, Accuracy: 96.41%\n",
            "Epoch [133/250], Step [120/263], Loss: 0.1196, Accuracy: 96.09%\n",
            "Epoch [133/250], Step [130/263], Loss: 0.1055, Accuracy: 96.41%\n",
            "Epoch [133/250], Step [140/263], Loss: 0.1342, Accuracy: 94.22%\n",
            "Epoch [133/250], Step [150/263], Loss: 0.1085, Accuracy: 95.47%\n",
            "Epoch [133/250], Step [160/263], Loss: 0.1126, Accuracy: 96.41%\n",
            "Epoch [133/250], Step [170/263], Loss: 0.0958, Accuracy: 96.56%\n",
            "Epoch [133/250], Step [180/263], Loss: 0.0984, Accuracy: 96.56%\n",
            "Epoch [133/250], Step [190/263], Loss: 0.1037, Accuracy: 95.78%\n",
            "Epoch [133/250], Step [200/263], Loss: 0.1146, Accuracy: 95.16%\n",
            "Epoch [133/250], Step [210/263], Loss: 0.0870, Accuracy: 96.72%\n",
            "Epoch [133/250], Step [220/263], Loss: 0.1153, Accuracy: 95.62%\n",
            "Epoch [133/250], Step [230/263], Loss: 0.1068, Accuracy: 95.78%\n",
            "Epoch [133/250], Step [240/263], Loss: 0.1336, Accuracy: 95.31%\n",
            "Epoch [133/250], Step [250/263], Loss: 0.1020, Accuracy: 95.62%\n",
            "Epoch [133/250], Step [260/263], Loss: 0.1183, Accuracy: 95.47%\n",
            "Epoch [134/250], Step [10/263], Loss: 0.0852, Accuracy: 97.03%\n",
            "Epoch [134/250], Step [20/263], Loss: 0.1054, Accuracy: 96.72%\n",
            "Epoch [134/250], Step [30/263], Loss: 0.0969, Accuracy: 97.19%\n",
            "Epoch [134/250], Step [40/263], Loss: 0.1100, Accuracy: 95.47%\n",
            "Epoch [134/250], Step [50/263], Loss: 0.1157, Accuracy: 95.78%\n",
            "Epoch [134/250], Step [60/263], Loss: 0.0854, Accuracy: 97.19%\n",
            "Epoch [134/250], Step [70/263], Loss: 0.0780, Accuracy: 98.12%\n",
            "Epoch [134/250], Step [80/263], Loss: 0.0898, Accuracy: 97.50%\n",
            "Epoch [134/250], Step [90/263], Loss: 0.1030, Accuracy: 96.09%\n",
            "Epoch [134/250], Step [100/263], Loss: 0.0964, Accuracy: 96.41%\n",
            "Epoch [134/250], Step [110/263], Loss: 0.0927, Accuracy: 97.34%\n",
            "Epoch [134/250], Step [120/263], Loss: 0.1061, Accuracy: 96.25%\n",
            "Epoch [134/250], Step [130/263], Loss: 0.1090, Accuracy: 97.19%\n",
            "Epoch [134/250], Step [140/263], Loss: 0.0842, Accuracy: 96.88%\n",
            "Epoch [134/250], Step [150/263], Loss: 0.1042, Accuracy: 95.78%\n",
            "Epoch [134/250], Step [160/263], Loss: 0.1032, Accuracy: 96.25%\n",
            "Epoch [134/250], Step [170/263], Loss: 0.1051, Accuracy: 96.56%\n",
            "Epoch [134/250], Step [180/263], Loss: 0.1185, Accuracy: 95.00%\n",
            "Epoch [134/250], Step [190/263], Loss: 0.1178, Accuracy: 95.78%\n",
            "Epoch [134/250], Step [200/263], Loss: 0.0938, Accuracy: 95.78%\n",
            "Epoch [134/250], Step [210/263], Loss: 0.0859, Accuracy: 97.34%\n",
            "Epoch [134/250], Step [220/263], Loss: 0.1196, Accuracy: 95.16%\n",
            "Epoch [134/250], Step [230/263], Loss: 0.1022, Accuracy: 96.09%\n",
            "Epoch [134/250], Step [240/263], Loss: 0.1135, Accuracy: 95.31%\n",
            "Epoch [134/250], Step [250/263], Loss: 0.1048, Accuracy: 96.72%\n",
            "Epoch [134/250], Step [260/263], Loss: 0.1265, Accuracy: 95.16%\n",
            "Epoch [135/250], Step [10/263], Loss: 0.0873, Accuracy: 97.34%\n",
            "Epoch [135/250], Step [20/263], Loss: 0.0921, Accuracy: 96.25%\n",
            "Epoch [135/250], Step [30/263], Loss: 0.1087, Accuracy: 96.09%\n",
            "Epoch [135/250], Step [40/263], Loss: 0.1162, Accuracy: 95.31%\n",
            "Epoch [135/250], Step [50/263], Loss: 0.0952, Accuracy: 96.88%\n",
            "Epoch [135/250], Step [60/263], Loss: 0.1270, Accuracy: 94.22%\n",
            "Epoch [135/250], Step [70/263], Loss: 0.0911, Accuracy: 96.72%\n",
            "Epoch [135/250], Step [80/263], Loss: 0.1331, Accuracy: 95.00%\n",
            "Epoch [135/250], Step [90/263], Loss: 0.1089, Accuracy: 95.94%\n",
            "Epoch [135/250], Step [100/263], Loss: 0.1469, Accuracy: 94.38%\n",
            "Epoch [135/250], Step [110/263], Loss: 0.1475, Accuracy: 94.06%\n",
            "Epoch [135/250], Step [120/263], Loss: 0.1299, Accuracy: 95.16%\n",
            "Epoch [135/250], Step [130/263], Loss: 0.1122, Accuracy: 96.09%\n",
            "Epoch [135/250], Step [140/263], Loss: 0.1239, Accuracy: 96.25%\n",
            "Epoch [135/250], Step [150/263], Loss: 0.1121, Accuracy: 95.31%\n",
            "Epoch [135/250], Step [160/263], Loss: 0.1033, Accuracy: 96.88%\n",
            "Epoch [135/250], Step [170/263], Loss: 0.0992, Accuracy: 96.25%\n",
            "Epoch [135/250], Step [180/263], Loss: 0.0996, Accuracy: 95.62%\n",
            "Epoch [135/250], Step [190/263], Loss: 0.0901, Accuracy: 96.25%\n",
            "Epoch [135/250], Step [200/263], Loss: 0.1438, Accuracy: 94.69%\n",
            "Epoch [135/250], Step [210/263], Loss: 0.1069, Accuracy: 95.78%\n",
            "Epoch [135/250], Step [220/263], Loss: 0.1280, Accuracy: 95.78%\n",
            "Epoch [135/250], Step [230/263], Loss: 0.1159, Accuracy: 94.84%\n",
            "Epoch [135/250], Step [240/263], Loss: 0.1391, Accuracy: 94.06%\n",
            "Epoch [135/250], Step [250/263], Loss: 0.1174, Accuracy: 96.41%\n",
            "Epoch [135/250], Step [260/263], Loss: 0.1045, Accuracy: 95.94%\n",
            "Epoch [136/250], Step [10/263], Loss: 0.0951, Accuracy: 96.72%\n",
            "Epoch [136/250], Step [20/263], Loss: 0.1336, Accuracy: 94.06%\n",
            "Epoch [136/250], Step [30/263], Loss: 0.1133, Accuracy: 95.31%\n",
            "Epoch [136/250], Step [40/263], Loss: 0.1075, Accuracy: 95.78%\n",
            "Epoch [136/250], Step [50/263], Loss: 0.1241, Accuracy: 95.00%\n",
            "Epoch [136/250], Step [60/263], Loss: 0.0965, Accuracy: 97.03%\n",
            "Epoch [136/250], Step [70/263], Loss: 0.1079, Accuracy: 96.25%\n",
            "Epoch [136/250], Step [80/263], Loss: 0.1399, Accuracy: 94.53%\n",
            "Epoch [136/250], Step [90/263], Loss: 0.0844, Accuracy: 97.03%\n",
            "Epoch [136/250], Step [100/263], Loss: 0.1065, Accuracy: 95.31%\n",
            "Epoch [136/250], Step [110/263], Loss: 0.1569, Accuracy: 93.28%\n",
            "Epoch [136/250], Step [120/263], Loss: 0.1243, Accuracy: 94.53%\n",
            "Epoch [136/250], Step [130/263], Loss: 0.1035, Accuracy: 96.25%\n",
            "Epoch [136/250], Step [140/263], Loss: 0.1268, Accuracy: 94.38%\n",
            "Epoch [136/250], Step [150/263], Loss: 0.1172, Accuracy: 95.47%\n",
            "Epoch [136/250], Step [160/263], Loss: 0.0969, Accuracy: 96.72%\n",
            "Epoch [136/250], Step [170/263], Loss: 0.1126, Accuracy: 94.69%\n",
            "Epoch [136/250], Step [180/263], Loss: 0.0993, Accuracy: 97.03%\n",
            "Epoch [136/250], Step [190/263], Loss: 0.0861, Accuracy: 96.56%\n",
            "Epoch [136/250], Step [200/263], Loss: 0.0971, Accuracy: 95.62%\n",
            "Epoch [136/250], Step [210/263], Loss: 0.0953, Accuracy: 97.19%\n",
            "Epoch [136/250], Step [220/263], Loss: 0.1196, Accuracy: 95.16%\n",
            "Epoch [136/250], Step [230/263], Loss: 0.1303, Accuracy: 94.06%\n",
            "Epoch [136/250], Step [240/263], Loss: 0.1181, Accuracy: 95.31%\n",
            "Epoch [136/250], Step [250/263], Loss: 0.1025, Accuracy: 96.09%\n",
            "Epoch [136/250], Step [260/263], Loss: 0.1180, Accuracy: 95.47%\n",
            "Epoch [137/250], Step [10/263], Loss: 0.1343, Accuracy: 94.38%\n",
            "Epoch [137/250], Step [20/263], Loss: 0.1884, Accuracy: 92.50%\n",
            "Epoch [137/250], Step [30/263], Loss: 0.2036, Accuracy: 91.72%\n",
            "Epoch [137/250], Step [40/263], Loss: 0.2301, Accuracy: 91.72%\n",
            "Epoch [137/250], Step [50/263], Loss: 0.1978, Accuracy: 91.41%\n",
            "Epoch [137/250], Step [60/263], Loss: 0.1599, Accuracy: 93.12%\n",
            "Epoch [137/250], Step [70/263], Loss: 0.1696, Accuracy: 93.28%\n",
            "Epoch [137/250], Step [80/263], Loss: 0.1557, Accuracy: 93.59%\n",
            "Epoch [137/250], Step [90/263], Loss: 0.1773, Accuracy: 92.66%\n",
            "Epoch [137/250], Step [100/263], Loss: 0.1497, Accuracy: 92.81%\n",
            "Epoch [137/250], Step [110/263], Loss: 0.1263, Accuracy: 95.47%\n",
            "Epoch [137/250], Step [120/263], Loss: 0.1016, Accuracy: 96.56%\n",
            "Epoch [137/250], Step [130/263], Loss: 0.1122, Accuracy: 96.41%\n",
            "Epoch [137/250], Step [140/263], Loss: 0.1252, Accuracy: 94.38%\n",
            "Epoch [137/250], Step [150/263], Loss: 0.0940, Accuracy: 96.41%\n",
            "Epoch [137/250], Step [160/263], Loss: 0.1146, Accuracy: 96.09%\n",
            "Epoch [137/250], Step [170/263], Loss: 0.1000, Accuracy: 96.56%\n",
            "Epoch [137/250], Step [180/263], Loss: 0.1271, Accuracy: 94.53%\n",
            "Epoch [137/250], Step [190/263], Loss: 0.1312, Accuracy: 95.00%\n",
            "Epoch [137/250], Step [200/263], Loss: 0.1287, Accuracy: 95.00%\n",
            "Epoch [137/250], Step [210/263], Loss: 0.0826, Accuracy: 97.19%\n",
            "Epoch [137/250], Step [220/263], Loss: 0.1607, Accuracy: 93.75%\n",
            "Epoch [137/250], Step [230/263], Loss: 0.1149, Accuracy: 95.00%\n",
            "Epoch [137/250], Step [240/263], Loss: 0.1528, Accuracy: 93.75%\n",
            "Epoch [137/250], Step [250/263], Loss: 0.1456, Accuracy: 94.69%\n",
            "Epoch [137/250], Step [260/263], Loss: 0.1648, Accuracy: 92.97%\n",
            "Epoch [138/250], Step [10/263], Loss: 0.1126, Accuracy: 95.47%\n",
            "Epoch [138/250], Step [20/263], Loss: 0.1178, Accuracy: 95.62%\n",
            "Epoch [138/250], Step [30/263], Loss: 0.1382, Accuracy: 95.00%\n",
            "Epoch [138/250], Step [40/263], Loss: 0.1325, Accuracy: 95.00%\n",
            "Epoch [138/250], Step [50/263], Loss: 0.0969, Accuracy: 96.88%\n",
            "Epoch [138/250], Step [60/263], Loss: 0.0976, Accuracy: 96.09%\n",
            "Epoch [138/250], Step [70/263], Loss: 0.1159, Accuracy: 95.31%\n",
            "Epoch [138/250], Step [80/263], Loss: 0.1080, Accuracy: 95.94%\n",
            "Epoch [138/250], Step [90/263], Loss: 0.0968, Accuracy: 97.03%\n",
            "Epoch [138/250], Step [100/263], Loss: 0.1092, Accuracy: 95.00%\n",
            "Epoch [138/250], Step [110/263], Loss: 0.1039, Accuracy: 95.94%\n",
            "Epoch [138/250], Step [120/263], Loss: 0.1055, Accuracy: 96.41%\n",
            "Epoch [138/250], Step [130/263], Loss: 0.0915, Accuracy: 97.03%\n",
            "Epoch [138/250], Step [140/263], Loss: 0.1009, Accuracy: 96.25%\n",
            "Epoch [138/250], Step [150/263], Loss: 0.0835, Accuracy: 96.88%\n",
            "Epoch [138/250], Step [160/263], Loss: 0.0992, Accuracy: 95.78%\n",
            "Epoch [138/250], Step [170/263], Loss: 0.0907, Accuracy: 96.56%\n",
            "Epoch [138/250], Step [180/263], Loss: 0.1116, Accuracy: 96.41%\n",
            "Epoch [138/250], Step [190/263], Loss: 0.0917, Accuracy: 97.03%\n",
            "Epoch [138/250], Step [200/263], Loss: 0.0782, Accuracy: 96.88%\n",
            "Epoch [138/250], Step [210/263], Loss: 0.0850, Accuracy: 96.88%\n",
            "Epoch [138/250], Step [220/263], Loss: 0.1195, Accuracy: 95.47%\n",
            "Epoch [138/250], Step [230/263], Loss: 0.1008, Accuracy: 95.78%\n",
            "Epoch [138/250], Step [240/263], Loss: 0.0939, Accuracy: 96.56%\n",
            "Epoch [138/250], Step [250/263], Loss: 0.0841, Accuracy: 97.34%\n",
            "Epoch [138/250], Step [260/263], Loss: 0.1113, Accuracy: 95.94%\n",
            "Epoch [139/250], Step [10/263], Loss: 0.0751, Accuracy: 97.50%\n",
            "Epoch [139/250], Step [20/263], Loss: 0.0796, Accuracy: 97.19%\n",
            "Epoch [139/250], Step [30/263], Loss: 0.0883, Accuracy: 96.41%\n",
            "Epoch [139/250], Step [40/263], Loss: 0.0772, Accuracy: 97.81%\n",
            "Epoch [139/250], Step [50/263], Loss: 0.0874, Accuracy: 96.88%\n",
            "Epoch [139/250], Step [60/263], Loss: 0.0925, Accuracy: 96.25%\n",
            "Epoch [139/250], Step [70/263], Loss: 0.0651, Accuracy: 97.97%\n",
            "Epoch [139/250], Step [80/263], Loss: 0.0753, Accuracy: 97.19%\n",
            "Epoch [139/250], Step [90/263], Loss: 0.0972, Accuracy: 96.09%\n",
            "Epoch [139/250], Step [100/263], Loss: 0.0971, Accuracy: 97.81%\n",
            "Epoch [139/250], Step [110/263], Loss: 0.0804, Accuracy: 97.81%\n",
            "Epoch [139/250], Step [120/263], Loss: 0.0735, Accuracy: 97.97%\n",
            "Epoch [139/250], Step [130/263], Loss: 0.0748, Accuracy: 97.19%\n",
            "Epoch [139/250], Step [140/263], Loss: 0.0660, Accuracy: 97.66%\n",
            "Epoch [139/250], Step [150/263], Loss: 0.1149, Accuracy: 95.94%\n",
            "Epoch [139/250], Step [160/263], Loss: 0.1092, Accuracy: 95.94%\n",
            "Epoch [139/250], Step [170/263], Loss: 0.0978, Accuracy: 96.56%\n",
            "Epoch [139/250], Step [180/263], Loss: 0.1117, Accuracy: 95.62%\n",
            "Epoch [139/250], Step [190/263], Loss: 0.0818, Accuracy: 97.50%\n",
            "Epoch [139/250], Step [200/263], Loss: 0.0908, Accuracy: 96.25%\n",
            "Epoch [139/250], Step [210/263], Loss: 0.0772, Accuracy: 97.34%\n",
            "Epoch [139/250], Step [220/263], Loss: 0.0748, Accuracy: 97.66%\n",
            "Epoch [139/250], Step [230/263], Loss: 0.0988, Accuracy: 95.47%\n",
            "Epoch [139/250], Step [240/263], Loss: 0.0948, Accuracy: 95.94%\n",
            "Epoch [139/250], Step [250/263], Loss: 0.1042, Accuracy: 96.09%\n",
            "Epoch [139/250], Step [260/263], Loss: 0.1139, Accuracy: 95.47%\n",
            "Epoch [140/250], Step [10/263], Loss: 0.0798, Accuracy: 96.88%\n",
            "Epoch [140/250], Step [20/263], Loss: 0.0862, Accuracy: 96.88%\n",
            "Epoch [140/250], Step [30/263], Loss: 0.0736, Accuracy: 97.66%\n",
            "Epoch [140/250], Step [40/263], Loss: 0.0923, Accuracy: 95.94%\n",
            "Epoch [140/250], Step [50/263], Loss: 0.0973, Accuracy: 96.25%\n",
            "Epoch [140/250], Step [60/263], Loss: 0.0826, Accuracy: 96.88%\n",
            "Epoch [140/250], Step [70/263], Loss: 0.0793, Accuracy: 97.19%\n",
            "Epoch [140/250], Step [80/263], Loss: 0.0677, Accuracy: 98.28%\n",
            "Epoch [140/250], Step [90/263], Loss: 0.0923, Accuracy: 96.25%\n",
            "Epoch [140/250], Step [100/263], Loss: 0.0731, Accuracy: 97.03%\n",
            "Epoch [140/250], Step [110/263], Loss: 0.0982, Accuracy: 95.78%\n",
            "Epoch [140/250], Step [120/263], Loss: 0.0707, Accuracy: 97.97%\n",
            "Epoch [140/250], Step [130/263], Loss: 0.0760, Accuracy: 97.66%\n",
            "Epoch [140/250], Step [140/263], Loss: 0.0871, Accuracy: 96.56%\n",
            "Epoch [140/250], Step [150/263], Loss: 0.0892, Accuracy: 96.72%\n",
            "Epoch [140/250], Step [160/263], Loss: 0.0822, Accuracy: 96.88%\n",
            "Epoch [140/250], Step [170/263], Loss: 0.1052, Accuracy: 96.56%\n",
            "Epoch [140/250], Step [180/263], Loss: 0.0820, Accuracy: 97.34%\n",
            "Epoch [140/250], Step [190/263], Loss: 0.1217, Accuracy: 95.47%\n",
            "Epoch [140/250], Step [200/263], Loss: 0.0888, Accuracy: 96.72%\n",
            "Epoch [140/250], Step [210/263], Loss: 0.0973, Accuracy: 96.41%\n",
            "Epoch [140/250], Step [220/263], Loss: 0.0907, Accuracy: 96.72%\n",
            "Epoch [140/250], Step [230/263], Loss: 0.0774, Accuracy: 96.88%\n",
            "Epoch [140/250], Step [240/263], Loss: 0.1010, Accuracy: 95.78%\n",
            "Epoch [140/250], Step [250/263], Loss: 0.1286, Accuracy: 95.78%\n",
            "Epoch [140/250], Step [260/263], Loss: 0.1071, Accuracy: 96.09%\n",
            "Epoch [141/250], Step [10/263], Loss: 0.0990, Accuracy: 96.09%\n",
            "Epoch [141/250], Step [20/263], Loss: 0.0493, Accuracy: 98.59%\n",
            "Epoch [141/250], Step [30/263], Loss: 0.0765, Accuracy: 96.72%\n",
            "Epoch [141/250], Step [40/263], Loss: 0.0580, Accuracy: 97.66%\n",
            "Epoch [141/250], Step [50/263], Loss: 0.0762, Accuracy: 97.34%\n",
            "Epoch [141/250], Step [60/263], Loss: 0.0769, Accuracy: 97.19%\n",
            "Epoch [141/250], Step [70/263], Loss: 0.0818, Accuracy: 97.34%\n",
            "Epoch [141/250], Step [80/263], Loss: 0.0752, Accuracy: 98.12%\n",
            "Epoch [141/250], Step [90/263], Loss: 0.0675, Accuracy: 97.66%\n",
            "Epoch [141/250], Step [100/263], Loss: 0.0836, Accuracy: 97.34%\n",
            "Epoch [141/250], Step [110/263], Loss: 0.0737, Accuracy: 97.81%\n",
            "Epoch [141/250], Step [120/263], Loss: 0.0822, Accuracy: 97.34%\n",
            "Epoch [141/250], Step [130/263], Loss: 0.0776, Accuracy: 96.88%\n",
            "Epoch [141/250], Step [140/263], Loss: 0.0796, Accuracy: 96.72%\n",
            "Epoch [141/250], Step [150/263], Loss: 0.0830, Accuracy: 97.03%\n",
            "Epoch [141/250], Step [160/263], Loss: 0.1203, Accuracy: 95.94%\n",
            "Epoch [141/250], Step [170/263], Loss: 0.0987, Accuracy: 96.56%\n",
            "Epoch [141/250], Step [180/263], Loss: 0.1046, Accuracy: 96.25%\n",
            "Epoch [141/250], Step [190/263], Loss: 0.1082, Accuracy: 96.41%\n",
            "Epoch [141/250], Step [200/263], Loss: 0.0761, Accuracy: 97.34%\n",
            "Epoch [141/250], Step [210/263], Loss: 0.0719, Accuracy: 97.19%\n",
            "Epoch [141/250], Step [220/263], Loss: 0.0899, Accuracy: 97.03%\n",
            "Epoch [141/250], Step [230/263], Loss: 0.0764, Accuracy: 97.50%\n",
            "Epoch [141/250], Step [240/263], Loss: 0.0867, Accuracy: 96.72%\n",
            "Epoch [141/250], Step [250/263], Loss: 0.1018, Accuracy: 95.47%\n",
            "Epoch [141/250], Step [260/263], Loss: 0.0898, Accuracy: 96.88%\n",
            "Epoch [142/250], Step [10/263], Loss: 0.0775, Accuracy: 97.66%\n",
            "Epoch [142/250], Step [20/263], Loss: 0.0614, Accuracy: 98.12%\n",
            "Epoch [142/250], Step [30/263], Loss: 0.0810, Accuracy: 97.50%\n",
            "Epoch [142/250], Step [40/263], Loss: 0.0784, Accuracy: 97.81%\n",
            "Epoch [142/250], Step [50/263], Loss: 0.0813, Accuracy: 96.88%\n",
            "Epoch [142/250], Step [60/263], Loss: 0.0821, Accuracy: 97.66%\n",
            "Epoch [142/250], Step [70/263], Loss: 0.0983, Accuracy: 95.94%\n",
            "Epoch [142/250], Step [80/263], Loss: 0.0986, Accuracy: 96.25%\n",
            "Epoch [142/250], Step [90/263], Loss: 0.0661, Accuracy: 98.44%\n",
            "Epoch [142/250], Step [100/263], Loss: 0.0876, Accuracy: 96.56%\n",
            "Epoch [142/250], Step [110/263], Loss: 0.0849, Accuracy: 96.72%\n",
            "Epoch [142/250], Step [120/263], Loss: 0.1048, Accuracy: 96.41%\n",
            "Epoch [142/250], Step [130/263], Loss: 0.1000, Accuracy: 96.56%\n",
            "Epoch [142/250], Step [140/263], Loss: 0.0934, Accuracy: 97.81%\n",
            "Epoch [142/250], Step [150/263], Loss: 0.0941, Accuracy: 97.03%\n",
            "Epoch [142/250], Step [160/263], Loss: 0.0922, Accuracy: 95.94%\n",
            "Epoch [142/250], Step [170/263], Loss: 0.0923, Accuracy: 96.41%\n",
            "Epoch [142/250], Step [180/263], Loss: 0.1244, Accuracy: 95.78%\n",
            "Epoch [142/250], Step [190/263], Loss: 0.1099, Accuracy: 95.78%\n",
            "Epoch [142/250], Step [200/263], Loss: 0.1031, Accuracy: 96.09%\n",
            "Epoch [142/250], Step [210/263], Loss: 0.1113, Accuracy: 96.25%\n",
            "Epoch [142/250], Step [220/263], Loss: 0.0919, Accuracy: 96.72%\n",
            "Epoch [142/250], Step [230/263], Loss: 0.1056, Accuracy: 95.78%\n",
            "Epoch [142/250], Step [240/263], Loss: 0.0886, Accuracy: 97.50%\n",
            "Epoch [142/250], Step [250/263], Loss: 0.0801, Accuracy: 96.88%\n",
            "Epoch [142/250], Step [260/263], Loss: 0.1250, Accuracy: 94.22%\n",
            "Epoch [143/250], Step [10/263], Loss: 0.0814, Accuracy: 97.19%\n",
            "Epoch [143/250], Step [20/263], Loss: 0.0558, Accuracy: 98.59%\n",
            "Epoch [143/250], Step [30/263], Loss: 0.0939, Accuracy: 95.62%\n",
            "Epoch [143/250], Step [40/263], Loss: 0.0735, Accuracy: 97.50%\n",
            "Epoch [143/250], Step [50/263], Loss: 0.0749, Accuracy: 97.81%\n",
            "Epoch [143/250], Step [60/263], Loss: 0.0739, Accuracy: 97.50%\n",
            "Epoch [143/250], Step [70/263], Loss: 0.1057, Accuracy: 95.78%\n",
            "Epoch [143/250], Step [80/263], Loss: 0.1109, Accuracy: 95.94%\n",
            "Epoch [143/250], Step [90/263], Loss: 0.0922, Accuracy: 97.50%\n",
            "Epoch [143/250], Step [100/263], Loss: 0.1023, Accuracy: 95.62%\n",
            "Epoch [143/250], Step [110/263], Loss: 0.1093, Accuracy: 95.47%\n",
            "Epoch [143/250], Step [120/263], Loss: 0.0872, Accuracy: 97.03%\n",
            "Epoch [143/250], Step [130/263], Loss: 0.1093, Accuracy: 96.09%\n",
            "Epoch [143/250], Step [140/263], Loss: 0.0868, Accuracy: 96.72%\n",
            "Epoch [143/250], Step [150/263], Loss: 0.0911, Accuracy: 97.66%\n",
            "Epoch [143/250], Step [160/263], Loss: 0.0698, Accuracy: 98.44%\n",
            "Epoch [143/250], Step [170/263], Loss: 0.0892, Accuracy: 96.56%\n",
            "Epoch [143/250], Step [180/263], Loss: 0.0739, Accuracy: 97.50%\n",
            "Epoch [143/250], Step [190/263], Loss: 0.0841, Accuracy: 97.03%\n",
            "Epoch [143/250], Step [200/263], Loss: 0.0776, Accuracy: 97.34%\n",
            "Epoch [143/250], Step [210/263], Loss: 0.0944, Accuracy: 96.72%\n",
            "Epoch [143/250], Step [220/263], Loss: 0.1054, Accuracy: 96.88%\n",
            "Epoch [143/250], Step [230/263], Loss: 0.0852, Accuracy: 96.88%\n",
            "Epoch [143/250], Step [240/263], Loss: 0.1092, Accuracy: 96.25%\n",
            "Epoch [143/250], Step [250/263], Loss: 0.0990, Accuracy: 96.72%\n",
            "Epoch [143/250], Step [260/263], Loss: 0.1112, Accuracy: 95.16%\n",
            "Epoch [144/250], Step [10/263], Loss: 0.1053, Accuracy: 96.41%\n",
            "Epoch [144/250], Step [20/263], Loss: 0.0714, Accuracy: 97.34%\n",
            "Epoch [144/250], Step [30/263], Loss: 0.1008, Accuracy: 96.72%\n",
            "Epoch [144/250], Step [40/263], Loss: 0.1156, Accuracy: 95.94%\n",
            "Epoch [144/250], Step [50/263], Loss: 0.1009, Accuracy: 95.62%\n",
            "Epoch [144/250], Step [60/263], Loss: 0.0933, Accuracy: 96.88%\n",
            "Epoch [144/250], Step [70/263], Loss: 0.1499, Accuracy: 94.69%\n",
            "Epoch [144/250], Step [80/263], Loss: 0.2231, Accuracy: 92.19%\n",
            "Epoch [144/250], Step [90/263], Loss: 0.3427, Accuracy: 88.75%\n",
            "Epoch [144/250], Step [100/263], Loss: 0.2788, Accuracy: 90.78%\n",
            "Epoch [144/250], Step [110/263], Loss: 0.2002, Accuracy: 93.75%\n",
            "Epoch [144/250], Step [120/263], Loss: 0.1835, Accuracy: 94.22%\n",
            "Epoch [144/250], Step [130/263], Loss: 0.1458, Accuracy: 94.53%\n",
            "Epoch [144/250], Step [140/263], Loss: 0.1605, Accuracy: 93.28%\n",
            "Epoch [144/250], Step [150/263], Loss: 0.1084, Accuracy: 96.72%\n",
            "Epoch [144/250], Step [160/263], Loss: 0.0904, Accuracy: 96.56%\n",
            "Epoch [144/250], Step [170/263], Loss: 0.1298, Accuracy: 96.09%\n",
            "Epoch [144/250], Step [180/263], Loss: 0.1197, Accuracy: 94.22%\n",
            "Epoch [144/250], Step [190/263], Loss: 0.0904, Accuracy: 96.56%\n",
            "Epoch [144/250], Step [200/263], Loss: 0.1057, Accuracy: 95.16%\n",
            "Epoch [144/250], Step [210/263], Loss: 0.0953, Accuracy: 96.25%\n",
            "Epoch [144/250], Step [220/263], Loss: 0.1022, Accuracy: 95.62%\n",
            "Epoch [144/250], Step [230/263], Loss: 0.0985, Accuracy: 96.41%\n",
            "Epoch [144/250], Step [240/263], Loss: 0.1258, Accuracy: 95.78%\n",
            "Epoch [144/250], Step [250/263], Loss: 0.1120, Accuracy: 96.56%\n",
            "Epoch [144/250], Step [260/263], Loss: 0.0944, Accuracy: 96.41%\n",
            "Epoch [145/250], Step [10/263], Loss: 0.0868, Accuracy: 97.34%\n",
            "Epoch [145/250], Step [20/263], Loss: 0.0762, Accuracy: 97.34%\n",
            "Epoch [145/250], Step [30/263], Loss: 0.0839, Accuracy: 96.72%\n",
            "Epoch [145/250], Step [40/263], Loss: 0.0918, Accuracy: 96.72%\n",
            "Epoch [145/250], Step [50/263], Loss: 0.0807, Accuracy: 96.88%\n",
            "Epoch [145/250], Step [60/263], Loss: 0.0808, Accuracy: 97.66%\n",
            "Epoch [145/250], Step [70/263], Loss: 0.0958, Accuracy: 96.72%\n",
            "Epoch [145/250], Step [80/263], Loss: 0.0595, Accuracy: 98.12%\n",
            "Epoch [145/250], Step [90/263], Loss: 0.0787, Accuracy: 97.34%\n",
            "Epoch [145/250], Step [100/263], Loss: 0.0788, Accuracy: 97.66%\n",
            "Epoch [145/250], Step [110/263], Loss: 0.0741, Accuracy: 97.50%\n",
            "Epoch [145/250], Step [120/263], Loss: 0.0738, Accuracy: 97.34%\n",
            "Epoch [145/250], Step [130/263], Loss: 0.0898, Accuracy: 96.09%\n",
            "Epoch [145/250], Step [140/263], Loss: 0.0692, Accuracy: 97.19%\n",
            "Epoch [145/250], Step [150/263], Loss: 0.0873, Accuracy: 96.56%\n",
            "Epoch [145/250], Step [160/263], Loss: 0.0735, Accuracy: 97.19%\n",
            "Epoch [145/250], Step [170/263], Loss: 0.0670, Accuracy: 98.12%\n",
            "Epoch [145/250], Step [180/263], Loss: 0.0783, Accuracy: 97.50%\n",
            "Epoch [145/250], Step [190/263], Loss: 0.0751, Accuracy: 97.81%\n",
            "Epoch [145/250], Step [200/263], Loss: 0.0700, Accuracy: 97.66%\n",
            "Epoch [145/250], Step [210/263], Loss: 0.0971, Accuracy: 96.72%\n",
            "Epoch [145/250], Step [220/263], Loss: 0.0495, Accuracy: 98.75%\n",
            "Epoch [145/250], Step [230/263], Loss: 0.0914, Accuracy: 96.56%\n",
            "Epoch [145/250], Step [240/263], Loss: 0.0798, Accuracy: 97.34%\n",
            "Epoch [145/250], Step [250/263], Loss: 0.0805, Accuracy: 96.56%\n",
            "Epoch [145/250], Step [260/263], Loss: 0.0796, Accuracy: 97.50%\n",
            "Epoch [146/250], Step [10/263], Loss: 0.1098, Accuracy: 96.88%\n",
            "Epoch [146/250], Step [20/263], Loss: 0.0821, Accuracy: 96.56%\n",
            "Epoch [146/250], Step [30/263], Loss: 0.0578, Accuracy: 97.97%\n",
            "Epoch [146/250], Step [40/263], Loss: 0.1029, Accuracy: 97.19%\n",
            "Epoch [146/250], Step [50/263], Loss: 0.0791, Accuracy: 97.34%\n",
            "Epoch [146/250], Step [60/263], Loss: 0.0824, Accuracy: 97.50%\n",
            "Epoch [146/250], Step [70/263], Loss: 0.0784, Accuracy: 97.03%\n",
            "Epoch [146/250], Step [80/263], Loss: 0.0814, Accuracy: 96.88%\n",
            "Epoch [146/250], Step [90/263], Loss: 0.0709, Accuracy: 97.50%\n",
            "Epoch [146/250], Step [100/263], Loss: 0.0649, Accuracy: 97.50%\n",
            "Epoch [146/250], Step [110/263], Loss: 0.0873, Accuracy: 96.41%\n",
            "Epoch [146/250], Step [120/263], Loss: 0.0782, Accuracy: 97.03%\n",
            "Epoch [146/250], Step [130/263], Loss: 0.0645, Accuracy: 97.50%\n",
            "Epoch [146/250], Step [140/263], Loss: 0.0813, Accuracy: 96.25%\n",
            "Epoch [146/250], Step [150/263], Loss: 0.0805, Accuracy: 97.34%\n",
            "Epoch [146/250], Step [160/263], Loss: 0.0679, Accuracy: 97.66%\n",
            "Epoch [146/250], Step [170/263], Loss: 0.0554, Accuracy: 97.97%\n",
            "Epoch [146/250], Step [180/263], Loss: 0.0869, Accuracy: 96.09%\n",
            "Epoch [146/250], Step [190/263], Loss: 0.0442, Accuracy: 99.06%\n",
            "Epoch [146/250], Step [200/263], Loss: 0.0627, Accuracy: 98.12%\n",
            "Epoch [146/250], Step [210/263], Loss: 0.0883, Accuracy: 96.41%\n",
            "Epoch [146/250], Step [220/263], Loss: 0.0851, Accuracy: 97.03%\n",
            "Epoch [146/250], Step [230/263], Loss: 0.0935, Accuracy: 96.88%\n",
            "Epoch [146/250], Step [240/263], Loss: 0.0803, Accuracy: 97.81%\n",
            "Epoch [146/250], Step [250/263], Loss: 0.0884, Accuracy: 96.88%\n",
            "Epoch [146/250], Step [260/263], Loss: 0.0852, Accuracy: 97.19%\n",
            "Epoch [147/250], Step [10/263], Loss: 0.0869, Accuracy: 97.19%\n",
            "Epoch [147/250], Step [20/263], Loss: 0.0703, Accuracy: 97.81%\n",
            "Epoch [147/250], Step [30/263], Loss: 0.0869, Accuracy: 96.72%\n",
            "Epoch [147/250], Step [40/263], Loss: 0.1095, Accuracy: 95.16%\n",
            "Epoch [147/250], Step [50/263], Loss: 0.0902, Accuracy: 97.19%\n",
            "Epoch [147/250], Step [60/263], Loss: 0.0871, Accuracy: 96.41%\n",
            "Epoch [147/250], Step [70/263], Loss: 0.1078, Accuracy: 95.00%\n",
            "Epoch [147/250], Step [80/263], Loss: 0.0719, Accuracy: 97.50%\n",
            "Epoch [147/250], Step [90/263], Loss: 0.0719, Accuracy: 97.66%\n",
            "Epoch [147/250], Step [100/263], Loss: 0.0736, Accuracy: 97.03%\n",
            "Epoch [147/250], Step [110/263], Loss: 0.0749, Accuracy: 97.50%\n",
            "Epoch [147/250], Step [120/263], Loss: 0.0663, Accuracy: 97.50%\n",
            "Epoch [147/250], Step [130/263], Loss: 0.1102, Accuracy: 95.16%\n",
            "Epoch [147/250], Step [140/263], Loss: 0.0755, Accuracy: 97.03%\n",
            "Epoch [147/250], Step [150/263], Loss: 0.0788, Accuracy: 96.25%\n",
            "Epoch [147/250], Step [160/263], Loss: 0.0820, Accuracy: 97.19%\n",
            "Epoch [147/250], Step [170/263], Loss: 0.0945, Accuracy: 97.03%\n",
            "Epoch [147/250], Step [180/263], Loss: 0.0818, Accuracy: 96.88%\n",
            "Epoch [147/250], Step [190/263], Loss: 0.0737, Accuracy: 97.50%\n",
            "Epoch [147/250], Step [200/263], Loss: 0.0974, Accuracy: 95.94%\n",
            "Epoch [147/250], Step [210/263], Loss: 0.0973, Accuracy: 97.03%\n",
            "Epoch [147/250], Step [220/263], Loss: 0.0663, Accuracy: 98.28%\n",
            "Epoch [147/250], Step [230/263], Loss: 0.0676, Accuracy: 97.97%\n",
            "Epoch [147/250], Step [240/263], Loss: 0.0843, Accuracy: 96.56%\n",
            "Epoch [147/250], Step [250/263], Loss: 0.0752, Accuracy: 97.66%\n",
            "Epoch [147/250], Step [260/263], Loss: 0.0774, Accuracy: 97.97%\n",
            "Epoch [148/250], Step [10/263], Loss: 0.0551, Accuracy: 98.59%\n",
            "Epoch [148/250], Step [20/263], Loss: 0.0584, Accuracy: 98.28%\n",
            "Epoch [148/250], Step [30/263], Loss: 0.0487, Accuracy: 99.06%\n",
            "Epoch [148/250], Step [40/263], Loss: 0.0584, Accuracy: 98.28%\n",
            "Epoch [148/250], Step [50/263], Loss: 0.0691, Accuracy: 97.50%\n",
            "Epoch [148/250], Step [60/263], Loss: 0.0862, Accuracy: 97.19%\n",
            "Epoch [148/250], Step [70/263], Loss: 0.0593, Accuracy: 98.12%\n",
            "Epoch [148/250], Step [80/263], Loss: 0.0654, Accuracy: 97.66%\n",
            "Epoch [148/250], Step [90/263], Loss: 0.0634, Accuracy: 97.81%\n",
            "Epoch [148/250], Step [100/263], Loss: 0.0567, Accuracy: 98.12%\n",
            "Epoch [148/250], Step [110/263], Loss: 0.0698, Accuracy: 97.50%\n",
            "Epoch [148/250], Step [120/263], Loss: 0.0701, Accuracy: 97.81%\n",
            "Epoch [148/250], Step [130/263], Loss: 0.0746, Accuracy: 97.19%\n",
            "Epoch [148/250], Step [140/263], Loss: 0.0567, Accuracy: 97.97%\n",
            "Epoch [148/250], Step [150/263], Loss: 0.0587, Accuracy: 98.28%\n",
            "Epoch [148/250], Step [160/263], Loss: 0.0953, Accuracy: 96.41%\n",
            "Epoch [148/250], Step [170/263], Loss: 0.0915, Accuracy: 96.72%\n",
            "Epoch [148/250], Step [180/263], Loss: 0.0682, Accuracy: 97.66%\n",
            "Epoch [148/250], Step [190/263], Loss: 0.0708, Accuracy: 97.66%\n",
            "Epoch [148/250], Step [200/263], Loss: 0.0825, Accuracy: 96.56%\n",
            "Epoch [148/250], Step [210/263], Loss: 0.0642, Accuracy: 97.81%\n",
            "Epoch [148/250], Step [220/263], Loss: 0.0614, Accuracy: 97.50%\n",
            "Epoch [148/250], Step [230/263], Loss: 0.0604, Accuracy: 97.66%\n",
            "Epoch [148/250], Step [240/263], Loss: 0.0695, Accuracy: 97.97%\n",
            "Epoch [148/250], Step [250/263], Loss: 0.0906, Accuracy: 96.72%\n",
            "Epoch [148/250], Step [260/263], Loss: 0.0737, Accuracy: 97.34%\n",
            "Epoch [149/250], Step [10/263], Loss: 0.0596, Accuracy: 98.44%\n",
            "Epoch [149/250], Step [20/263], Loss: 0.0475, Accuracy: 98.59%\n",
            "Epoch [149/250], Step [30/263], Loss: 0.0551, Accuracy: 98.12%\n",
            "Epoch [149/250], Step [40/263], Loss: 0.0467, Accuracy: 98.91%\n",
            "Epoch [149/250], Step [50/263], Loss: 0.0449, Accuracy: 99.06%\n",
            "Epoch [149/250], Step [60/263], Loss: 0.0690, Accuracy: 97.50%\n",
            "Epoch [149/250], Step [70/263], Loss: 0.0615, Accuracy: 98.12%\n",
            "Epoch [149/250], Step [80/263], Loss: 0.0639, Accuracy: 97.66%\n",
            "Epoch [149/250], Step [90/263], Loss: 0.0581, Accuracy: 98.44%\n",
            "Epoch [149/250], Step [100/263], Loss: 0.0726, Accuracy: 97.03%\n",
            "Epoch [149/250], Step [110/263], Loss: 0.0655, Accuracy: 97.66%\n",
            "Epoch [149/250], Step [120/263], Loss: 0.0703, Accuracy: 97.19%\n",
            "Epoch [149/250], Step [130/263], Loss: 0.0623, Accuracy: 97.66%\n",
            "Epoch [149/250], Step [140/263], Loss: 0.0650, Accuracy: 98.12%\n",
            "Epoch [149/250], Step [150/263], Loss: 0.0697, Accuracy: 97.97%\n",
            "Epoch [149/250], Step [160/263], Loss: 0.0544, Accuracy: 98.28%\n",
            "Epoch [149/250], Step [170/263], Loss: 0.0801, Accuracy: 96.72%\n",
            "Epoch [149/250], Step [180/263], Loss: 0.0450, Accuracy: 98.59%\n",
            "Epoch [149/250], Step [190/263], Loss: 0.0766, Accuracy: 97.66%\n",
            "Epoch [149/250], Step [200/263], Loss: 0.0511, Accuracy: 98.28%\n",
            "Epoch [149/250], Step [210/263], Loss: 0.0799, Accuracy: 96.88%\n",
            "Epoch [149/250], Step [220/263], Loss: 0.0638, Accuracy: 97.50%\n",
            "Epoch [149/250], Step [230/263], Loss: 0.0743, Accuracy: 96.72%\n",
            "Epoch [149/250], Step [240/263], Loss: 0.0755, Accuracy: 97.03%\n",
            "Epoch [149/250], Step [250/263], Loss: 0.0506, Accuracy: 99.06%\n",
            "Epoch [149/250], Step [260/263], Loss: 0.1025, Accuracy: 96.72%\n",
            "Epoch [150/250], Step [10/263], Loss: 0.0924, Accuracy: 96.41%\n",
            "Epoch [150/250], Step [20/263], Loss: 0.1561, Accuracy: 94.06%\n",
            "Epoch [150/250], Step [30/263], Loss: 0.1149, Accuracy: 95.16%\n",
            "Epoch [150/250], Step [40/263], Loss: 0.1307, Accuracy: 95.62%\n",
            "Epoch [150/250], Step [50/263], Loss: 0.0992, Accuracy: 95.47%\n",
            "Epoch [150/250], Step [60/263], Loss: 0.1021, Accuracy: 96.41%\n",
            "Epoch [150/250], Step [70/263], Loss: 0.1034, Accuracy: 96.09%\n",
            "Epoch [150/250], Step [80/263], Loss: 0.1209, Accuracy: 95.31%\n",
            "Epoch [150/250], Step [90/263], Loss: 0.1012, Accuracy: 96.25%\n",
            "Epoch [150/250], Step [100/263], Loss: 0.0659, Accuracy: 98.44%\n",
            "Epoch [150/250], Step [110/263], Loss: 0.0884, Accuracy: 96.88%\n",
            "Epoch [150/250], Step [120/263], Loss: 0.1003, Accuracy: 96.09%\n",
            "Epoch [150/250], Step [130/263], Loss: 0.1421, Accuracy: 93.75%\n",
            "Epoch [150/250], Step [140/263], Loss: 0.1004, Accuracy: 96.41%\n",
            "Epoch [150/250], Step [150/263], Loss: 0.1070, Accuracy: 96.56%\n",
            "Epoch [150/250], Step [160/263], Loss: 0.1013, Accuracy: 96.41%\n",
            "Epoch [150/250], Step [170/263], Loss: 0.1330, Accuracy: 94.06%\n",
            "Epoch [150/250], Step [180/263], Loss: 0.0825, Accuracy: 97.03%\n",
            "Epoch [150/250], Step [190/263], Loss: 0.1196, Accuracy: 96.25%\n",
            "Epoch [150/250], Step [200/263], Loss: 0.1197, Accuracy: 95.31%\n",
            "Epoch [150/250], Step [210/263], Loss: 0.0987, Accuracy: 95.78%\n",
            "Epoch [150/250], Step [220/263], Loss: 0.1019, Accuracy: 95.78%\n",
            "Epoch [150/250], Step [230/263], Loss: 0.0928, Accuracy: 96.25%\n",
            "Epoch [150/250], Step [240/263], Loss: 0.0970, Accuracy: 96.09%\n",
            "Epoch [150/250], Step [250/263], Loss: 0.0840, Accuracy: 97.03%\n",
            "Epoch [150/250], Step [260/263], Loss: 0.0671, Accuracy: 97.50%\n",
            "Epoch [151/250], Step [10/263], Loss: 0.0622, Accuracy: 98.12%\n",
            "Epoch [151/250], Step [20/263], Loss: 0.0853, Accuracy: 96.72%\n",
            "Epoch [151/250], Step [30/263], Loss: 0.0791, Accuracy: 97.66%\n",
            "Epoch [151/250], Step [40/263], Loss: 0.0785, Accuracy: 97.19%\n",
            "Epoch [151/250], Step [50/263], Loss: 0.0793, Accuracy: 96.88%\n",
            "Epoch [151/250], Step [60/263], Loss: 0.0613, Accuracy: 97.19%\n",
            "Epoch [151/250], Step [70/263], Loss: 0.0928, Accuracy: 96.41%\n",
            "Epoch [151/250], Step [80/263], Loss: 0.0684, Accuracy: 98.28%\n",
            "Epoch [151/250], Step [90/263], Loss: 0.0621, Accuracy: 98.12%\n",
            "Epoch [151/250], Step [100/263], Loss: 0.0726, Accuracy: 97.34%\n",
            "Epoch [151/250], Step [110/263], Loss: 0.0880, Accuracy: 96.88%\n",
            "Epoch [151/250], Step [120/263], Loss: 0.0761, Accuracy: 97.66%\n",
            "Epoch [151/250], Step [130/263], Loss: 0.0962, Accuracy: 96.41%\n",
            "Epoch [151/250], Step [140/263], Loss: 0.0705, Accuracy: 97.34%\n",
            "Epoch [151/250], Step [150/263], Loss: 0.0881, Accuracy: 96.25%\n",
            "Epoch [151/250], Step [160/263], Loss: 0.0983, Accuracy: 96.72%\n",
            "Epoch [151/250], Step [170/263], Loss: 0.1142, Accuracy: 95.47%\n",
            "Epoch [151/250], Step [180/263], Loss: 0.1014, Accuracy: 95.62%\n",
            "Epoch [151/250], Step [190/263], Loss: 0.1523, Accuracy: 94.53%\n",
            "Epoch [151/250], Step [200/263], Loss: 0.3063, Accuracy: 89.22%\n",
            "Epoch [151/250], Step [210/263], Loss: 0.4859, Accuracy: 84.38%\n",
            "Epoch [151/250], Step [220/263], Loss: 0.3085, Accuracy: 89.84%\n",
            "Epoch [151/250], Step [230/263], Loss: 0.3972, Accuracy: 87.97%\n",
            "Epoch [151/250], Step [240/263], Loss: 0.2771, Accuracy: 90.16%\n",
            "Epoch [151/250], Step [250/263], Loss: 0.2731, Accuracy: 90.78%\n",
            "Epoch [151/250], Step [260/263], Loss: 0.2406, Accuracy: 91.88%\n",
            "Epoch [152/250], Step [10/263], Loss: 0.2282, Accuracy: 92.19%\n",
            "Epoch [152/250], Step [20/263], Loss: 0.1913, Accuracy: 93.28%\n",
            "Epoch [152/250], Step [30/263], Loss: 0.1798, Accuracy: 93.28%\n",
            "Epoch [152/250], Step [40/263], Loss: 0.2011, Accuracy: 93.91%\n",
            "Epoch [152/250], Step [50/263], Loss: 0.1843, Accuracy: 92.66%\n",
            "Epoch [152/250], Step [60/263], Loss: 0.1480, Accuracy: 93.91%\n",
            "Epoch [152/250], Step [70/263], Loss: 0.1518, Accuracy: 93.75%\n",
            "Epoch [152/250], Step [80/263], Loss: 0.1336, Accuracy: 94.22%\n",
            "Epoch [152/250], Step [90/263], Loss: 0.1260, Accuracy: 94.53%\n",
            "Epoch [152/250], Step [100/263], Loss: 0.1201, Accuracy: 95.31%\n",
            "Epoch [152/250], Step [110/263], Loss: 0.1501, Accuracy: 94.22%\n",
            "Epoch [152/250], Step [120/263], Loss: 0.1132, Accuracy: 95.78%\n",
            "Epoch [152/250], Step [130/263], Loss: 0.1393, Accuracy: 95.78%\n",
            "Epoch [152/250], Step [140/263], Loss: 0.1248, Accuracy: 96.25%\n",
            "Epoch [152/250], Step [150/263], Loss: 0.1343, Accuracy: 94.53%\n",
            "Epoch [152/250], Step [160/263], Loss: 0.0923, Accuracy: 96.09%\n",
            "Epoch [152/250], Step [170/263], Loss: 0.1308, Accuracy: 95.31%\n",
            "Epoch [152/250], Step [180/263], Loss: 0.1195, Accuracy: 95.62%\n",
            "Epoch [152/250], Step [190/263], Loss: 0.1099, Accuracy: 95.94%\n",
            "Epoch [152/250], Step [200/263], Loss: 0.1154, Accuracy: 95.62%\n",
            "Epoch [152/250], Step [210/263], Loss: 0.1589, Accuracy: 94.22%\n",
            "Epoch [152/250], Step [220/263], Loss: 0.1795, Accuracy: 92.81%\n",
            "Epoch [152/250], Step [230/263], Loss: 0.2178, Accuracy: 91.88%\n",
            "Epoch [152/250], Step [240/263], Loss: 0.1792, Accuracy: 94.84%\n",
            "Epoch [152/250], Step [250/263], Loss: 0.1772, Accuracy: 94.53%\n",
            "Epoch [152/250], Step [260/263], Loss: 0.1241, Accuracy: 95.16%\n",
            "Epoch [153/250], Step [10/263], Loss: 0.1379, Accuracy: 95.47%\n",
            "Epoch [153/250], Step [20/263], Loss: 0.1162, Accuracy: 96.41%\n",
            "Epoch [153/250], Step [30/263], Loss: 0.1042, Accuracy: 95.62%\n",
            "Epoch [153/250], Step [40/263], Loss: 0.1171, Accuracy: 95.16%\n",
            "Epoch [153/250], Step [50/263], Loss: 0.0801, Accuracy: 96.41%\n",
            "Epoch [153/250], Step [60/263], Loss: 0.0820, Accuracy: 97.03%\n",
            "Epoch [153/250], Step [70/263], Loss: 0.1038, Accuracy: 94.69%\n",
            "Epoch [153/250], Step [80/263], Loss: 0.1170, Accuracy: 95.94%\n",
            "Epoch [153/250], Step [90/263], Loss: 0.0957, Accuracy: 96.56%\n",
            "Epoch [153/250], Step [100/263], Loss: 0.0716, Accuracy: 97.97%\n",
            "Epoch [153/250], Step [110/263], Loss: 0.0817, Accuracy: 97.34%\n",
            "Epoch [153/250], Step [120/263], Loss: 0.0764, Accuracy: 97.03%\n",
            "Epoch [153/250], Step [130/263], Loss: 0.0946, Accuracy: 97.19%\n",
            "Epoch [153/250], Step [140/263], Loss: 0.1028, Accuracy: 96.41%\n",
            "Epoch [153/250], Step [150/263], Loss: 0.0669, Accuracy: 97.81%\n",
            "Epoch [153/250], Step [160/263], Loss: 0.0800, Accuracy: 97.19%\n",
            "Epoch [153/250], Step [170/263], Loss: 0.1019, Accuracy: 95.94%\n",
            "Epoch [153/250], Step [180/263], Loss: 0.1063, Accuracy: 96.56%\n",
            "Epoch [153/250], Step [190/263], Loss: 0.0749, Accuracy: 98.12%\n",
            "Epoch [153/250], Step [200/263], Loss: 0.0788, Accuracy: 96.88%\n",
            "Epoch [153/250], Step [210/263], Loss: 0.0798, Accuracy: 96.56%\n",
            "Epoch [153/250], Step [220/263], Loss: 0.0811, Accuracy: 97.50%\n",
            "Epoch [153/250], Step [230/263], Loss: 0.0777, Accuracy: 97.81%\n",
            "Epoch [153/250], Step [240/263], Loss: 0.0894, Accuracy: 97.03%\n",
            "Epoch [153/250], Step [250/263], Loss: 0.0641, Accuracy: 98.75%\n",
            "Epoch [153/250], Step [260/263], Loss: 0.0786, Accuracy: 97.66%\n",
            "Epoch [154/250], Step [10/263], Loss: 0.0616, Accuracy: 97.81%\n",
            "Epoch [154/250], Step [20/263], Loss: 0.0683, Accuracy: 97.66%\n",
            "Epoch [154/250], Step [30/263], Loss: 0.0561, Accuracy: 97.97%\n",
            "Epoch [154/250], Step [40/263], Loss: 0.0624, Accuracy: 97.97%\n",
            "Epoch [154/250], Step [50/263], Loss: 0.0697, Accuracy: 97.19%\n",
            "Epoch [154/250], Step [60/263], Loss: 0.0538, Accuracy: 98.28%\n",
            "Epoch [154/250], Step [70/263], Loss: 0.0568, Accuracy: 98.28%\n",
            "Epoch [154/250], Step [80/263], Loss: 0.0520, Accuracy: 98.59%\n",
            "Epoch [154/250], Step [90/263], Loss: 0.0434, Accuracy: 99.38%\n",
            "Epoch [154/250], Step [100/263], Loss: 0.0719, Accuracy: 97.66%\n",
            "Epoch [154/250], Step [110/263], Loss: 0.0699, Accuracy: 97.34%\n",
            "Epoch [154/250], Step [120/263], Loss: 0.0533, Accuracy: 98.28%\n",
            "Epoch [154/250], Step [130/263], Loss: 0.0814, Accuracy: 97.19%\n",
            "Epoch [154/250], Step [140/263], Loss: 0.0694, Accuracy: 97.97%\n",
            "Epoch [154/250], Step [150/263], Loss: 0.0489, Accuracy: 98.12%\n",
            "Epoch [154/250], Step [160/263], Loss: 0.0779, Accuracy: 97.03%\n",
            "Epoch [154/250], Step [170/263], Loss: 0.0680, Accuracy: 97.81%\n",
            "Epoch [154/250], Step [180/263], Loss: 0.0577, Accuracy: 98.12%\n",
            "Epoch [154/250], Step [190/263], Loss: 0.0648, Accuracy: 97.34%\n",
            "Epoch [154/250], Step [200/263], Loss: 0.0632, Accuracy: 98.28%\n",
            "Epoch [154/250], Step [210/263], Loss: 0.0533, Accuracy: 98.28%\n",
            "Epoch [154/250], Step [220/263], Loss: 0.0959, Accuracy: 96.56%\n",
            "Epoch [154/250], Step [230/263], Loss: 0.0804, Accuracy: 97.03%\n",
            "Epoch [154/250], Step [240/263], Loss: 0.0812, Accuracy: 97.19%\n",
            "Epoch [154/250], Step [250/263], Loss: 0.0686, Accuracy: 97.19%\n",
            "Epoch [154/250], Step [260/263], Loss: 0.0653, Accuracy: 97.66%\n",
            "Epoch [155/250], Step [10/263], Loss: 0.0546, Accuracy: 98.75%\n",
            "Epoch [155/250], Step [20/263], Loss: 0.0762, Accuracy: 97.34%\n",
            "Epoch [155/250], Step [30/263], Loss: 0.0605, Accuracy: 98.12%\n",
            "Epoch [155/250], Step [40/263], Loss: 0.0629, Accuracy: 98.28%\n",
            "Epoch [155/250], Step [50/263], Loss: 0.0596, Accuracy: 97.50%\n",
            "Epoch [155/250], Step [60/263], Loss: 0.0544, Accuracy: 97.97%\n",
            "Epoch [155/250], Step [70/263], Loss: 0.0551, Accuracy: 97.97%\n",
            "Epoch [155/250], Step [80/263], Loss: 0.0489, Accuracy: 98.91%\n",
            "Epoch [155/250], Step [90/263], Loss: 0.0475, Accuracy: 98.91%\n",
            "Epoch [155/250], Step [100/263], Loss: 0.0574, Accuracy: 98.12%\n",
            "Epoch [155/250], Step [110/263], Loss: 0.0502, Accuracy: 98.59%\n",
            "Epoch [155/250], Step [120/263], Loss: 0.0503, Accuracy: 98.28%\n",
            "Epoch [155/250], Step [130/263], Loss: 0.0346, Accuracy: 99.38%\n",
            "Epoch [155/250], Step [140/263], Loss: 0.0656, Accuracy: 98.12%\n",
            "Epoch [155/250], Step [150/263], Loss: 0.0542, Accuracy: 97.81%\n",
            "Epoch [155/250], Step [160/263], Loss: 0.0439, Accuracy: 98.59%\n",
            "Epoch [155/250], Step [170/263], Loss: 0.0478, Accuracy: 98.59%\n",
            "Epoch [155/250], Step [180/263], Loss: 0.0468, Accuracy: 98.59%\n",
            "Epoch [155/250], Step [190/263], Loss: 0.0459, Accuracy: 98.75%\n",
            "Epoch [155/250], Step [200/263], Loss: 0.0520, Accuracy: 98.44%\n",
            "Epoch [155/250], Step [210/263], Loss: 0.0650, Accuracy: 97.81%\n",
            "Epoch [155/250], Step [220/263], Loss: 0.0438, Accuracy: 98.75%\n",
            "Epoch [155/250], Step [230/263], Loss: 0.0512, Accuracy: 98.12%\n",
            "Epoch [155/250], Step [240/263], Loss: 0.0405, Accuracy: 98.75%\n",
            "Epoch [155/250], Step [250/263], Loss: 0.0528, Accuracy: 98.28%\n",
            "Epoch [155/250], Step [260/263], Loss: 0.0574, Accuracy: 97.81%\n",
            "Epoch [156/250], Step [10/263], Loss: 0.0523, Accuracy: 98.75%\n",
            "Epoch [156/250], Step [20/263], Loss: 0.0342, Accuracy: 99.22%\n",
            "Epoch [156/250], Step [30/263], Loss: 0.0305, Accuracy: 99.22%\n",
            "Epoch [156/250], Step [40/263], Loss: 0.0394, Accuracy: 98.91%\n",
            "Epoch [156/250], Step [50/263], Loss: 0.0459, Accuracy: 98.75%\n",
            "Epoch [156/250], Step [60/263], Loss: 0.0340, Accuracy: 99.22%\n",
            "Epoch [156/250], Step [70/263], Loss: 0.0267, Accuracy: 99.53%\n",
            "Epoch [156/250], Step [80/263], Loss: 0.0521, Accuracy: 98.28%\n",
            "Epoch [156/250], Step [90/263], Loss: 0.0389, Accuracy: 98.91%\n",
            "Epoch [156/250], Step [100/263], Loss: 0.0301, Accuracy: 99.53%\n",
            "Epoch [156/250], Step [110/263], Loss: 0.0409, Accuracy: 98.44%\n",
            "Epoch [156/250], Step [120/263], Loss: 0.0265, Accuracy: 99.69%\n",
            "Epoch [156/250], Step [130/263], Loss: 0.0325, Accuracy: 99.38%\n",
            "Epoch [156/250], Step [140/263], Loss: 0.0573, Accuracy: 98.28%\n",
            "Epoch [156/250], Step [150/263], Loss: 0.0400, Accuracy: 97.97%\n",
            "Epoch [156/250], Step [160/263], Loss: 0.0416, Accuracy: 98.91%\n",
            "Epoch [156/250], Step [170/263], Loss: 0.0338, Accuracy: 99.53%\n",
            "Epoch [156/250], Step [180/263], Loss: 0.0501, Accuracy: 98.44%\n",
            "Epoch [156/250], Step [190/263], Loss: 0.0319, Accuracy: 99.38%\n",
            "Epoch [156/250], Step [200/263], Loss: 0.0452, Accuracy: 98.91%\n",
            "Epoch [156/250], Step [210/263], Loss: 0.0358, Accuracy: 99.06%\n",
            "Epoch [156/250], Step [220/263], Loss: 0.0499, Accuracy: 98.75%\n",
            "Epoch [156/250], Step [230/263], Loss: 0.0342, Accuracy: 99.38%\n",
            "Epoch [156/250], Step [240/263], Loss: 0.0411, Accuracy: 98.75%\n",
            "Epoch [156/250], Step [250/263], Loss: 0.0489, Accuracy: 97.81%\n",
            "Epoch [156/250], Step [260/263], Loss: 0.0361, Accuracy: 99.06%\n",
            "Epoch [157/250], Step [10/263], Loss: 0.0292, Accuracy: 99.38%\n",
            "Epoch [157/250], Step [20/263], Loss: 0.0267, Accuracy: 99.06%\n",
            "Epoch [157/250], Step [30/263], Loss: 0.0303, Accuracy: 99.38%\n",
            "Epoch [157/250], Step [40/263], Loss: 0.0335, Accuracy: 98.91%\n",
            "Epoch [157/250], Step [50/263], Loss: 0.0246, Accuracy: 99.53%\n",
            "Epoch [157/250], Step [60/263], Loss: 0.0271, Accuracy: 99.53%\n",
            "Epoch [157/250], Step [70/263], Loss: 0.0300, Accuracy: 99.53%\n",
            "Epoch [157/250], Step [80/263], Loss: 0.0326, Accuracy: 98.91%\n",
            "Epoch [157/250], Step [90/263], Loss: 0.0336, Accuracy: 99.06%\n",
            "Epoch [157/250], Step [100/263], Loss: 0.0341, Accuracy: 99.22%\n",
            "Epoch [157/250], Step [110/263], Loss: 0.0333, Accuracy: 99.38%\n",
            "Epoch [157/250], Step [120/263], Loss: 0.0289, Accuracy: 99.38%\n",
            "Epoch [157/250], Step [130/263], Loss: 0.0352, Accuracy: 99.06%\n",
            "Epoch [157/250], Step [140/263], Loss: 0.0341, Accuracy: 99.53%\n",
            "Epoch [157/250], Step [150/263], Loss: 0.0290, Accuracy: 99.69%\n",
            "Epoch [157/250], Step [160/263], Loss: 0.0341, Accuracy: 99.38%\n",
            "Epoch [157/250], Step [170/263], Loss: 0.0330, Accuracy: 99.06%\n",
            "Epoch [157/250], Step [180/263], Loss: 0.0414, Accuracy: 98.44%\n",
            "Epoch [157/250], Step [190/263], Loss: 0.0280, Accuracy: 99.53%\n",
            "Epoch [157/250], Step [200/263], Loss: 0.0380, Accuracy: 98.59%\n",
            "Epoch [157/250], Step [210/263], Loss: 0.0516, Accuracy: 98.44%\n",
            "Epoch [157/250], Step [220/263], Loss: 0.0473, Accuracy: 98.75%\n",
            "Epoch [157/250], Step [230/263], Loss: 0.0360, Accuracy: 98.75%\n",
            "Epoch [157/250], Step [240/263], Loss: 0.0477, Accuracy: 98.75%\n",
            "Epoch [157/250], Step [250/263], Loss: 0.0299, Accuracy: 99.38%\n",
            "Epoch [157/250], Step [260/263], Loss: 0.0564, Accuracy: 98.12%\n",
            "Epoch [158/250], Step [10/263], Loss: 0.0194, Accuracy: 99.84%\n",
            "Epoch [158/250], Step [20/263], Loss: 0.0263, Accuracy: 99.22%\n",
            "Epoch [158/250], Step [30/263], Loss: 0.0263, Accuracy: 99.38%\n",
            "Epoch [158/250], Step [40/263], Loss: 0.0249, Accuracy: 99.38%\n",
            "Epoch [158/250], Step [50/263], Loss: 0.0252, Accuracy: 99.69%\n",
            "Epoch [158/250], Step [60/263], Loss: 0.0307, Accuracy: 99.22%\n",
            "Epoch [158/250], Step [70/263], Loss: 0.0275, Accuracy: 99.22%\n",
            "Epoch [158/250], Step [80/263], Loss: 0.0365, Accuracy: 99.06%\n",
            "Epoch [158/250], Step [90/263], Loss: 0.0309, Accuracy: 99.06%\n",
            "Epoch [158/250], Step [100/263], Loss: 0.0340, Accuracy: 99.06%\n",
            "Epoch [158/250], Step [110/263], Loss: 0.0405, Accuracy: 99.22%\n",
            "Epoch [158/250], Step [120/263], Loss: 0.0348, Accuracy: 99.38%\n",
            "Epoch [158/250], Step [130/263], Loss: 0.0240, Accuracy: 99.69%\n",
            "Epoch [158/250], Step [140/263], Loss: 0.0316, Accuracy: 99.53%\n",
            "Epoch [158/250], Step [150/263], Loss: 0.0323, Accuracy: 99.22%\n",
            "Epoch [158/250], Step [160/263], Loss: 0.0250, Accuracy: 99.69%\n",
            "Epoch [158/250], Step [170/263], Loss: 0.0261, Accuracy: 99.22%\n",
            "Epoch [158/250], Step [180/263], Loss: 0.0324, Accuracy: 98.59%\n",
            "Epoch [158/250], Step [190/263], Loss: 0.0304, Accuracy: 99.06%\n",
            "Epoch [158/250], Step [200/263], Loss: 0.0333, Accuracy: 99.06%\n",
            "Epoch [158/250], Step [210/263], Loss: 0.0499, Accuracy: 98.28%\n",
            "Epoch [158/250], Step [220/263], Loss: 0.0353, Accuracy: 98.91%\n",
            "Epoch [158/250], Step [230/263], Loss: 0.0483, Accuracy: 97.81%\n",
            "Epoch [158/250], Step [240/263], Loss: 0.0378, Accuracy: 99.06%\n",
            "Epoch [158/250], Step [250/263], Loss: 0.0419, Accuracy: 99.06%\n",
            "Epoch [158/250], Step [260/263], Loss: 0.0381, Accuracy: 98.91%\n",
            "Epoch [159/250], Step [10/263], Loss: 0.0323, Accuracy: 99.38%\n",
            "Epoch [159/250], Step [20/263], Loss: 0.0304, Accuracy: 99.53%\n",
            "Epoch [159/250], Step [30/263], Loss: 0.0363, Accuracy: 99.06%\n",
            "Epoch [159/250], Step [40/263], Loss: 0.0443, Accuracy: 98.91%\n",
            "Epoch [159/250], Step [50/263], Loss: 0.0347, Accuracy: 98.91%\n",
            "Epoch [159/250], Step [60/263], Loss: 0.0264, Accuracy: 99.84%\n",
            "Epoch [159/250], Step [70/263], Loss: 0.0404, Accuracy: 98.59%\n",
            "Epoch [159/250], Step [80/263], Loss: 0.0400, Accuracy: 98.59%\n",
            "Epoch [159/250], Step [90/263], Loss: 0.0367, Accuracy: 99.06%\n",
            "Epoch [159/250], Step [100/263], Loss: 0.0398, Accuracy: 98.91%\n",
            "Epoch [159/250], Step [110/263], Loss: 0.0692, Accuracy: 97.97%\n",
            "Epoch [159/250], Step [120/263], Loss: 0.0613, Accuracy: 97.66%\n",
            "Epoch [159/250], Step [130/263], Loss: 0.0655, Accuracy: 97.66%\n",
            "Epoch [159/250], Step [140/263], Loss: 0.0543, Accuracy: 98.59%\n",
            "Epoch [159/250], Step [150/263], Loss: 0.0657, Accuracy: 97.03%\n",
            "Epoch [159/250], Step [160/263], Loss: 0.0815, Accuracy: 97.03%\n",
            "Epoch [159/250], Step [170/263], Loss: 0.0576, Accuracy: 97.81%\n",
            "Epoch [159/250], Step [180/263], Loss: 0.0624, Accuracy: 97.19%\n",
            "Epoch [159/250], Step [190/263], Loss: 0.0584, Accuracy: 97.97%\n",
            "Epoch [159/250], Step [200/263], Loss: 0.0958, Accuracy: 97.19%\n",
            "Epoch [159/250], Step [210/263], Loss: 0.0756, Accuracy: 97.66%\n",
            "Epoch [159/250], Step [220/263], Loss: 0.0865, Accuracy: 96.56%\n",
            "Epoch [159/250], Step [230/263], Loss: 0.0592, Accuracy: 98.12%\n",
            "Epoch [159/250], Step [240/263], Loss: 0.0711, Accuracy: 97.19%\n",
            "Epoch [159/250], Step [250/263], Loss: 0.0723, Accuracy: 97.03%\n",
            "Epoch [159/250], Step [260/263], Loss: 0.0823, Accuracy: 97.19%\n",
            "Epoch [160/250], Step [10/263], Loss: 0.0822, Accuracy: 97.19%\n",
            "Epoch [160/250], Step [20/263], Loss: 0.0736, Accuracy: 97.66%\n",
            "Epoch [160/250], Step [30/263], Loss: 0.0761, Accuracy: 97.34%\n",
            "Epoch [160/250], Step [40/263], Loss: 0.0877, Accuracy: 97.03%\n",
            "Epoch [160/250], Step [50/263], Loss: 0.0755, Accuracy: 96.88%\n",
            "Epoch [160/250], Step [60/263], Loss: 0.0945, Accuracy: 96.88%\n",
            "Epoch [160/250], Step [70/263], Loss: 0.0771, Accuracy: 98.12%\n",
            "Epoch [160/250], Step [80/263], Loss: 0.0582, Accuracy: 97.97%\n",
            "Epoch [160/250], Step [90/263], Loss: 0.0907, Accuracy: 96.41%\n",
            "Epoch [160/250], Step [100/263], Loss: 0.0901, Accuracy: 96.09%\n",
            "Epoch [160/250], Step [110/263], Loss: 0.0686, Accuracy: 97.66%\n",
            "Epoch [160/250], Step [120/263], Loss: 0.0999, Accuracy: 96.09%\n",
            "Epoch [160/250], Step [130/263], Loss: 0.1074, Accuracy: 95.47%\n",
            "Epoch [160/250], Step [140/263], Loss: 0.0798, Accuracy: 97.34%\n",
            "Epoch [160/250], Step [150/263], Loss: 0.0922, Accuracy: 96.72%\n",
            "Epoch [160/250], Step [160/263], Loss: 0.1028, Accuracy: 95.94%\n",
            "Epoch [160/250], Step [170/263], Loss: 0.1163, Accuracy: 95.47%\n",
            "Epoch [160/250], Step [180/263], Loss: 0.1828, Accuracy: 94.06%\n",
            "Epoch [160/250], Step [190/263], Loss: 0.2192, Accuracy: 91.72%\n",
            "Epoch [160/250], Step [200/263], Loss: 0.1282, Accuracy: 94.84%\n",
            "Epoch [160/250], Step [210/263], Loss: 0.1460, Accuracy: 95.62%\n",
            "Epoch [160/250], Step [220/263], Loss: 0.1464, Accuracy: 94.06%\n",
            "Epoch [160/250], Step [230/263], Loss: 0.1678, Accuracy: 93.28%\n",
            "Epoch [160/250], Step [240/263], Loss: 0.1431, Accuracy: 93.59%\n",
            "Epoch [160/250], Step [250/263], Loss: 0.1286, Accuracy: 94.84%\n",
            "Epoch [160/250], Step [260/263], Loss: 0.1281, Accuracy: 95.62%\n",
            "Epoch [161/250], Step [10/263], Loss: 0.0877, Accuracy: 96.72%\n",
            "Epoch [161/250], Step [20/263], Loss: 0.0806, Accuracy: 97.19%\n",
            "Epoch [161/250], Step [30/263], Loss: 0.1000, Accuracy: 96.25%\n",
            "Epoch [161/250], Step [40/263], Loss: 0.0506, Accuracy: 98.12%\n",
            "Epoch [161/250], Step [50/263], Loss: 0.0724, Accuracy: 97.66%\n",
            "Epoch [161/250], Step [60/263], Loss: 0.0940, Accuracy: 95.62%\n",
            "Epoch [161/250], Step [70/263], Loss: 0.0934, Accuracy: 96.41%\n",
            "Epoch [161/250], Step [80/263], Loss: 0.0909, Accuracy: 96.56%\n",
            "Epoch [161/250], Step [90/263], Loss: 0.0657, Accuracy: 97.34%\n",
            "Epoch [161/250], Step [100/263], Loss: 0.0616, Accuracy: 97.50%\n",
            "Epoch [161/250], Step [110/263], Loss: 0.0745, Accuracy: 96.88%\n",
            "Epoch [161/250], Step [120/263], Loss: 0.0621, Accuracy: 97.97%\n",
            "Epoch [161/250], Step [130/263], Loss: 0.0643, Accuracy: 97.66%\n",
            "Epoch [161/250], Step [140/263], Loss: 0.0662, Accuracy: 97.97%\n",
            "Epoch [161/250], Step [150/263], Loss: 0.0610, Accuracy: 97.50%\n",
            "Epoch [161/250], Step [160/263], Loss: 0.0637, Accuracy: 97.50%\n",
            "Epoch [161/250], Step [170/263], Loss: 0.0562, Accuracy: 97.81%\n",
            "Epoch [161/250], Step [180/263], Loss: 0.0771, Accuracy: 96.88%\n",
            "Epoch [161/250], Step [190/263], Loss: 0.0733, Accuracy: 97.34%\n",
            "Epoch [161/250], Step [200/263], Loss: 0.0634, Accuracy: 97.66%\n",
            "Epoch [161/250], Step [210/263], Loss: 0.0572, Accuracy: 97.50%\n",
            "Epoch [161/250], Step [220/263], Loss: 0.0570, Accuracy: 98.44%\n",
            "Epoch [161/250], Step [230/263], Loss: 0.0657, Accuracy: 97.81%\n",
            "Epoch [161/250], Step [240/263], Loss: 0.0767, Accuracy: 97.19%\n",
            "Epoch [161/250], Step [250/263], Loss: 0.0746, Accuracy: 96.72%\n",
            "Epoch [161/250], Step [260/263], Loss: 0.0683, Accuracy: 97.50%\n",
            "Epoch [162/250], Step [10/263], Loss: 0.0556, Accuracy: 98.28%\n",
            "Epoch [162/250], Step [20/263], Loss: 0.0454, Accuracy: 98.59%\n",
            "Epoch [162/250], Step [30/263], Loss: 0.0771, Accuracy: 97.19%\n",
            "Epoch [162/250], Step [40/263], Loss: 0.0531, Accuracy: 98.75%\n",
            "Epoch [162/250], Step [50/263], Loss: 0.0358, Accuracy: 99.53%\n",
            "Epoch [162/250], Step [60/263], Loss: 0.0433, Accuracy: 98.28%\n",
            "Epoch [162/250], Step [70/263], Loss: 0.0367, Accuracy: 99.22%\n",
            "Epoch [162/250], Step [80/263], Loss: 0.0373, Accuracy: 98.91%\n",
            "Epoch [162/250], Step [90/263], Loss: 0.0373, Accuracy: 98.59%\n",
            "Epoch [162/250], Step [100/263], Loss: 0.0531, Accuracy: 98.44%\n",
            "Epoch [162/250], Step [110/263], Loss: 0.0479, Accuracy: 98.28%\n",
            "Epoch [162/250], Step [120/263], Loss: 0.0494, Accuracy: 98.12%\n",
            "Epoch [162/250], Step [130/263], Loss: 0.0340, Accuracy: 99.06%\n",
            "Epoch [162/250], Step [140/263], Loss: 0.0424, Accuracy: 98.75%\n",
            "Epoch [162/250], Step [150/263], Loss: 0.0502, Accuracy: 98.44%\n",
            "Epoch [162/250], Step [160/263], Loss: 0.0475, Accuracy: 98.44%\n",
            "Epoch [162/250], Step [170/263], Loss: 0.0404, Accuracy: 98.59%\n",
            "Epoch [162/250], Step [180/263], Loss: 0.0475, Accuracy: 98.75%\n",
            "Epoch [162/250], Step [190/263], Loss: 0.0442, Accuracy: 98.75%\n",
            "Epoch [162/250], Step [200/263], Loss: 0.0328, Accuracy: 99.06%\n",
            "Epoch [162/250], Step [210/263], Loss: 0.0407, Accuracy: 98.59%\n",
            "Epoch [162/250], Step [220/263], Loss: 0.0420, Accuracy: 99.06%\n",
            "Epoch [162/250], Step [230/263], Loss: 0.0443, Accuracy: 98.75%\n",
            "Epoch [162/250], Step [240/263], Loss: 0.0394, Accuracy: 98.44%\n",
            "Epoch [162/250], Step [250/263], Loss: 0.0486, Accuracy: 98.44%\n",
            "Epoch [162/250], Step [260/263], Loss: 0.0626, Accuracy: 98.12%\n",
            "Epoch [163/250], Step [10/263], Loss: 0.0339, Accuracy: 98.91%\n",
            "Epoch [163/250], Step [20/263], Loss: 0.0388, Accuracy: 98.59%\n",
            "Epoch [163/250], Step [30/263], Loss: 0.0373, Accuracy: 98.91%\n",
            "Epoch [163/250], Step [40/263], Loss: 0.0278, Accuracy: 99.53%\n",
            "Epoch [163/250], Step [50/263], Loss: 0.0210, Accuracy: 99.84%\n",
            "Epoch [163/250], Step [60/263], Loss: 0.0227, Accuracy: 99.69%\n",
            "Epoch [163/250], Step [70/263], Loss: 0.0190, Accuracy: 100.00%\n",
            "Epoch [163/250], Step [80/263], Loss: 0.0206, Accuracy: 99.69%\n",
            "Epoch [163/250], Step [90/263], Loss: 0.0320, Accuracy: 99.22%\n",
            "Epoch [163/250], Step [100/263], Loss: 0.0357, Accuracy: 99.06%\n",
            "Epoch [163/250], Step [110/263], Loss: 0.0377, Accuracy: 98.91%\n",
            "Epoch [163/250], Step [120/263], Loss: 0.0223, Accuracy: 99.53%\n",
            "Epoch [163/250], Step [130/263], Loss: 0.0297, Accuracy: 99.22%\n",
            "Epoch [163/250], Step [140/263], Loss: 0.0399, Accuracy: 98.44%\n",
            "Epoch [163/250], Step [150/263], Loss: 0.0308, Accuracy: 99.22%\n",
            "Epoch [163/250], Step [160/263], Loss: 0.0284, Accuracy: 99.38%\n",
            "Epoch [163/250], Step [170/263], Loss: 0.0315, Accuracy: 99.22%\n",
            "Epoch [163/250], Step [180/263], Loss: 0.0274, Accuracy: 99.53%\n",
            "Epoch [163/250], Step [190/263], Loss: 0.0323, Accuracy: 99.22%\n",
            "Epoch [163/250], Step [200/263], Loss: 0.0425, Accuracy: 98.75%\n",
            "Epoch [163/250], Step [210/263], Loss: 0.0358, Accuracy: 99.22%\n",
            "Epoch [163/250], Step [220/263], Loss: 0.0324, Accuracy: 98.91%\n",
            "Epoch [163/250], Step [230/263], Loss: 0.0303, Accuracy: 99.22%\n",
            "Epoch [163/250], Step [240/263], Loss: 0.0416, Accuracy: 98.28%\n",
            "Epoch [163/250], Step [250/263], Loss: 0.0354, Accuracy: 98.91%\n",
            "Epoch [163/250], Step [260/263], Loss: 0.0361, Accuracy: 98.75%\n",
            "Epoch [164/250], Step [10/263], Loss: 0.0489, Accuracy: 98.59%\n",
            "Epoch [164/250], Step [20/263], Loss: 0.0575, Accuracy: 98.12%\n",
            "Epoch [164/250], Step [30/263], Loss: 0.0770, Accuracy: 97.19%\n",
            "Epoch [164/250], Step [40/263], Loss: 0.0858, Accuracy: 96.72%\n",
            "Epoch [164/250], Step [50/263], Loss: 0.0750, Accuracy: 98.12%\n",
            "Epoch [164/250], Step [60/263], Loss: 0.0757, Accuracy: 96.88%\n",
            "Epoch [164/250], Step [70/263], Loss: 0.0613, Accuracy: 97.81%\n",
            "Epoch [164/250], Step [80/263], Loss: 0.0634, Accuracy: 97.50%\n",
            "Epoch [164/250], Step [90/263], Loss: 0.0843, Accuracy: 97.19%\n",
            "Epoch [164/250], Step [100/263], Loss: 0.0905, Accuracy: 97.03%\n",
            "Epoch [164/250], Step [110/263], Loss: 0.0820, Accuracy: 97.03%\n",
            "Epoch [164/250], Step [120/263], Loss: 0.1225, Accuracy: 95.62%\n",
            "Epoch [164/250], Step [130/263], Loss: 0.0732, Accuracy: 96.56%\n",
            "Epoch [164/250], Step [140/263], Loss: 0.0681, Accuracy: 97.50%\n",
            "Epoch [164/250], Step [150/263], Loss: 0.1463, Accuracy: 93.91%\n",
            "Epoch [164/250], Step [160/263], Loss: 0.1202, Accuracy: 95.00%\n",
            "Epoch [164/250], Step [170/263], Loss: 0.0772, Accuracy: 97.50%\n",
            "Epoch [164/250], Step [180/263], Loss: 0.1198, Accuracy: 96.41%\n",
            "Epoch [164/250], Step [190/263], Loss: 0.1310, Accuracy: 95.78%\n",
            "Epoch [164/250], Step [200/263], Loss: 0.1454, Accuracy: 95.16%\n",
            "Epoch [164/250], Step [210/263], Loss: 0.1790, Accuracy: 94.06%\n",
            "Epoch [164/250], Step [220/263], Loss: 0.1313, Accuracy: 95.31%\n",
            "Epoch [164/250], Step [230/263], Loss: 0.0831, Accuracy: 96.88%\n",
            "Epoch [164/250], Step [240/263], Loss: 0.5696, Accuracy: 85.31%\n",
            "Epoch [164/250], Step [250/263], Loss: 0.3497, Accuracy: 87.34%\n",
            "Epoch [164/250], Step [260/263], Loss: 0.2752, Accuracy: 91.09%\n",
            "Epoch [165/250], Step [10/263], Loss: 0.1433, Accuracy: 93.91%\n",
            "Epoch [165/250], Step [20/263], Loss: 0.1802, Accuracy: 93.12%\n",
            "Epoch [165/250], Step [30/263], Loss: 0.1431, Accuracy: 93.91%\n",
            "Epoch [165/250], Step [40/263], Loss: 0.1296, Accuracy: 95.31%\n",
            "Epoch [165/250], Step [50/263], Loss: 0.1168, Accuracy: 94.84%\n",
            "Epoch [165/250], Step [60/263], Loss: 0.1094, Accuracy: 96.88%\n",
            "Epoch [165/250], Step [70/263], Loss: 0.1130, Accuracy: 96.09%\n",
            "Epoch [165/250], Step [80/263], Loss: 0.1059, Accuracy: 95.47%\n",
            "Epoch [165/250], Step [90/263], Loss: 0.1077, Accuracy: 96.25%\n",
            "Epoch [165/250], Step [100/263], Loss: 0.1099, Accuracy: 96.09%\n",
            "Epoch [165/250], Step [110/263], Loss: 0.1014, Accuracy: 95.94%\n",
            "Epoch [165/250], Step [120/263], Loss: 0.0643, Accuracy: 97.97%\n",
            "Epoch [165/250], Step [130/263], Loss: 0.0990, Accuracy: 95.62%\n",
            "Epoch [165/250], Step [140/263], Loss: 0.0947, Accuracy: 97.03%\n",
            "Epoch [165/250], Step [150/263], Loss: 0.0712, Accuracy: 97.66%\n",
            "Epoch [165/250], Step [160/263], Loss: 0.0816, Accuracy: 96.56%\n",
            "Epoch [165/250], Step [170/263], Loss: 0.0624, Accuracy: 97.50%\n",
            "Epoch [165/250], Step [180/263], Loss: 0.0688, Accuracy: 97.81%\n",
            "Epoch [165/250], Step [190/263], Loss: 0.0781, Accuracy: 97.50%\n",
            "Epoch [165/250], Step [200/263], Loss: 0.1036, Accuracy: 96.09%\n",
            "Epoch [165/250], Step [210/263], Loss: 0.0700, Accuracy: 97.50%\n",
            "Epoch [165/250], Step [220/263], Loss: 0.0620, Accuracy: 97.97%\n",
            "Epoch [165/250], Step [230/263], Loss: 0.0590, Accuracy: 97.66%\n",
            "Epoch [165/250], Step [240/263], Loss: 0.0845, Accuracy: 96.41%\n",
            "Epoch [165/250], Step [250/263], Loss: 0.0593, Accuracy: 98.75%\n",
            "Epoch [165/250], Step [260/263], Loss: 0.0520, Accuracy: 98.59%\n",
            "Epoch [166/250], Step [10/263], Loss: 0.0370, Accuracy: 98.91%\n",
            "Epoch [166/250], Step [20/263], Loss: 0.0368, Accuracy: 98.91%\n",
            "Epoch [166/250], Step [30/263], Loss: 0.0319, Accuracy: 99.38%\n",
            "Epoch [166/250], Step [40/263], Loss: 0.0257, Accuracy: 99.53%\n",
            "Epoch [166/250], Step [50/263], Loss: 0.0434, Accuracy: 98.75%\n",
            "Epoch [166/250], Step [60/263], Loss: 0.0327, Accuracy: 99.53%\n",
            "Epoch [166/250], Step [70/263], Loss: 0.0307, Accuracy: 99.69%\n",
            "Epoch [166/250], Step [80/263], Loss: 0.0324, Accuracy: 99.38%\n",
            "Epoch [166/250], Step [90/263], Loss: 0.0428, Accuracy: 99.06%\n",
            "Epoch [166/250], Step [100/263], Loss: 0.0447, Accuracy: 98.91%\n",
            "Epoch [166/250], Step [110/263], Loss: 0.0269, Accuracy: 99.69%\n",
            "Epoch [166/250], Step [120/263], Loss: 0.0384, Accuracy: 98.91%\n",
            "Epoch [166/250], Step [130/263], Loss: 0.0451, Accuracy: 98.59%\n",
            "Epoch [166/250], Step [140/263], Loss: 0.0298, Accuracy: 99.38%\n",
            "Epoch [166/250], Step [150/263], Loss: 0.0410, Accuracy: 98.44%\n",
            "Epoch [166/250], Step [160/263], Loss: 0.0385, Accuracy: 99.06%\n",
            "Epoch [166/250], Step [170/263], Loss: 0.0442, Accuracy: 98.44%\n",
            "Epoch [166/250], Step [180/263], Loss: 0.0571, Accuracy: 98.12%\n",
            "Epoch [166/250], Step [190/263], Loss: 0.0512, Accuracy: 98.28%\n",
            "Epoch [166/250], Step [200/263], Loss: 0.0655, Accuracy: 97.50%\n",
            "Epoch [166/250], Step [210/263], Loss: 0.0512, Accuracy: 98.44%\n",
            "Epoch [166/250], Step [220/263], Loss: 0.0494, Accuracy: 98.44%\n",
            "Epoch [166/250], Step [230/263], Loss: 0.0379, Accuracy: 98.44%\n",
            "Epoch [166/250], Step [240/263], Loss: 0.0550, Accuracy: 98.12%\n",
            "Epoch [166/250], Step [250/263], Loss: 0.0403, Accuracy: 99.06%\n",
            "Epoch [166/250], Step [260/263], Loss: 0.0358, Accuracy: 98.75%\n",
            "Epoch [167/250], Step [10/263], Loss: 0.0418, Accuracy: 98.91%\n",
            "Epoch [167/250], Step [20/263], Loss: 0.0309, Accuracy: 98.75%\n",
            "Epoch [167/250], Step [30/263], Loss: 0.0301, Accuracy: 99.38%\n",
            "Epoch [167/250], Step [40/263], Loss: 0.0255, Accuracy: 99.38%\n",
            "Epoch [167/250], Step [50/263], Loss: 0.0315, Accuracy: 99.38%\n",
            "Epoch [167/250], Step [60/263], Loss: 0.0194, Accuracy: 99.84%\n",
            "Epoch [167/250], Step [70/263], Loss: 0.0354, Accuracy: 98.91%\n",
            "Epoch [167/250], Step [80/263], Loss: 0.0365, Accuracy: 99.38%\n",
            "Epoch [167/250], Step [90/263], Loss: 0.0330, Accuracy: 98.75%\n",
            "Epoch [167/250], Step [100/263], Loss: 0.0365, Accuracy: 99.06%\n",
            "Epoch [167/250], Step [110/263], Loss: 0.0308, Accuracy: 98.91%\n",
            "Epoch [167/250], Step [120/263], Loss: 0.0290, Accuracy: 99.22%\n",
            "Epoch [167/250], Step [130/263], Loss: 0.0307, Accuracy: 99.53%\n",
            "Epoch [167/250], Step [140/263], Loss: 0.0379, Accuracy: 99.06%\n",
            "Epoch [167/250], Step [150/263], Loss: 0.0304, Accuracy: 99.38%\n",
            "Epoch [167/250], Step [160/263], Loss: 0.0261, Accuracy: 99.38%\n",
            "Epoch [167/250], Step [170/263], Loss: 0.0290, Accuracy: 98.91%\n",
            "Epoch [167/250], Step [180/263], Loss: 0.0299, Accuracy: 99.22%\n",
            "Epoch [167/250], Step [190/263], Loss: 0.0245, Accuracy: 99.53%\n",
            "Epoch [167/250], Step [200/263], Loss: 0.0344, Accuracy: 99.06%\n",
            "Epoch [167/250], Step [210/263], Loss: 0.0172, Accuracy: 99.84%\n",
            "Epoch [167/250], Step [220/263], Loss: 0.0385, Accuracy: 98.91%\n",
            "Epoch [167/250], Step [230/263], Loss: 0.0299, Accuracy: 99.53%\n",
            "Epoch [167/250], Step [240/263], Loss: 0.0458, Accuracy: 98.28%\n",
            "Epoch [167/250], Step [250/263], Loss: 0.0403, Accuracy: 98.91%\n",
            "Epoch [167/250], Step [260/263], Loss: 0.0304, Accuracy: 99.53%\n",
            "Epoch [168/250], Step [10/263], Loss: 0.0184, Accuracy: 99.84%\n",
            "Epoch [168/250], Step [20/263], Loss: 0.0249, Accuracy: 99.38%\n",
            "Epoch [168/250], Step [30/263], Loss: 0.0190, Accuracy: 99.69%\n",
            "Epoch [168/250], Step [40/263], Loss: 0.0209, Accuracy: 99.69%\n",
            "Epoch [168/250], Step [50/263], Loss: 0.0293, Accuracy: 99.53%\n",
            "Epoch [168/250], Step [60/263], Loss: 0.0258, Accuracy: 99.38%\n",
            "Epoch [168/250], Step [70/263], Loss: 0.0382, Accuracy: 98.12%\n",
            "Epoch [168/250], Step [80/263], Loss: 0.0272, Accuracy: 99.38%\n",
            "Epoch [168/250], Step [90/263], Loss: 0.0286, Accuracy: 99.06%\n",
            "Epoch [168/250], Step [100/263], Loss: 0.0289, Accuracy: 99.22%\n",
            "Epoch [168/250], Step [110/263], Loss: 0.0421, Accuracy: 98.28%\n",
            "Epoch [168/250], Step [120/263], Loss: 0.0438, Accuracy: 98.44%\n",
            "Epoch [168/250], Step [130/263], Loss: 0.0312, Accuracy: 99.69%\n",
            "Epoch [168/250], Step [140/263], Loss: 0.0505, Accuracy: 98.28%\n",
            "Epoch [168/250], Step [150/263], Loss: 0.0535, Accuracy: 98.28%\n",
            "Epoch [168/250], Step [160/263], Loss: 0.0463, Accuracy: 98.12%\n",
            "Epoch [168/250], Step [170/263], Loss: 0.0375, Accuracy: 98.91%\n",
            "Epoch [168/250], Step [180/263], Loss: 0.0350, Accuracy: 99.53%\n",
            "Epoch [168/250], Step [190/263], Loss: 0.0398, Accuracy: 98.75%\n",
            "Epoch [168/250], Step [200/263], Loss: 0.0355, Accuracy: 98.59%\n",
            "Epoch [168/250], Step [210/263], Loss: 0.0389, Accuracy: 98.59%\n",
            "Epoch [168/250], Step [220/263], Loss: 0.0278, Accuracy: 99.38%\n",
            "Epoch [168/250], Step [230/263], Loss: 0.0304, Accuracy: 99.69%\n",
            "Epoch [168/250], Step [240/263], Loss: 0.0250, Accuracy: 99.53%\n",
            "Epoch [168/250], Step [250/263], Loss: 0.0226, Accuracy: 99.69%\n",
            "Epoch [168/250], Step [260/263], Loss: 0.0213, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [10/263], Loss: 0.0210, Accuracy: 99.53%\n",
            "Epoch [169/250], Step [20/263], Loss: 0.0210, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [30/263], Loss: 0.0185, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [40/263], Loss: 0.0188, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [50/263], Loss: 0.0170, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [60/263], Loss: 0.0199, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [70/263], Loss: 0.0171, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [80/263], Loss: 0.0142, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [90/263], Loss: 0.0214, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [100/263], Loss: 0.0157, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [110/263], Loss: 0.0137, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [120/263], Loss: 0.0166, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [130/263], Loss: 0.0167, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [140/263], Loss: 0.0137, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [150/263], Loss: 0.0205, Accuracy: 99.53%\n",
            "Epoch [169/250], Step [160/263], Loss: 0.0168, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [170/263], Loss: 0.0155, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [180/263], Loss: 0.0212, Accuracy: 99.53%\n",
            "Epoch [169/250], Step [190/263], Loss: 0.0187, Accuracy: 99.53%\n",
            "Epoch [169/250], Step [200/263], Loss: 0.0201, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [210/263], Loss: 0.0192, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [220/263], Loss: 0.0179, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [230/263], Loss: 0.0150, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [240/263], Loss: 0.0178, Accuracy: 99.84%\n",
            "Epoch [169/250], Step [250/263], Loss: 0.0201, Accuracy: 99.69%\n",
            "Epoch [169/250], Step [260/263], Loss: 0.0177, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [10/263], Loss: 0.0161, Accuracy: 99.69%\n",
            "Epoch [170/250], Step [20/263], Loss: 0.0141, Accuracy: 99.84%\n",
            "Epoch [170/250], Step [30/263], Loss: 0.0165, Accuracy: 99.38%\n",
            "Epoch [170/250], Step [40/263], Loss: 0.0133, Accuracy: 99.84%\n",
            "Epoch [170/250], Step [50/263], Loss: 0.0276, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [60/263], Loss: 0.0179, Accuracy: 99.69%\n",
            "Epoch [170/250], Step [70/263], Loss: 0.0199, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [80/263], Loss: 0.0193, Accuracy: 99.69%\n",
            "Epoch [170/250], Step [90/263], Loss: 0.0184, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [100/263], Loss: 0.0195, Accuracy: 99.69%\n",
            "Epoch [170/250], Step [110/263], Loss: 0.0247, Accuracy: 99.38%\n",
            "Epoch [170/250], Step [120/263], Loss: 0.0144, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [130/263], Loss: 0.0151, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [140/263], Loss: 0.0142, Accuracy: 99.84%\n",
            "Epoch [170/250], Step [150/263], Loss: 0.0127, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [160/263], Loss: 0.0164, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [170/263], Loss: 0.0169, Accuracy: 99.84%\n",
            "Epoch [170/250], Step [180/263], Loss: 0.0172, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [190/263], Loss: 0.0238, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [200/263], Loss: 0.0195, Accuracy: 99.84%\n",
            "Epoch [170/250], Step [210/263], Loss: 0.0135, Accuracy: 99.84%\n",
            "Epoch [170/250], Step [220/263], Loss: 0.0311, Accuracy: 99.22%\n",
            "Epoch [170/250], Step [230/263], Loss: 0.0228, Accuracy: 99.69%\n",
            "Epoch [170/250], Step [240/263], Loss: 0.0224, Accuracy: 99.22%\n",
            "Epoch [170/250], Step [250/263], Loss: 0.0220, Accuracy: 99.53%\n",
            "Epoch [170/250], Step [260/263], Loss: 0.0192, Accuracy: 99.53%\n",
            "Epoch [171/250], Step [10/263], Loss: 0.0129, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [20/263], Loss: 0.0129, Accuracy: 99.69%\n",
            "Epoch [171/250], Step [30/263], Loss: 0.0163, Accuracy: 99.69%\n",
            "Epoch [171/250], Step [40/263], Loss: 0.0122, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [50/263], Loss: 0.0115, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [60/263], Loss: 0.0137, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [70/263], Loss: 0.0125, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [80/263], Loss: 0.0158, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [90/263], Loss: 0.0125, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [100/263], Loss: 0.0169, Accuracy: 99.69%\n",
            "Epoch [171/250], Step [110/263], Loss: 0.0179, Accuracy: 99.69%\n",
            "Epoch [171/250], Step [120/263], Loss: 0.0137, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [130/263], Loss: 0.0109, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [140/263], Loss: 0.0178, Accuracy: 99.38%\n",
            "Epoch [171/250], Step [150/263], Loss: 0.0227, Accuracy: 99.22%\n",
            "Epoch [171/250], Step [160/263], Loss: 0.0153, Accuracy: 99.53%\n",
            "Epoch [171/250], Step [170/263], Loss: 0.0173, Accuracy: 99.53%\n",
            "Epoch [171/250], Step [180/263], Loss: 0.0219, Accuracy: 99.38%\n",
            "Epoch [171/250], Step [190/263], Loss: 0.0117, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [200/263], Loss: 0.0118, Accuracy: 99.84%\n",
            "Epoch [171/250], Step [210/263], Loss: 0.0267, Accuracy: 98.91%\n",
            "Epoch [171/250], Step [220/263], Loss: 0.0588, Accuracy: 97.34%\n",
            "Epoch [171/250], Step [230/263], Loss: 0.0799, Accuracy: 97.66%\n",
            "Epoch [171/250], Step [240/263], Loss: 0.0993, Accuracy: 96.41%\n",
            "Epoch [171/250], Step [250/263], Loss: 0.1537, Accuracy: 94.38%\n",
            "Epoch [171/250], Step [260/263], Loss: 0.1738, Accuracy: 94.38%\n",
            "Epoch [172/250], Step [10/263], Loss: 0.1370, Accuracy: 95.62%\n",
            "Epoch [172/250], Step [20/263], Loss: 0.1316, Accuracy: 95.94%\n",
            "Epoch [172/250], Step [30/263], Loss: 0.1150, Accuracy: 95.62%\n",
            "Epoch [172/250], Step [40/263], Loss: 0.1064, Accuracy: 95.47%\n",
            "Epoch [172/250], Step [50/263], Loss: 0.0757, Accuracy: 96.41%\n",
            "Epoch [172/250], Step [60/263], Loss: 0.0867, Accuracy: 97.34%\n",
            "Epoch [172/250], Step [70/263], Loss: 0.0893, Accuracy: 96.09%\n",
            "Epoch [172/250], Step [80/263], Loss: 0.1021, Accuracy: 96.41%\n",
            "Epoch [172/250], Step [90/263], Loss: 0.0731, Accuracy: 97.50%\n",
            "Epoch [172/250], Step [100/263], Loss: 0.1028, Accuracy: 96.41%\n",
            "Epoch [172/250], Step [110/263], Loss: 0.0755, Accuracy: 97.34%\n",
            "Epoch [172/250], Step [120/263], Loss: 0.0696, Accuracy: 97.34%\n",
            "Epoch [172/250], Step [130/263], Loss: 0.0942, Accuracy: 96.56%\n",
            "Epoch [172/250], Step [140/263], Loss: 0.1206, Accuracy: 95.00%\n",
            "Epoch [172/250], Step [150/263], Loss: 0.0985, Accuracy: 96.09%\n",
            "Epoch [172/250], Step [160/263], Loss: 0.0767, Accuracy: 96.56%\n",
            "Epoch [172/250], Step [170/263], Loss: 0.0705, Accuracy: 97.81%\n",
            "Epoch [172/250], Step [180/263], Loss: 0.0907, Accuracy: 97.03%\n",
            "Epoch [172/250], Step [190/263], Loss: 0.0851, Accuracy: 97.34%\n",
            "Epoch [172/250], Step [200/263], Loss: 0.0729, Accuracy: 96.88%\n",
            "Epoch [172/250], Step [210/263], Loss: 0.1050, Accuracy: 95.47%\n",
            "Epoch [172/250], Step [220/263], Loss: 0.0679, Accuracy: 97.81%\n",
            "Epoch [172/250], Step [230/263], Loss: 0.0956, Accuracy: 95.94%\n",
            "Epoch [172/250], Step [240/263], Loss: 0.1133, Accuracy: 96.25%\n",
            "Epoch [172/250], Step [250/263], Loss: 0.1165, Accuracy: 96.09%\n",
            "Epoch [172/250], Step [260/263], Loss: 0.0884, Accuracy: 96.56%\n",
            "Epoch [173/250], Step [10/263], Loss: 0.0757, Accuracy: 98.28%\n",
            "Epoch [173/250], Step [20/263], Loss: 0.0763, Accuracy: 97.19%\n",
            "Epoch [173/250], Step [30/263], Loss: 0.0677, Accuracy: 97.50%\n",
            "Epoch [173/250], Step [40/263], Loss: 0.0841, Accuracy: 96.25%\n",
            "Epoch [173/250], Step [50/263], Loss: 0.0577, Accuracy: 97.50%\n",
            "Epoch [173/250], Step [60/263], Loss: 0.0658, Accuracy: 97.81%\n",
            "Epoch [173/250], Step [70/263], Loss: 0.0538, Accuracy: 97.97%\n",
            "Epoch [173/250], Step [80/263], Loss: 0.0579, Accuracy: 98.12%\n",
            "Epoch [173/250], Step [90/263], Loss: 0.0721, Accuracy: 97.81%\n",
            "Epoch [173/250], Step [100/263], Loss: 0.0587, Accuracy: 97.66%\n",
            "Epoch [173/250], Step [110/263], Loss: 0.0690, Accuracy: 97.34%\n",
            "Epoch [173/250], Step [120/263], Loss: 0.0838, Accuracy: 97.34%\n",
            "Epoch [173/250], Step [130/263], Loss: 0.0995, Accuracy: 95.94%\n",
            "Epoch [173/250], Step [140/263], Loss: 0.0862, Accuracy: 96.88%\n",
            "Epoch [173/250], Step [150/263], Loss: 0.0811, Accuracy: 97.66%\n",
            "Epoch [173/250], Step [160/263], Loss: 0.0760, Accuracy: 97.34%\n",
            "Epoch [173/250], Step [170/263], Loss: 0.0808, Accuracy: 96.88%\n",
            "Epoch [173/250], Step [180/263], Loss: 0.0740, Accuracy: 96.56%\n",
            "Epoch [173/250], Step [190/263], Loss: 0.0746, Accuracy: 97.03%\n",
            "Epoch [173/250], Step [200/263], Loss: 0.0686, Accuracy: 97.03%\n",
            "Epoch [173/250], Step [210/263], Loss: 0.0463, Accuracy: 98.91%\n",
            "Epoch [173/250], Step [220/263], Loss: 0.0678, Accuracy: 97.81%\n",
            "Epoch [173/250], Step [230/263], Loss: 0.0378, Accuracy: 98.75%\n",
            "Epoch [173/250], Step [240/263], Loss: 0.0583, Accuracy: 97.50%\n",
            "Epoch [173/250], Step [250/263], Loss: 0.0511, Accuracy: 98.44%\n",
            "Epoch [173/250], Step [260/263], Loss: 0.0451, Accuracy: 98.28%\n",
            "Epoch [174/250], Step [10/263], Loss: 0.0461, Accuracy: 98.28%\n",
            "Epoch [174/250], Step [20/263], Loss: 0.0385, Accuracy: 99.06%\n",
            "Epoch [174/250], Step [30/263], Loss: 0.0375, Accuracy: 98.28%\n",
            "Epoch [174/250], Step [40/263], Loss: 0.0304, Accuracy: 99.38%\n",
            "Epoch [174/250], Step [50/263], Loss: 0.0314, Accuracy: 99.22%\n",
            "Epoch [174/250], Step [60/263], Loss: 0.0324, Accuracy: 99.06%\n",
            "Epoch [174/250], Step [70/263], Loss: 0.0387, Accuracy: 98.59%\n",
            "Epoch [174/250], Step [80/263], Loss: 0.0328, Accuracy: 99.22%\n",
            "Epoch [174/250], Step [90/263], Loss: 0.0395, Accuracy: 99.06%\n",
            "Epoch [174/250], Step [100/263], Loss: 0.0391, Accuracy: 98.91%\n",
            "Epoch [174/250], Step [110/263], Loss: 0.0409, Accuracy: 98.59%\n",
            "Epoch [174/250], Step [120/263], Loss: 0.0334, Accuracy: 99.22%\n",
            "Epoch [174/250], Step [130/263], Loss: 0.0458, Accuracy: 98.44%\n",
            "Epoch [174/250], Step [140/263], Loss: 0.0464, Accuracy: 98.91%\n",
            "Epoch [174/250], Step [150/263], Loss: 0.0330, Accuracy: 99.22%\n",
            "Epoch [174/250], Step [160/263], Loss: 0.0496, Accuracy: 98.12%\n",
            "Epoch [174/250], Step [170/263], Loss: 0.0465, Accuracy: 97.97%\n",
            "Epoch [174/250], Step [180/263], Loss: 0.0286, Accuracy: 99.22%\n",
            "Epoch [174/250], Step [190/263], Loss: 0.0359, Accuracy: 98.75%\n",
            "Epoch [174/250], Step [200/263], Loss: 0.0494, Accuracy: 98.28%\n",
            "Epoch [174/250], Step [210/263], Loss: 0.0389, Accuracy: 99.06%\n",
            "Epoch [174/250], Step [220/263], Loss: 0.0384, Accuracy: 99.06%\n",
            "Epoch [174/250], Step [230/263], Loss: 0.0383, Accuracy: 98.59%\n",
            "Epoch [174/250], Step [240/263], Loss: 0.0335, Accuracy: 99.06%\n",
            "Epoch [174/250], Step [250/263], Loss: 0.0419, Accuracy: 98.44%\n",
            "Epoch [174/250], Step [260/263], Loss: 0.0318, Accuracy: 99.06%\n",
            "Epoch [175/250], Step [10/263], Loss: 0.0269, Accuracy: 99.53%\n",
            "Epoch [175/250], Step [20/263], Loss: 0.0269, Accuracy: 99.22%\n",
            "Epoch [175/250], Step [30/263], Loss: 0.0405, Accuracy: 98.75%\n",
            "Epoch [175/250], Step [40/263], Loss: 0.0502, Accuracy: 97.97%\n",
            "Epoch [175/250], Step [50/263], Loss: 0.0465, Accuracy: 97.97%\n",
            "Epoch [175/250], Step [60/263], Loss: 0.0420, Accuracy: 98.75%\n",
            "Epoch [175/250], Step [70/263], Loss: 0.0578, Accuracy: 97.81%\n",
            "Epoch [175/250], Step [80/263], Loss: 0.0270, Accuracy: 99.06%\n",
            "Epoch [175/250], Step [90/263], Loss: 0.0219, Accuracy: 99.22%\n",
            "Epoch [175/250], Step [100/263], Loss: 0.0291, Accuracy: 99.38%\n",
            "Epoch [175/250], Step [110/263], Loss: 0.0234, Accuracy: 99.53%\n",
            "Epoch [175/250], Step [120/263], Loss: 0.0442, Accuracy: 98.59%\n",
            "Epoch [175/250], Step [130/263], Loss: 0.0224, Accuracy: 99.53%\n",
            "Epoch [175/250], Step [140/263], Loss: 0.0250, Accuracy: 99.53%\n",
            "Epoch [175/250], Step [150/263], Loss: 0.0243, Accuracy: 99.84%\n",
            "Epoch [175/250], Step [160/263], Loss: 0.0386, Accuracy: 99.06%\n",
            "Epoch [175/250], Step [170/263], Loss: 0.0394, Accuracy: 98.59%\n",
            "Epoch [175/250], Step [180/263], Loss: 0.0345, Accuracy: 99.22%\n",
            "Epoch [175/250], Step [190/263], Loss: 0.0328, Accuracy: 98.91%\n",
            "Epoch [175/250], Step [200/263], Loss: 0.0310, Accuracy: 99.06%\n",
            "Epoch [175/250], Step [210/263], Loss: 0.0392, Accuracy: 98.44%\n",
            "Epoch [175/250], Step [220/263], Loss: 0.0311, Accuracy: 99.22%\n",
            "Epoch [175/250], Step [230/263], Loss: 0.0342, Accuracy: 98.75%\n",
            "Epoch [175/250], Step [240/263], Loss: 0.0230, Accuracy: 99.69%\n",
            "Epoch [175/250], Step [250/263], Loss: 0.0394, Accuracy: 98.91%\n",
            "Epoch [175/250], Step [260/263], Loss: 0.0335, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [10/263], Loss: 0.0296, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [20/263], Loss: 0.0234, Accuracy: 99.22%\n",
            "Epoch [176/250], Step [30/263], Loss: 0.0163, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [40/263], Loss: 0.0218, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [50/263], Loss: 0.0177, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [60/263], Loss: 0.0191, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [70/263], Loss: 0.0145, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [80/263], Loss: 0.0176, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [90/263], Loss: 0.0184, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [100/263], Loss: 0.0133, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [110/263], Loss: 0.0192, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [120/263], Loss: 0.0148, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [130/263], Loss: 0.0235, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [140/263], Loss: 0.0175, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [150/263], Loss: 0.0166, Accuracy: 99.22%\n",
            "Epoch [176/250], Step [160/263], Loss: 0.0234, Accuracy: 99.22%\n",
            "Epoch [176/250], Step [170/263], Loss: 0.0167, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [180/263], Loss: 0.0192, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [190/263], Loss: 0.0217, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [200/263], Loss: 0.0130, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [210/263], Loss: 0.0163, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [220/263], Loss: 0.0321, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [230/263], Loss: 0.0318, Accuracy: 98.59%\n",
            "Epoch [176/250], Step [240/263], Loss: 0.0261, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [250/263], Loss: 0.0190, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [260/263], Loss: 0.0182, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [10/263], Loss: 0.0465, Accuracy: 98.44%\n",
            "Epoch [177/250], Step [20/263], Loss: 0.0305, Accuracy: 99.22%\n",
            "Epoch [177/250], Step [30/263], Loss: 0.0298, Accuracy: 98.75%\n",
            "Epoch [177/250], Step [40/263], Loss: 0.0311, Accuracy: 99.06%\n",
            "Epoch [177/250], Step [50/263], Loss: 0.0190, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [60/263], Loss: 0.0266, Accuracy: 99.22%\n",
            "Epoch [177/250], Step [70/263], Loss: 0.0273, Accuracy: 99.38%\n",
            "Epoch [177/250], Step [80/263], Loss: 0.0176, Accuracy: 99.53%\n",
            "Epoch [177/250], Step [90/263], Loss: 0.0201, Accuracy: 99.38%\n",
            "Epoch [177/250], Step [100/263], Loss: 0.0228, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [110/263], Loss: 0.0229, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [120/263], Loss: 0.0156, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [130/263], Loss: 0.0265, Accuracy: 99.22%\n",
            "Epoch [177/250], Step [140/263], Loss: 0.0156, Accuracy: 99.53%\n",
            "Epoch [177/250], Step [150/263], Loss: 0.0326, Accuracy: 98.75%\n",
            "Epoch [177/250], Step [160/263], Loss: 0.0293, Accuracy: 99.22%\n",
            "Epoch [177/250], Step [170/263], Loss: 0.0263, Accuracy: 99.38%\n",
            "Epoch [177/250], Step [180/263], Loss: 0.0342, Accuracy: 99.22%\n",
            "Epoch [177/250], Step [190/263], Loss: 0.0446, Accuracy: 98.44%\n",
            "Epoch [177/250], Step [200/263], Loss: 0.0453, Accuracy: 98.44%\n",
            "Epoch [177/250], Step [210/263], Loss: 0.0352, Accuracy: 99.06%\n",
            "Epoch [177/250], Step [220/263], Loss: 0.0286, Accuracy: 99.22%\n",
            "Epoch [177/250], Step [230/263], Loss: 0.0317, Accuracy: 99.06%\n",
            "Epoch [177/250], Step [240/263], Loss: 0.0256, Accuracy: 99.38%\n",
            "Epoch [177/250], Step [250/263], Loss: 0.0420, Accuracy: 98.44%\n",
            "Epoch [177/250], Step [260/263], Loss: 0.0373, Accuracy: 98.59%\n",
            "Epoch [178/250], Step [10/263], Loss: 0.0433, Accuracy: 98.75%\n",
            "Epoch [178/250], Step [20/263], Loss: 0.0809, Accuracy: 97.97%\n",
            "Epoch [178/250], Step [30/263], Loss: 0.0649, Accuracy: 98.44%\n",
            "Epoch [178/250], Step [40/263], Loss: 0.0529, Accuracy: 98.44%\n",
            "Epoch [178/250], Step [50/263], Loss: 0.0542, Accuracy: 97.81%\n",
            "Epoch [178/250], Step [60/263], Loss: 0.0508, Accuracy: 98.12%\n",
            "Epoch [178/250], Step [70/263], Loss: 0.0361, Accuracy: 98.91%\n",
            "Epoch [178/250], Step [80/263], Loss: 0.0564, Accuracy: 98.75%\n",
            "Epoch [178/250], Step [90/263], Loss: 0.0333, Accuracy: 98.91%\n",
            "Epoch [178/250], Step [100/263], Loss: 0.0503, Accuracy: 98.59%\n",
            "Epoch [178/250], Step [110/263], Loss: 0.0582, Accuracy: 98.59%\n",
            "Epoch [178/250], Step [120/263], Loss: 0.0524, Accuracy: 97.97%\n",
            "Epoch [178/250], Step [130/263], Loss: 0.0293, Accuracy: 99.06%\n",
            "Epoch [178/250], Step [140/263], Loss: 0.0504, Accuracy: 98.12%\n",
            "Epoch [178/250], Step [150/263], Loss: 0.0413, Accuracy: 98.28%\n",
            "Epoch [178/250], Step [160/263], Loss: 0.0517, Accuracy: 98.44%\n",
            "Epoch [178/250], Step [170/263], Loss: 0.0394, Accuracy: 98.44%\n",
            "Epoch [178/250], Step [180/263], Loss: 0.0407, Accuracy: 98.44%\n",
            "Epoch [178/250], Step [190/263], Loss: 0.0305, Accuracy: 98.75%\n",
            "Epoch [178/250], Step [200/263], Loss: 0.0368, Accuracy: 98.44%\n",
            "Epoch [178/250], Step [210/263], Loss: 0.0465, Accuracy: 98.44%\n",
            "Epoch [178/250], Step [220/263], Loss: 0.0549, Accuracy: 98.12%\n",
            "Epoch [178/250], Step [230/263], Loss: 0.0528, Accuracy: 97.97%\n",
            "Epoch [178/250], Step [240/263], Loss: 0.0368, Accuracy: 98.59%\n",
            "Epoch [178/250], Step [250/263], Loss: 0.0531, Accuracy: 98.12%\n",
            "Epoch [178/250], Step [260/263], Loss: 0.0478, Accuracy: 98.12%\n",
            "Epoch [179/250], Step [10/263], Loss: 0.0227, Accuracy: 99.38%\n",
            "Epoch [179/250], Step [20/263], Loss: 0.0353, Accuracy: 98.44%\n",
            "Epoch [179/250], Step [30/263], Loss: 0.0441, Accuracy: 99.06%\n",
            "Epoch [179/250], Step [40/263], Loss: 0.0295, Accuracy: 99.38%\n",
            "Epoch [179/250], Step [50/263], Loss: 0.0375, Accuracy: 99.06%\n",
            "Epoch [179/250], Step [60/263], Loss: 0.0235, Accuracy: 99.53%\n",
            "Epoch [179/250], Step [70/263], Loss: 0.0329, Accuracy: 99.22%\n",
            "Epoch [179/250], Step [80/263], Loss: 0.0281, Accuracy: 99.06%\n",
            "Epoch [179/250], Step [90/263], Loss: 0.0300, Accuracy: 99.38%\n",
            "Epoch [179/250], Step [100/263], Loss: 0.0367, Accuracy: 98.91%\n",
            "Epoch [179/250], Step [110/263], Loss: 0.0302, Accuracy: 98.91%\n",
            "Epoch [179/250], Step [120/263], Loss: 0.0446, Accuracy: 98.12%\n",
            "Epoch [179/250], Step [130/263], Loss: 0.0388, Accuracy: 98.59%\n",
            "Epoch [179/250], Step [140/263], Loss: 0.0722, Accuracy: 97.03%\n",
            "Epoch [179/250], Step [150/263], Loss: 0.0746, Accuracy: 96.88%\n",
            "Epoch [179/250], Step [160/263], Loss: 0.0832, Accuracy: 97.34%\n",
            "Epoch [179/250], Step [170/263], Loss: 0.0602, Accuracy: 97.81%\n",
            "Epoch [179/250], Step [180/263], Loss: 0.1354, Accuracy: 95.62%\n",
            "Epoch [179/250], Step [190/263], Loss: 0.0957, Accuracy: 96.09%\n",
            "Epoch [179/250], Step [200/263], Loss: 0.0632, Accuracy: 97.81%\n",
            "Epoch [179/250], Step [210/263], Loss: 0.0871, Accuracy: 96.88%\n",
            "Epoch [179/250], Step [220/263], Loss: 0.0623, Accuracy: 97.81%\n",
            "Epoch [179/250], Step [230/263], Loss: 0.1096, Accuracy: 96.25%\n",
            "Epoch [179/250], Step [240/263], Loss: 0.1075, Accuracy: 96.72%\n",
            "Epoch [179/250], Step [250/263], Loss: 0.1255, Accuracy: 96.41%\n",
            "Epoch [179/250], Step [260/263], Loss: 0.1325, Accuracy: 95.94%\n",
            "Epoch [180/250], Step [10/263], Loss: 0.0728, Accuracy: 97.50%\n",
            "Epoch [180/250], Step [20/263], Loss: 0.0711, Accuracy: 97.34%\n",
            "Epoch [180/250], Step [30/263], Loss: 0.0621, Accuracy: 97.81%\n",
            "Epoch [180/250], Step [40/263], Loss: 0.0777, Accuracy: 97.19%\n",
            "Epoch [180/250], Step [50/263], Loss: 0.1433, Accuracy: 94.53%\n",
            "Epoch [180/250], Step [60/263], Loss: 0.1035, Accuracy: 96.41%\n",
            "Epoch [180/250], Step [70/263], Loss: 0.1098, Accuracy: 96.72%\n",
            "Epoch [180/250], Step [80/263], Loss: 0.0968, Accuracy: 96.72%\n",
            "Epoch [180/250], Step [90/263], Loss: 0.0880, Accuracy: 96.56%\n",
            "Epoch [180/250], Step [100/263], Loss: 0.0524, Accuracy: 97.66%\n",
            "Epoch [180/250], Step [110/263], Loss: 0.0442, Accuracy: 98.44%\n",
            "Epoch [180/250], Step [120/263], Loss: 0.0801, Accuracy: 97.50%\n",
            "Epoch [180/250], Step [130/263], Loss: 0.0803, Accuracy: 97.34%\n",
            "Epoch [180/250], Step [140/263], Loss: 0.0806, Accuracy: 97.50%\n",
            "Epoch [180/250], Step [150/263], Loss: 0.0936, Accuracy: 95.94%\n",
            "Epoch [180/250], Step [160/263], Loss: 0.1517, Accuracy: 94.84%\n",
            "Epoch [180/250], Step [170/263], Loss: 0.1556, Accuracy: 94.84%\n",
            "Epoch [180/250], Step [180/263], Loss: 0.1418, Accuracy: 95.47%\n",
            "Epoch [180/250], Step [190/263], Loss: 0.1713, Accuracy: 95.31%\n",
            "Epoch [180/250], Step [200/263], Loss: 0.1332, Accuracy: 95.00%\n",
            "Epoch [180/250], Step [210/263], Loss: 0.1291, Accuracy: 95.16%\n",
            "Epoch [180/250], Step [220/263], Loss: 0.0962, Accuracy: 96.56%\n",
            "Epoch [180/250], Step [230/263], Loss: 0.1193, Accuracy: 95.16%\n",
            "Epoch [180/250], Step [240/263], Loss: 0.0811, Accuracy: 96.41%\n",
            "Epoch [180/250], Step [250/263], Loss: 0.0895, Accuracy: 96.25%\n",
            "Epoch [180/250], Step [260/263], Loss: 0.1061, Accuracy: 96.25%\n",
            "Epoch [181/250], Step [10/263], Loss: 0.0895, Accuracy: 96.72%\n",
            "Epoch [181/250], Step [20/263], Loss: 0.0887, Accuracy: 97.19%\n",
            "Epoch [181/250], Step [30/263], Loss: 0.0797, Accuracy: 97.34%\n",
            "Epoch [181/250], Step [40/263], Loss: 0.0838, Accuracy: 97.19%\n",
            "Epoch [181/250], Step [50/263], Loss: 0.0565, Accuracy: 98.28%\n",
            "Epoch [181/250], Step [60/263], Loss: 0.0899, Accuracy: 97.34%\n",
            "Epoch [181/250], Step [70/263], Loss: 0.0880, Accuracy: 96.56%\n",
            "Epoch [181/250], Step [80/263], Loss: 0.0871, Accuracy: 97.19%\n",
            "Epoch [181/250], Step [90/263], Loss: 0.0656, Accuracy: 97.66%\n",
            "Epoch [181/250], Step [100/263], Loss: 0.0988, Accuracy: 97.19%\n",
            "Epoch [181/250], Step [110/263], Loss: 0.0547, Accuracy: 98.12%\n",
            "Epoch [181/250], Step [120/263], Loss: 0.0769, Accuracy: 96.88%\n",
            "Epoch [181/250], Step [130/263], Loss: 0.0508, Accuracy: 97.97%\n",
            "Epoch [181/250], Step [140/263], Loss: 0.0528, Accuracy: 98.12%\n",
            "Epoch [181/250], Step [150/263], Loss: 0.0473, Accuracy: 98.12%\n",
            "Epoch [181/250], Step [160/263], Loss: 0.0379, Accuracy: 98.75%\n",
            "Epoch [181/250], Step [170/263], Loss: 0.0662, Accuracy: 97.81%\n",
            "Epoch [181/250], Step [180/263], Loss: 0.0597, Accuracy: 97.19%\n",
            "Epoch [181/250], Step [190/263], Loss: 0.0533, Accuracy: 98.28%\n",
            "Epoch [181/250], Step [200/263], Loss: 0.0450, Accuracy: 99.06%\n",
            "Epoch [181/250], Step [210/263], Loss: 0.0388, Accuracy: 98.75%\n",
            "Epoch [181/250], Step [220/263], Loss: 0.0346, Accuracy: 98.91%\n",
            "Epoch [181/250], Step [230/263], Loss: 0.0358, Accuracy: 98.91%\n",
            "Epoch [181/250], Step [240/263], Loss: 0.0290, Accuracy: 99.38%\n",
            "Epoch [181/250], Step [250/263], Loss: 0.0403, Accuracy: 98.91%\n",
            "Epoch [181/250], Step [260/263], Loss: 0.0327, Accuracy: 98.75%\n",
            "Epoch [182/250], Step [10/263], Loss: 0.0238, Accuracy: 99.22%\n",
            "Epoch [182/250], Step [20/263], Loss: 0.0236, Accuracy: 99.53%\n",
            "Epoch [182/250], Step [30/263], Loss: 0.0227, Accuracy: 99.38%\n",
            "Epoch [182/250], Step [40/263], Loss: 0.0320, Accuracy: 99.06%\n",
            "Epoch [182/250], Step [50/263], Loss: 0.0645, Accuracy: 98.44%\n",
            "Epoch [182/250], Step [60/263], Loss: 0.0500, Accuracy: 98.44%\n",
            "Epoch [182/250], Step [70/263], Loss: 0.0490, Accuracy: 98.44%\n",
            "Epoch [182/250], Step [80/263], Loss: 0.0463, Accuracy: 98.75%\n",
            "Epoch [182/250], Step [90/263], Loss: 0.0313, Accuracy: 99.53%\n",
            "Epoch [182/250], Step [100/263], Loss: 0.0440, Accuracy: 98.44%\n",
            "Epoch [182/250], Step [110/263], Loss: 0.0481, Accuracy: 98.28%\n",
            "Epoch [182/250], Step [120/263], Loss: 0.0325, Accuracy: 99.22%\n",
            "Epoch [182/250], Step [130/263], Loss: 0.0358, Accuracy: 99.22%\n",
            "Epoch [182/250], Step [140/263], Loss: 0.0529, Accuracy: 97.97%\n",
            "Epoch [182/250], Step [150/263], Loss: 0.0271, Accuracy: 99.53%\n",
            "Epoch [182/250], Step [160/263], Loss: 0.0362, Accuracy: 98.59%\n",
            "Epoch [182/250], Step [170/263], Loss: 0.0284, Accuracy: 99.06%\n",
            "Epoch [182/250], Step [180/263], Loss: 0.0276, Accuracy: 99.69%\n",
            "Epoch [182/250], Step [190/263], Loss: 0.0247, Accuracy: 99.53%\n",
            "Epoch [182/250], Step [200/263], Loss: 0.0192, Accuracy: 99.53%\n",
            "Epoch [182/250], Step [210/263], Loss: 0.0257, Accuracy: 99.38%\n",
            "Epoch [182/250], Step [220/263], Loss: 0.0269, Accuracy: 99.38%\n",
            "Epoch [182/250], Step [230/263], Loss: 0.0233, Accuracy: 99.38%\n",
            "Epoch [182/250], Step [240/263], Loss: 0.0244, Accuracy: 99.38%\n",
            "Epoch [182/250], Step [250/263], Loss: 0.0349, Accuracy: 99.38%\n",
            "Epoch [182/250], Step [260/263], Loss: 0.0374, Accuracy: 99.06%\n",
            "Epoch [183/250], Step [10/263], Loss: 0.0229, Accuracy: 99.53%\n",
            "Epoch [183/250], Step [20/263], Loss: 0.0256, Accuracy: 99.06%\n",
            "Epoch [183/250], Step [30/263], Loss: 0.0152, Accuracy: 99.53%\n",
            "Epoch [183/250], Step [40/263], Loss: 0.0248, Accuracy: 99.22%\n",
            "Epoch [183/250], Step [50/263], Loss: 0.0224, Accuracy: 99.53%\n",
            "Epoch [183/250], Step [60/263], Loss: 0.0192, Accuracy: 99.22%\n",
            "Epoch [183/250], Step [70/263], Loss: 0.0161, Accuracy: 99.69%\n",
            "Epoch [183/250], Step [80/263], Loss: 0.0169, Accuracy: 99.38%\n",
            "Epoch [183/250], Step [90/263], Loss: 0.0152, Accuracy: 99.84%\n",
            "Epoch [183/250], Step [100/263], Loss: 0.0177, Accuracy: 99.69%\n",
            "Epoch [183/250], Step [110/263], Loss: 0.0151, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [120/263], Loss: 0.0249, Accuracy: 99.53%\n",
            "Epoch [183/250], Step [130/263], Loss: 0.0160, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [140/263], Loss: 0.0150, Accuracy: 99.69%\n",
            "Epoch [183/250], Step [150/263], Loss: 0.0172, Accuracy: 99.69%\n",
            "Epoch [183/250], Step [160/263], Loss: 0.0177, Accuracy: 99.53%\n",
            "Epoch [183/250], Step [170/263], Loss: 0.0113, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [180/263], Loss: 0.0166, Accuracy: 99.53%\n",
            "Epoch [183/250], Step [190/263], Loss: 0.0152, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [200/263], Loss: 0.0112, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [210/263], Loss: 0.0150, Accuracy: 99.53%\n",
            "Epoch [183/250], Step [220/263], Loss: 0.0111, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [230/263], Loss: 0.0161, Accuracy: 99.69%\n",
            "Epoch [183/250], Step [240/263], Loss: 0.0099, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [250/263], Loss: 0.0108, Accuracy: 99.84%\n",
            "Epoch [183/250], Step [260/263], Loss: 0.0109, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [10/263], Loss: 0.0109, Accuracy: 99.84%\n",
            "Epoch [184/250], Step [20/263], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [30/263], Loss: 0.0087, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [40/263], Loss: 0.0091, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [50/263], Loss: 0.0116, Accuracy: 99.84%\n",
            "Epoch [184/250], Step [60/263], Loss: 0.0085, Accuracy: 99.84%\n",
            "Epoch [184/250], Step [70/263], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [80/263], Loss: 0.0078, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [90/263], Loss: 0.0106, Accuracy: 99.84%\n",
            "Epoch [184/250], Step [100/263], Loss: 0.0078, Accuracy: 99.84%\n",
            "Epoch [184/250], Step [110/263], Loss: 0.0055, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [120/263], Loss: 0.0084, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [130/263], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [140/263], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [150/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [184/250], Step [160/263], Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [170/263], Loss: 0.0086, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [180/263], Loss: 0.0093, Accuracy: 99.84%\n",
            "Epoch [184/250], Step [190/263], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [200/263], Loss: 0.0114, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [210/263], Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [220/263], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [230/263], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [240/263], Loss: 0.0076, Accuracy: 99.84%\n",
            "Epoch [184/250], Step [250/263], Loss: 0.0089, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [260/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [185/250], Step [10/263], Loss: 0.0090, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [20/263], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [30/263], Loss: 0.0080, Accuracy: 99.84%\n",
            "Epoch [185/250], Step [40/263], Loss: 0.0075, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [50/263], Loss: 0.0056, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [60/263], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [70/263], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [80/263], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [90/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [100/263], Loss: 0.0080, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [110/263], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [120/263], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [130/263], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [140/263], Loss: 0.0131, Accuracy: 99.84%\n",
            "Epoch [185/250], Step [150/263], Loss: 0.0075, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [160/263], Loss: 0.0069, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [170/263], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [180/263], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [190/263], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [200/263], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [210/263], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [220/263], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [230/263], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [240/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [185/250], Step [250/263], Loss: 0.0176, Accuracy: 99.69%\n",
            "Epoch [185/250], Step [260/263], Loss: 0.0106, Accuracy: 99.84%\n",
            "Epoch [186/250], Step [10/263], Loss: 0.0092, Accuracy: 99.84%\n",
            "Epoch [186/250], Step [20/263], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [30/263], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [40/263], Loss: 0.0184, Accuracy: 99.84%\n",
            "Epoch [186/250], Step [50/263], Loss: 0.0090, Accuracy: 99.84%\n",
            "Epoch [186/250], Step [60/263], Loss: 0.0104, Accuracy: 99.69%\n",
            "Epoch [186/250], Step [70/263], Loss: 0.0078, Accuracy: 99.84%\n",
            "Epoch [186/250], Step [80/263], Loss: 0.0101, Accuracy: 99.84%\n",
            "Epoch [186/250], Step [90/263], Loss: 0.0088, Accuracy: 99.69%\n",
            "Epoch [186/250], Step [100/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [186/250], Step [110/263], Loss: 0.0294, Accuracy: 99.38%\n",
            "Epoch [186/250], Step [120/263], Loss: 0.0298, Accuracy: 99.22%\n",
            "Epoch [186/250], Step [130/263], Loss: 0.0523, Accuracy: 98.44%\n",
            "Epoch [186/250], Step [140/263], Loss: 0.0294, Accuracy: 99.22%\n",
            "Epoch [186/250], Step [150/263], Loss: 0.0363, Accuracy: 98.59%\n",
            "Epoch [186/250], Step [160/263], Loss: 0.0618, Accuracy: 97.66%\n",
            "Epoch [186/250], Step [170/263], Loss: 0.0384, Accuracy: 98.28%\n",
            "Epoch [186/250], Step [180/263], Loss: 0.0434, Accuracy: 98.91%\n",
            "Epoch [186/250], Step [190/263], Loss: 0.0481, Accuracy: 98.28%\n",
            "Epoch [186/250], Step [200/263], Loss: 0.0339, Accuracy: 99.53%\n",
            "Epoch [186/250], Step [210/263], Loss: 0.0749, Accuracy: 97.66%\n",
            "Epoch [186/250], Step [220/263], Loss: 0.2483, Accuracy: 92.81%\n",
            "Epoch [186/250], Step [230/263], Loss: 0.1918, Accuracy: 95.16%\n",
            "Epoch [186/250], Step [240/263], Loss: 0.1297, Accuracy: 96.09%\n",
            "Epoch [186/250], Step [250/263], Loss: 0.1739, Accuracy: 94.06%\n",
            "Epoch [186/250], Step [260/263], Loss: 0.1746, Accuracy: 93.28%\n",
            "Epoch [187/250], Step [10/263], Loss: 0.1263, Accuracy: 96.09%\n",
            "Epoch [187/250], Step [20/263], Loss: 0.1520, Accuracy: 95.78%\n",
            "Epoch [187/250], Step [30/263], Loss: 0.0704, Accuracy: 97.50%\n",
            "Epoch [187/250], Step [40/263], Loss: 0.0967, Accuracy: 96.41%\n",
            "Epoch [187/250], Step [50/263], Loss: 0.1371, Accuracy: 95.47%\n",
            "Epoch [187/250], Step [60/263], Loss: 0.1094, Accuracy: 96.09%\n",
            "Epoch [187/250], Step [70/263], Loss: 0.0932, Accuracy: 96.25%\n",
            "Epoch [187/250], Step [80/263], Loss: 0.1198, Accuracy: 96.09%\n",
            "Epoch [187/250], Step [90/263], Loss: 0.0877, Accuracy: 96.88%\n",
            "Epoch [187/250], Step [100/263], Loss: 0.1026, Accuracy: 96.41%\n",
            "Epoch [187/250], Step [110/263], Loss: 0.0940, Accuracy: 95.78%\n",
            "Epoch [187/250], Step [120/263], Loss: 0.0705, Accuracy: 97.03%\n",
            "Epoch [187/250], Step [130/263], Loss: 0.0700, Accuracy: 96.56%\n",
            "Epoch [187/250], Step [140/263], Loss: 0.0813, Accuracy: 96.41%\n",
            "Epoch [187/250], Step [150/263], Loss: 0.0757, Accuracy: 97.19%\n",
            "Epoch [187/250], Step [160/263], Loss: 0.0619, Accuracy: 97.50%\n",
            "Epoch [187/250], Step [170/263], Loss: 0.0717, Accuracy: 98.12%\n",
            "Epoch [187/250], Step [180/263], Loss: 0.0954, Accuracy: 96.72%\n",
            "Epoch [187/250], Step [190/263], Loss: 0.0508, Accuracy: 98.91%\n",
            "Epoch [187/250], Step [200/263], Loss: 0.0632, Accuracy: 97.97%\n",
            "Epoch [187/250], Step [210/263], Loss: 0.0752, Accuracy: 96.72%\n",
            "Epoch [187/250], Step [220/263], Loss: 0.0517, Accuracy: 98.12%\n",
            "Epoch [187/250], Step [230/263], Loss: 0.0497, Accuracy: 98.28%\n",
            "Epoch [187/250], Step [240/263], Loss: 0.0478, Accuracy: 98.44%\n",
            "Epoch [187/250], Step [250/263], Loss: 0.0397, Accuracy: 98.75%\n",
            "Epoch [187/250], Step [260/263], Loss: 0.0470, Accuracy: 98.12%\n",
            "Epoch [188/250], Step [10/263], Loss: 0.0351, Accuracy: 98.91%\n",
            "Epoch [188/250], Step [20/263], Loss: 0.0451, Accuracy: 98.12%\n",
            "Epoch [188/250], Step [30/263], Loss: 0.0382, Accuracy: 99.06%\n",
            "Epoch [188/250], Step [40/263], Loss: 0.0269, Accuracy: 99.38%\n",
            "Epoch [188/250], Step [50/263], Loss: 0.0461, Accuracy: 98.12%\n",
            "Epoch [188/250], Step [60/263], Loss: 0.0373, Accuracy: 98.91%\n",
            "Epoch [188/250], Step [70/263], Loss: 0.0334, Accuracy: 98.91%\n",
            "Epoch [188/250], Step [80/263], Loss: 0.0313, Accuracy: 98.91%\n",
            "Epoch [188/250], Step [90/263], Loss: 0.0217, Accuracy: 99.84%\n",
            "Epoch [188/250], Step [100/263], Loss: 0.0243, Accuracy: 98.91%\n",
            "Epoch [188/250], Step [110/263], Loss: 0.0228, Accuracy: 99.69%\n",
            "Epoch [188/250], Step [120/263], Loss: 0.0283, Accuracy: 99.06%\n",
            "Epoch [188/250], Step [130/263], Loss: 0.0193, Accuracy: 99.53%\n",
            "Epoch [188/250], Step [140/263], Loss: 0.0208, Accuracy: 99.84%\n",
            "Epoch [188/250], Step [150/263], Loss: 0.0228, Accuracy: 99.38%\n",
            "Epoch [188/250], Step [160/263], Loss: 0.0208, Accuracy: 99.69%\n",
            "Epoch [188/250], Step [170/263], Loss: 0.0190, Accuracy: 99.53%\n",
            "Epoch [188/250], Step [180/263], Loss: 0.0259, Accuracy: 99.06%\n",
            "Epoch [188/250], Step [190/263], Loss: 0.0304, Accuracy: 99.22%\n",
            "Epoch [188/250], Step [200/263], Loss: 0.0202, Accuracy: 99.38%\n",
            "Epoch [188/250], Step [210/263], Loss: 0.0297, Accuracy: 99.06%\n",
            "Epoch [188/250], Step [220/263], Loss: 0.0248, Accuracy: 99.22%\n",
            "Epoch [188/250], Step [230/263], Loss: 0.0188, Accuracy: 99.69%\n",
            "Epoch [188/250], Step [240/263], Loss: 0.0229, Accuracy: 99.38%\n",
            "Epoch [188/250], Step [250/263], Loss: 0.0194, Accuracy: 99.53%\n",
            "Epoch [188/250], Step [260/263], Loss: 0.0228, Accuracy: 99.69%\n",
            "Epoch [189/250], Step [10/263], Loss: 0.0200, Accuracy: 99.53%\n",
            "Epoch [189/250], Step [20/263], Loss: 0.0134, Accuracy: 99.69%\n",
            "Epoch [189/250], Step [30/263], Loss: 0.0175, Accuracy: 99.69%\n",
            "Epoch [189/250], Step [40/263], Loss: 0.0116, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [50/263], Loss: 0.0089, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [60/263], Loss: 0.0087, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [70/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [189/250], Step [80/263], Loss: 0.0111, Accuracy: 99.84%\n",
            "Epoch [189/250], Step [90/263], Loss: 0.0119, Accuracy: 99.84%\n",
            "Epoch [189/250], Step [100/263], Loss: 0.0140, Accuracy: 99.69%\n",
            "Epoch [189/250], Step [110/263], Loss: 0.0316, Accuracy: 98.91%\n",
            "Epoch [189/250], Step [120/263], Loss: 0.0264, Accuracy: 98.91%\n",
            "Epoch [189/250], Step [130/263], Loss: 0.0207, Accuracy: 99.53%\n",
            "Epoch [189/250], Step [140/263], Loss: 0.0197, Accuracy: 99.38%\n",
            "Epoch [189/250], Step [150/263], Loss: 0.0223, Accuracy: 99.53%\n",
            "Epoch [189/250], Step [160/263], Loss: 0.0218, Accuracy: 99.38%\n",
            "Epoch [189/250], Step [170/263], Loss: 0.0199, Accuracy: 99.69%\n",
            "Epoch [189/250], Step [180/263], Loss: 0.0231, Accuracy: 99.38%\n",
            "Epoch [189/250], Step [190/263], Loss: 0.0266, Accuracy: 99.22%\n",
            "Epoch [189/250], Step [200/263], Loss: 0.0170, Accuracy: 99.69%\n",
            "Epoch [189/250], Step [210/263], Loss: 0.0224, Accuracy: 99.22%\n",
            "Epoch [189/250], Step [220/263], Loss: 0.0116, Accuracy: 99.84%\n",
            "Epoch [189/250], Step [230/263], Loss: 0.0142, Accuracy: 99.84%\n",
            "Epoch [189/250], Step [240/263], Loss: 0.0126, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [250/263], Loss: 0.0167, Accuracy: 99.69%\n",
            "Epoch [189/250], Step [260/263], Loss: 0.0133, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [10/263], Loss: 0.0181, Accuracy: 99.38%\n",
            "Epoch [190/250], Step [20/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [30/263], Loss: 0.0173, Accuracy: 99.22%\n",
            "Epoch [190/250], Step [40/263], Loss: 0.0159, Accuracy: 99.22%\n",
            "Epoch [190/250], Step [50/263], Loss: 0.0117, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [60/263], Loss: 0.0102, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [70/263], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [190/250], Step [80/263], Loss: 0.0078, Accuracy: 100.00%\n",
            "Epoch [190/250], Step [90/263], Loss: 0.0084, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [100/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [110/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [120/263], Loss: 0.0129, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [130/263], Loss: 0.0088, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [140/263], Loss: 0.0093, Accuracy: 100.00%\n",
            "Epoch [190/250], Step [150/263], Loss: 0.0092, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [160/263], Loss: 0.0203, Accuracy: 99.53%\n",
            "Epoch [190/250], Step [170/263], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [190/250], Step [180/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [190/263], Loss: 0.0130, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [200/263], Loss: 0.0138, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [210/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [190/250], Step [220/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [230/263], Loss: 0.0117, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [240/263], Loss: 0.0097, Accuracy: 100.00%\n",
            "Epoch [190/250], Step [250/263], Loss: 0.0108, Accuracy: 99.84%\n",
            "Epoch [190/250], Step [260/263], Loss: 0.0117, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [10/263], Loss: 0.0076, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [20/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [30/263], Loss: 0.0131, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [40/263], Loss: 0.0084, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [50/263], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [60/263], Loss: 0.0096, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [70/263], Loss: 0.0118, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [80/263], Loss: 0.0096, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [90/263], Loss: 0.0084, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [100/263], Loss: 0.0101, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [110/263], Loss: 0.0108, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [120/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [130/263], Loss: 0.0276, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [140/263], Loss: 0.0165, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [150/263], Loss: 0.0181, Accuracy: 99.53%\n",
            "Epoch [191/250], Step [160/263], Loss: 0.0256, Accuracy: 99.38%\n",
            "Epoch [191/250], Step [170/263], Loss: 0.0110, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [180/263], Loss: 0.0160, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [190/263], Loss: 0.0180, Accuracy: 99.53%\n",
            "Epoch [191/250], Step [200/263], Loss: 0.0112, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [210/263], Loss: 0.0067, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [220/263], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [230/263], Loss: 0.0090, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [240/263], Loss: 0.0116, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [250/263], Loss: 0.0087, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [260/263], Loss: 0.0121, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [10/263], Loss: 0.0104, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [20/263], Loss: 0.0107, Accuracy: 99.69%\n",
            "Epoch [192/250], Step [30/263], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [40/263], Loss: 0.0128, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [50/263], Loss: 0.0156, Accuracy: 99.69%\n",
            "Epoch [192/250], Step [60/263], Loss: 0.0139, Accuracy: 99.69%\n",
            "Epoch [192/250], Step [70/263], Loss: 0.0084, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [80/263], Loss: 0.0099, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [90/263], Loss: 0.0085, Accuracy: 99.69%\n",
            "Epoch [192/250], Step [100/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [110/263], Loss: 0.0120, Accuracy: 99.69%\n",
            "Epoch [192/250], Step [120/263], Loss: 0.0080, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [130/263], Loss: 0.0088, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [140/263], Loss: 0.0063, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [150/263], Loss: 0.0190, Accuracy: 99.53%\n",
            "Epoch [192/250], Step [160/263], Loss: 0.0218, Accuracy: 99.38%\n",
            "Epoch [192/250], Step [170/263], Loss: 0.0234, Accuracy: 99.22%\n",
            "Epoch [192/250], Step [180/263], Loss: 0.0270, Accuracy: 98.91%\n",
            "Epoch [192/250], Step [190/263], Loss: 0.0353, Accuracy: 98.91%\n",
            "Epoch [192/250], Step [200/263], Loss: 0.0350, Accuracy: 98.91%\n",
            "Epoch [192/250], Step [210/263], Loss: 0.0335, Accuracy: 98.44%\n",
            "Epoch [192/250], Step [220/263], Loss: 0.0268, Accuracy: 99.53%\n",
            "Epoch [192/250], Step [230/263], Loss: 0.0317, Accuracy: 98.75%\n",
            "Epoch [192/250], Step [240/263], Loss: 0.0473, Accuracy: 97.97%\n",
            "Epoch [192/250], Step [250/263], Loss: 0.0700, Accuracy: 97.66%\n",
            "Epoch [192/250], Step [260/263], Loss: 0.0615, Accuracy: 97.66%\n",
            "Epoch [193/250], Step [10/263], Loss: 0.0638, Accuracy: 97.66%\n",
            "Epoch [193/250], Step [20/263], Loss: 0.0742, Accuracy: 97.34%\n",
            "Epoch [193/250], Step [30/263], Loss: 0.0615, Accuracy: 97.81%\n",
            "Epoch [193/250], Step [40/263], Loss: 0.0831, Accuracy: 97.50%\n",
            "Epoch [193/250], Step [50/263], Loss: 0.1066, Accuracy: 96.72%\n",
            "Epoch [193/250], Step [60/263], Loss: 0.0975, Accuracy: 96.09%\n",
            "Epoch [193/250], Step [70/263], Loss: 0.1087, Accuracy: 95.62%\n",
            "Epoch [193/250], Step [80/263], Loss: 0.0882, Accuracy: 97.34%\n",
            "Epoch [193/250], Step [90/263], Loss: 0.2147, Accuracy: 94.06%\n",
            "Epoch [193/250], Step [100/263], Loss: 0.1336, Accuracy: 95.31%\n",
            "Epoch [193/250], Step [110/263], Loss: 0.1627, Accuracy: 93.44%\n",
            "Epoch [193/250], Step [120/263], Loss: 0.1348, Accuracy: 95.00%\n",
            "Epoch [193/250], Step [130/263], Loss: 0.2546, Accuracy: 91.88%\n",
            "Epoch [193/250], Step [140/263], Loss: 0.1347, Accuracy: 95.16%\n",
            "Epoch [193/250], Step [150/263], Loss: 0.1191, Accuracy: 95.94%\n",
            "Epoch [193/250], Step [160/263], Loss: 0.1210, Accuracy: 95.78%\n",
            "Epoch [193/250], Step [170/263], Loss: 0.1187, Accuracy: 95.00%\n",
            "Epoch [193/250], Step [180/263], Loss: 0.1168, Accuracy: 95.62%\n",
            "Epoch [193/250], Step [190/263], Loss: 0.1346, Accuracy: 95.47%\n",
            "Epoch [193/250], Step [200/263], Loss: 0.1061, Accuracy: 96.25%\n",
            "Epoch [193/250], Step [210/263], Loss: 0.0975, Accuracy: 96.88%\n",
            "Epoch [193/250], Step [220/263], Loss: 0.0833, Accuracy: 97.03%\n",
            "Epoch [193/250], Step [230/263], Loss: 0.1109, Accuracy: 95.62%\n",
            "Epoch [193/250], Step [240/263], Loss: 0.1028, Accuracy: 96.09%\n",
            "Epoch [193/250], Step [250/263], Loss: 0.1019, Accuracy: 95.94%\n",
            "Epoch [193/250], Step [260/263], Loss: 0.0754, Accuracy: 97.97%\n",
            "Epoch [194/250], Step [10/263], Loss: 0.2483, Accuracy: 92.34%\n",
            "Epoch [194/250], Step [20/263], Loss: 0.2804, Accuracy: 91.41%\n",
            "Epoch [194/250], Step [30/263], Loss: 0.1917, Accuracy: 93.28%\n",
            "Epoch [194/250], Step [40/263], Loss: 0.2083, Accuracy: 94.53%\n",
            "Epoch [194/250], Step [50/263], Loss: 0.1660, Accuracy: 93.12%\n",
            "Epoch [194/250], Step [60/263], Loss: 0.1405, Accuracy: 95.62%\n",
            "Epoch [194/250], Step [70/263], Loss: 0.1061, Accuracy: 96.41%\n",
            "Epoch [194/250], Step [80/263], Loss: 0.1081, Accuracy: 95.78%\n",
            "Epoch [194/250], Step [90/263], Loss: 0.0941, Accuracy: 96.88%\n",
            "Epoch [194/250], Step [100/263], Loss: 0.0818, Accuracy: 96.09%\n",
            "Epoch [194/250], Step [110/263], Loss: 0.0632, Accuracy: 97.66%\n",
            "Epoch [194/250], Step [120/263], Loss: 0.0834, Accuracy: 97.34%\n",
            "Epoch [194/250], Step [130/263], Loss: 0.1041, Accuracy: 96.41%\n",
            "Epoch [194/250], Step [140/263], Loss: 0.0669, Accuracy: 97.50%\n",
            "Epoch [194/250], Step [150/263], Loss: 0.0937, Accuracy: 96.09%\n",
            "Epoch [194/250], Step [160/263], Loss: 0.0470, Accuracy: 98.12%\n",
            "Epoch [194/250], Step [170/263], Loss: 0.0633, Accuracy: 97.97%\n",
            "Epoch [194/250], Step [180/263], Loss: 0.0535, Accuracy: 97.19%\n",
            "Epoch [194/250], Step [190/263], Loss: 0.0447, Accuracy: 98.28%\n",
            "Epoch [194/250], Step [200/263], Loss: 0.0453, Accuracy: 98.44%\n",
            "Epoch [194/250], Step [210/263], Loss: 0.0491, Accuracy: 98.59%\n",
            "Epoch [194/250], Step [220/263], Loss: 0.0385, Accuracy: 98.44%\n",
            "Epoch [194/250], Step [230/263], Loss: 0.0387, Accuracy: 98.91%\n",
            "Epoch [194/250], Step [240/263], Loss: 0.0338, Accuracy: 98.75%\n",
            "Epoch [194/250], Step [250/263], Loss: 0.0550, Accuracy: 97.97%\n",
            "Epoch [194/250], Step [260/263], Loss: 0.0480, Accuracy: 98.59%\n",
            "Epoch [195/250], Step [10/263], Loss: 0.0298, Accuracy: 99.38%\n",
            "Epoch [195/250], Step [20/263], Loss: 0.0222, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [30/263], Loss: 0.0315, Accuracy: 99.22%\n",
            "Epoch [195/250], Step [40/263], Loss: 0.0257, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [50/263], Loss: 0.0207, Accuracy: 99.69%\n",
            "Epoch [195/250], Step [60/263], Loss: 0.0253, Accuracy: 99.22%\n",
            "Epoch [195/250], Step [70/263], Loss: 0.0230, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [80/263], Loss: 0.0203, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [90/263], Loss: 0.0166, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [100/263], Loss: 0.0264, Accuracy: 99.22%\n",
            "Epoch [195/250], Step [110/263], Loss: 0.0204, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [120/263], Loss: 0.0135, Accuracy: 99.84%\n",
            "Epoch [195/250], Step [130/263], Loss: 0.0196, Accuracy: 99.84%\n",
            "Epoch [195/250], Step [140/263], Loss: 0.0208, Accuracy: 99.38%\n",
            "Epoch [195/250], Step [150/263], Loss: 0.0158, Accuracy: 99.69%\n",
            "Epoch [195/250], Step [160/263], Loss: 0.0155, Accuracy: 99.69%\n",
            "Epoch [195/250], Step [170/263], Loss: 0.0298, Accuracy: 99.38%\n",
            "Epoch [195/250], Step [180/263], Loss: 0.0443, Accuracy: 98.91%\n",
            "Epoch [195/250], Step [190/263], Loss: 0.0375, Accuracy: 98.91%\n",
            "Epoch [195/250], Step [200/263], Loss: 0.0219, Accuracy: 99.38%\n",
            "Epoch [195/250], Step [210/263], Loss: 0.0191, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [220/263], Loss: 0.0186, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [230/263], Loss: 0.0195, Accuracy: 99.53%\n",
            "Epoch [195/250], Step [240/263], Loss: 0.0222, Accuracy: 99.22%\n",
            "Epoch [195/250], Step [250/263], Loss: 0.0262, Accuracy: 99.38%\n",
            "Epoch [195/250], Step [260/263], Loss: 0.0187, Accuracy: 99.53%\n",
            "Epoch [196/250], Step [10/263], Loss: 0.0151, Accuracy: 99.84%\n",
            "Epoch [196/250], Step [20/263], Loss: 0.0133, Accuracy: 99.69%\n",
            "Epoch [196/250], Step [30/263], Loss: 0.0127, Accuracy: 99.84%\n",
            "Epoch [196/250], Step [40/263], Loss: 0.0271, Accuracy: 99.69%\n",
            "Epoch [196/250], Step [50/263], Loss: 0.0258, Accuracy: 99.22%\n",
            "Epoch [196/250], Step [60/263], Loss: 0.0152, Accuracy: 99.69%\n",
            "Epoch [196/250], Step [70/263], Loss: 0.0229, Accuracy: 99.38%\n",
            "Epoch [196/250], Step [80/263], Loss: 0.0131, Accuracy: 99.69%\n",
            "Epoch [196/250], Step [90/263], Loss: 0.0135, Accuracy: 99.38%\n",
            "Epoch [196/250], Step [100/263], Loss: 0.0123, Accuracy: 99.84%\n",
            "Epoch [196/250], Step [110/263], Loss: 0.0113, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [120/263], Loss: 0.0123, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [130/263], Loss: 0.0170, Accuracy: 99.53%\n",
            "Epoch [196/250], Step [140/263], Loss: 0.0090, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [150/263], Loss: 0.0139, Accuracy: 99.53%\n",
            "Epoch [196/250], Step [160/263], Loss: 0.0162, Accuracy: 99.69%\n",
            "Epoch [196/250], Step [170/263], Loss: 0.0210, Accuracy: 99.53%\n",
            "Epoch [196/250], Step [180/263], Loss: 0.0173, Accuracy: 99.84%\n",
            "Epoch [196/250], Step [190/263], Loss: 0.0127, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [200/263], Loss: 0.0131, Accuracy: 99.53%\n",
            "Epoch [196/250], Step [210/263], Loss: 0.0122, Accuracy: 99.84%\n",
            "Epoch [196/250], Step [220/263], Loss: 0.0162, Accuracy: 99.69%\n",
            "Epoch [196/250], Step [230/263], Loss: 0.0151, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [240/263], Loss: 0.0111, Accuracy: 99.84%\n",
            "Epoch [196/250], Step [250/263], Loss: 0.0116, Accuracy: 99.84%\n",
            "Epoch [196/250], Step [260/263], Loss: 0.0118, Accuracy: 99.84%\n",
            "Epoch [197/250], Step [10/263], Loss: 0.0130, Accuracy: 99.69%\n",
            "Epoch [197/250], Step [20/263], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [30/263], Loss: 0.0087, Accuracy: 99.84%\n",
            "Epoch [197/250], Step [40/263], Loss: 0.0094, Accuracy: 99.84%\n",
            "Epoch [197/250], Step [50/263], Loss: 0.0077, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [60/263], Loss: 0.0113, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [70/263], Loss: 0.0076, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [80/263], Loss: 0.0136, Accuracy: 99.69%\n",
            "Epoch [197/250], Step [90/263], Loss: 0.0256, Accuracy: 99.38%\n",
            "Epoch [197/250], Step [100/263], Loss: 0.0595, Accuracy: 97.81%\n",
            "Epoch [197/250], Step [110/263], Loss: 0.0448, Accuracy: 98.59%\n",
            "Epoch [197/250], Step [120/263], Loss: 0.0499, Accuracy: 98.12%\n",
            "Epoch [197/250], Step [130/263], Loss: 0.0360, Accuracy: 98.91%\n",
            "Epoch [197/250], Step [140/263], Loss: 0.0537, Accuracy: 98.44%\n",
            "Epoch [197/250], Step [150/263], Loss: 0.0248, Accuracy: 99.22%\n",
            "Epoch [197/250], Step [160/263], Loss: 0.0434, Accuracy: 98.12%\n",
            "Epoch [197/250], Step [170/263], Loss: 0.0458, Accuracy: 98.59%\n",
            "Epoch [197/250], Step [180/263], Loss: 0.0424, Accuracy: 98.91%\n",
            "Epoch [197/250], Step [190/263], Loss: 0.0379, Accuracy: 98.91%\n",
            "Epoch [197/250], Step [200/263], Loss: 0.0330, Accuracy: 98.75%\n",
            "Epoch [197/250], Step [210/263], Loss: 0.0461, Accuracy: 98.75%\n",
            "Epoch [197/250], Step [220/263], Loss: 0.0329, Accuracy: 99.22%\n",
            "Epoch [197/250], Step [230/263], Loss: 0.0413, Accuracy: 98.59%\n",
            "Epoch [197/250], Step [240/263], Loss: 0.0263, Accuracy: 99.53%\n",
            "Epoch [197/250], Step [250/263], Loss: 0.0247, Accuracy: 99.06%\n",
            "Epoch [197/250], Step [260/263], Loss: 0.0248, Accuracy: 98.91%\n",
            "Epoch [198/250], Step [10/263], Loss: 0.0136, Accuracy: 99.84%\n",
            "Epoch [198/250], Step [20/263], Loss: 0.0148, Accuracy: 99.53%\n",
            "Epoch [198/250], Step [30/263], Loss: 0.0107, Accuracy: 99.84%\n",
            "Epoch [198/250], Step [40/263], Loss: 0.0145, Accuracy: 99.84%\n",
            "Epoch [198/250], Step [50/263], Loss: 0.0141, Accuracy: 99.69%\n",
            "Epoch [198/250], Step [60/263], Loss: 0.0194, Accuracy: 99.53%\n",
            "Epoch [198/250], Step [70/263], Loss: 0.0141, Accuracy: 99.84%\n",
            "Epoch [198/250], Step [80/263], Loss: 0.0262, Accuracy: 99.22%\n",
            "Epoch [198/250], Step [90/263], Loss: 0.0118, Accuracy: 99.69%\n",
            "Epoch [198/250], Step [100/263], Loss: 0.0097, Accuracy: 99.69%\n",
            "Epoch [198/250], Step [110/263], Loss: 0.0139, Accuracy: 99.84%\n",
            "Epoch [198/250], Step [120/263], Loss: 0.0254, Accuracy: 99.69%\n",
            "Epoch [198/250], Step [130/263], Loss: 0.0382, Accuracy: 98.91%\n",
            "Epoch [198/250], Step [140/263], Loss: 0.0171, Accuracy: 99.53%\n",
            "Epoch [198/250], Step [150/263], Loss: 0.0395, Accuracy: 99.22%\n",
            "Epoch [198/250], Step [160/263], Loss: 0.0138, Accuracy: 99.84%\n",
            "Epoch [198/250], Step [170/263], Loss: 0.0154, Accuracy: 99.69%\n",
            "Epoch [198/250], Step [180/263], Loss: 0.0132, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [190/263], Loss: 0.0118, Accuracy: 99.69%\n",
            "Epoch [198/250], Step [200/263], Loss: 0.0123, Accuracy: 99.69%\n",
            "Epoch [198/250], Step [210/263], Loss: 0.0185, Accuracy: 99.53%\n",
            "Epoch [198/250], Step [220/263], Loss: 0.0152, Accuracy: 99.84%\n",
            "Epoch [198/250], Step [230/263], Loss: 0.0260, Accuracy: 99.38%\n",
            "Epoch [198/250], Step [240/263], Loss: 0.0185, Accuracy: 99.38%\n",
            "Epoch [198/250], Step [250/263], Loss: 0.0174, Accuracy: 99.53%\n",
            "Epoch [198/250], Step [260/263], Loss: 0.0152, Accuracy: 99.69%\n",
            "Epoch [199/250], Step [10/263], Loss: 0.0150, Accuracy: 99.53%\n",
            "Epoch [199/250], Step [20/263], Loss: 0.0089, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [30/263], Loss: 0.0211, Accuracy: 99.22%\n",
            "Epoch [199/250], Step [40/263], Loss: 0.0180, Accuracy: 99.38%\n",
            "Epoch [199/250], Step [50/263], Loss: 0.0080, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [60/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [199/250], Step [70/263], Loss: 0.0126, Accuracy: 99.69%\n",
            "Epoch [199/250], Step [80/263], Loss: 0.0094, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [90/263], Loss: 0.0081, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [100/263], Loss: 0.0137, Accuracy: 99.53%\n",
            "Epoch [199/250], Step [110/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [199/250], Step [120/263], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [130/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [199/250], Step [140/263], Loss: 0.0097, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [150/263], Loss: 0.0067, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [160/263], Loss: 0.0078, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [170/263], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [180/263], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [190/263], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [200/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [199/250], Step [210/263], Loss: 0.0054, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [220/263], Loss: 0.0139, Accuracy: 99.84%\n",
            "Epoch [199/250], Step [230/263], Loss: 0.0082, Accuracy: 99.84%\n",
            "Epoch [199/250], Step [240/263], Loss: 0.0125, Accuracy: 99.53%\n",
            "Epoch [199/250], Step [250/263], Loss: 0.0276, Accuracy: 99.53%\n",
            "Epoch [199/250], Step [260/263], Loss: 0.0578, Accuracy: 98.44%\n",
            "Epoch [200/250], Step [10/263], Loss: 0.1146, Accuracy: 97.03%\n",
            "Epoch [200/250], Step [20/263], Loss: 0.0391, Accuracy: 99.06%\n",
            "Epoch [200/250], Step [30/263], Loss: 0.0933, Accuracy: 97.19%\n",
            "Epoch [200/250], Step [40/263], Loss: 0.0417, Accuracy: 98.59%\n",
            "Epoch [200/250], Step [50/263], Loss: 0.0600, Accuracy: 98.12%\n",
            "Epoch [200/250], Step [60/263], Loss: 0.0532, Accuracy: 97.81%\n",
            "Epoch [200/250], Step [70/263], Loss: 0.1233, Accuracy: 96.72%\n",
            "Epoch [200/250], Step [80/263], Loss: 0.0758, Accuracy: 97.66%\n",
            "Epoch [200/250], Step [90/263], Loss: 0.0521, Accuracy: 98.44%\n",
            "Epoch [200/250], Step [100/263], Loss: 0.0761, Accuracy: 97.03%\n",
            "Epoch [200/250], Step [110/263], Loss: 0.0584, Accuracy: 97.66%\n",
            "Epoch [200/250], Step [120/263], Loss: 0.0656, Accuracy: 97.50%\n",
            "Epoch [200/250], Step [130/263], Loss: 0.0384, Accuracy: 98.91%\n",
            "Epoch [200/250], Step [140/263], Loss: 0.0749, Accuracy: 97.81%\n",
            "Epoch [200/250], Step [150/263], Loss: 0.0506, Accuracy: 98.28%\n",
            "Epoch [200/250], Step [160/263], Loss: 0.0698, Accuracy: 97.03%\n",
            "Epoch [200/250], Step [170/263], Loss: 0.1178, Accuracy: 96.09%\n",
            "Epoch [200/250], Step [180/263], Loss: 0.0920, Accuracy: 96.72%\n",
            "Epoch [200/250], Step [190/263], Loss: 0.0834, Accuracy: 97.03%\n",
            "Epoch [200/250], Step [200/263], Loss: 0.1084, Accuracy: 96.25%\n",
            "Epoch [200/250], Step [210/263], Loss: 0.1119, Accuracy: 96.56%\n",
            "Epoch [200/250], Step [220/263], Loss: 0.0978, Accuracy: 97.03%\n",
            "Epoch [200/250], Step [230/263], Loss: 0.0935, Accuracy: 96.88%\n",
            "Epoch [200/250], Step [240/263], Loss: 0.0905, Accuracy: 97.50%\n",
            "Epoch [200/250], Step [250/263], Loss: 0.1037, Accuracy: 97.03%\n",
            "Epoch [200/250], Step [260/263], Loss: 0.0672, Accuracy: 97.34%\n",
            "Epoch [201/250], Step [10/263], Loss: 0.0603, Accuracy: 97.81%\n",
            "Epoch [201/250], Step [20/263], Loss: 0.0770, Accuracy: 97.34%\n",
            "Epoch [201/250], Step [30/263], Loss: 0.0546, Accuracy: 98.59%\n",
            "Epoch [201/250], Step [40/263], Loss: 0.0460, Accuracy: 98.59%\n",
            "Epoch [201/250], Step [50/263], Loss: 0.0497, Accuracy: 97.97%\n",
            "Epoch [201/250], Step [60/263], Loss: 0.0544, Accuracy: 97.66%\n",
            "Epoch [201/250], Step [70/263], Loss: 0.0373, Accuracy: 98.59%\n",
            "Epoch [201/250], Step [80/263], Loss: 0.0229, Accuracy: 99.69%\n",
            "Epoch [201/250], Step [90/263], Loss: 0.0247, Accuracy: 99.38%\n",
            "Epoch [201/250], Step [100/263], Loss: 0.0435, Accuracy: 98.28%\n",
            "Epoch [201/250], Step [110/263], Loss: 0.0338, Accuracy: 98.75%\n",
            "Epoch [201/250], Step [120/263], Loss: 0.0384, Accuracy: 98.44%\n",
            "Epoch [201/250], Step [130/263], Loss: 0.0595, Accuracy: 98.12%\n",
            "Epoch [201/250], Step [140/263], Loss: 0.0309, Accuracy: 99.06%\n",
            "Epoch [201/250], Step [150/263], Loss: 0.0282, Accuracy: 99.06%\n",
            "Epoch [201/250], Step [160/263], Loss: 0.0304, Accuracy: 98.91%\n",
            "Epoch [201/250], Step [170/263], Loss: 0.0273, Accuracy: 99.22%\n",
            "Epoch [201/250], Step [180/263], Loss: 0.0297, Accuracy: 99.38%\n",
            "Epoch [201/250], Step [190/263], Loss: 0.0335, Accuracy: 99.06%\n",
            "Epoch [201/250], Step [200/263], Loss: 0.0315, Accuracy: 98.91%\n",
            "Epoch [201/250], Step [210/263], Loss: 0.0408, Accuracy: 99.06%\n",
            "Epoch [201/250], Step [220/263], Loss: 0.0229, Accuracy: 99.38%\n",
            "Epoch [201/250], Step [230/263], Loss: 0.0154, Accuracy: 99.84%\n",
            "Epoch [201/250], Step [240/263], Loss: 0.0172, Accuracy: 99.53%\n",
            "Epoch [201/250], Step [250/263], Loss: 0.0279, Accuracy: 99.38%\n",
            "Epoch [201/250], Step [260/263], Loss: 0.0283, Accuracy: 98.91%\n",
            "Epoch [202/250], Step [10/263], Loss: 0.0129, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [20/263], Loss: 0.0171, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [30/263], Loss: 0.0226, Accuracy: 99.53%\n",
            "Epoch [202/250], Step [40/263], Loss: 0.0140, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [50/263], Loss: 0.0145, Accuracy: 99.84%\n",
            "Epoch [202/250], Step [60/263], Loss: 0.0105, Accuracy: 99.84%\n",
            "Epoch [202/250], Step [70/263], Loss: 0.0112, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [80/263], Loss: 0.0144, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [90/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [100/263], Loss: 0.0172, Accuracy: 99.38%\n",
            "Epoch [202/250], Step [110/263], Loss: 0.0227, Accuracy: 99.38%\n",
            "Epoch [202/250], Step [120/263], Loss: 0.0149, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [130/263], Loss: 0.0279, Accuracy: 99.38%\n",
            "Epoch [202/250], Step [140/263], Loss: 0.0107, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [150/263], Loss: 0.0121, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [160/263], Loss: 0.0142, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [170/263], Loss: 0.0182, Accuracy: 99.22%\n",
            "Epoch [202/250], Step [180/263], Loss: 0.0189, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [190/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [202/250], Step [200/263], Loss: 0.0076, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [210/263], Loss: 0.0101, Accuracy: 99.84%\n",
            "Epoch [202/250], Step [220/263], Loss: 0.0117, Accuracy: 99.84%\n",
            "Epoch [202/250], Step [230/263], Loss: 0.0196, Accuracy: 99.69%\n",
            "Epoch [202/250], Step [240/263], Loss: 0.0166, Accuracy: 99.53%\n",
            "Epoch [202/250], Step [250/263], Loss: 0.0222, Accuracy: 99.53%\n",
            "Epoch [202/250], Step [260/263], Loss: 0.0165, Accuracy: 99.53%\n",
            "Epoch [203/250], Step [10/263], Loss: 0.0167, Accuracy: 99.53%\n",
            "Epoch [203/250], Step [20/263], Loss: 0.0198, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [30/263], Loss: 0.0199, Accuracy: 99.53%\n",
            "Epoch [203/250], Step [40/263], Loss: 0.0170, Accuracy: 99.53%\n",
            "Epoch [203/250], Step [50/263], Loss: 0.0177, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [60/263], Loss: 0.0190, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [70/263], Loss: 0.0135, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [80/263], Loss: 0.0202, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [90/263], Loss: 0.0367, Accuracy: 99.06%\n",
            "Epoch [203/250], Step [100/263], Loss: 0.0500, Accuracy: 98.44%\n",
            "Epoch [203/250], Step [110/263], Loss: 0.0619, Accuracy: 98.59%\n",
            "Epoch [203/250], Step [120/263], Loss: 0.0182, Accuracy: 99.53%\n",
            "Epoch [203/250], Step [130/263], Loss: 0.0246, Accuracy: 99.06%\n",
            "Epoch [203/250], Step [140/263], Loss: 0.0162, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [150/263], Loss: 0.0236, Accuracy: 99.53%\n",
            "Epoch [203/250], Step [160/263], Loss: 0.0211, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [170/263], Loss: 0.0239, Accuracy: 99.22%\n",
            "Epoch [203/250], Step [180/263], Loss: 0.0175, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [190/263], Loss: 0.0127, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [200/263], Loss: 0.0225, Accuracy: 99.53%\n",
            "Epoch [203/250], Step [210/263], Loss: 0.0127, Accuracy: 99.84%\n",
            "Epoch [203/250], Step [220/263], Loss: 0.0121, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [230/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [203/250], Step [240/263], Loss: 0.0081, Accuracy: 100.00%\n",
            "Epoch [203/250], Step [250/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [203/250], Step [260/263], Loss: 0.0134, Accuracy: 99.69%\n",
            "Epoch [204/250], Step [10/263], Loss: 0.0084, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [20/263], Loss: 0.0081, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [30/263], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [40/263], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [50/263], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [60/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [70/263], Loss: 0.0060, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [80/263], Loss: 0.0055, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [90/263], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [100/263], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [110/263], Loss: 0.0138, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [120/263], Loss: 0.1339, Accuracy: 96.41%\n",
            "Epoch [204/250], Step [130/263], Loss: 0.0483, Accuracy: 98.75%\n",
            "Epoch [204/250], Step [140/263], Loss: 0.0292, Accuracy: 99.38%\n",
            "Epoch [204/250], Step [150/263], Loss: 0.0521, Accuracy: 98.44%\n",
            "Epoch [204/250], Step [160/263], Loss: 0.0300, Accuracy: 99.06%\n",
            "Epoch [204/250], Step [170/263], Loss: 0.0603, Accuracy: 98.44%\n",
            "Epoch [204/250], Step [180/263], Loss: 0.0471, Accuracy: 98.91%\n",
            "Epoch [204/250], Step [190/263], Loss: 0.0310, Accuracy: 98.44%\n",
            "Epoch [204/250], Step [200/263], Loss: 0.0355, Accuracy: 98.91%\n",
            "Epoch [204/250], Step [210/263], Loss: 0.0411, Accuracy: 98.75%\n",
            "Epoch [204/250], Step [220/263], Loss: 0.0513, Accuracy: 98.28%\n",
            "Epoch [204/250], Step [230/263], Loss: 0.0313, Accuracy: 99.06%\n",
            "Epoch [204/250], Step [240/263], Loss: 0.0536, Accuracy: 98.28%\n",
            "Epoch [204/250], Step [250/263], Loss: 0.0409, Accuracy: 98.12%\n",
            "Epoch [204/250], Step [260/263], Loss: 0.0444, Accuracy: 98.12%\n",
            "Epoch [205/250], Step [10/263], Loss: 0.0483, Accuracy: 98.44%\n",
            "Epoch [205/250], Step [20/263], Loss: 0.0504, Accuracy: 98.12%\n",
            "Epoch [205/250], Step [30/263], Loss: 0.1255, Accuracy: 96.09%\n",
            "Epoch [205/250], Step [40/263], Loss: 0.0703, Accuracy: 97.50%\n",
            "Epoch [205/250], Step [50/263], Loss: 0.0437, Accuracy: 98.75%\n",
            "Epoch [205/250], Step [60/263], Loss: 0.0392, Accuracy: 98.91%\n",
            "Epoch [205/250], Step [70/263], Loss: 0.0483, Accuracy: 97.81%\n",
            "Epoch [205/250], Step [80/263], Loss: 0.0365, Accuracy: 98.75%\n",
            "Epoch [205/250], Step [90/263], Loss: 0.0544, Accuracy: 98.28%\n",
            "Epoch [205/250], Step [100/263], Loss: 0.0253, Accuracy: 99.06%\n",
            "Epoch [205/250], Step [110/263], Loss: 0.0251, Accuracy: 99.22%\n",
            "Epoch [205/250], Step [120/263], Loss: 0.0417, Accuracy: 98.44%\n",
            "Epoch [205/250], Step [130/263], Loss: 0.0192, Accuracy: 99.06%\n",
            "Epoch [205/250], Step [140/263], Loss: 0.0242, Accuracy: 99.06%\n",
            "Epoch [205/250], Step [150/263], Loss: 0.0226, Accuracy: 99.53%\n",
            "Epoch [205/250], Step [160/263], Loss: 0.0316, Accuracy: 98.91%\n",
            "Epoch [205/250], Step [170/263], Loss: 0.0248, Accuracy: 99.38%\n",
            "Epoch [205/250], Step [180/263], Loss: 0.0284, Accuracy: 99.22%\n",
            "Epoch [205/250], Step [190/263], Loss: 0.0290, Accuracy: 99.06%\n",
            "Epoch [205/250], Step [200/263], Loss: 0.0176, Accuracy: 99.53%\n",
            "Epoch [205/250], Step [210/263], Loss: 0.0221, Accuracy: 99.22%\n",
            "Epoch [205/250], Step [220/263], Loss: 0.0208, Accuracy: 99.53%\n",
            "Epoch [205/250], Step [230/263], Loss: 0.0275, Accuracy: 99.38%\n",
            "Epoch [205/250], Step [240/263], Loss: 0.0284, Accuracy: 98.91%\n",
            "Epoch [205/250], Step [250/263], Loss: 0.0262, Accuracy: 99.22%\n",
            "Epoch [205/250], Step [260/263], Loss: 0.0138, Accuracy: 99.53%\n",
            "Epoch [206/250], Step [10/263], Loss: 0.0309, Accuracy: 98.75%\n",
            "Epoch [206/250], Step [20/263], Loss: 0.0139, Accuracy: 99.69%\n",
            "Epoch [206/250], Step [30/263], Loss: 0.0161, Accuracy: 99.69%\n",
            "Epoch [206/250], Step [40/263], Loss: 0.0109, Accuracy: 99.84%\n",
            "Epoch [206/250], Step [50/263], Loss: 0.0146, Accuracy: 99.53%\n",
            "Epoch [206/250], Step [60/263], Loss: 0.0369, Accuracy: 99.06%\n",
            "Epoch [206/250], Step [70/263], Loss: 0.0161, Accuracy: 99.53%\n",
            "Epoch [206/250], Step [80/263], Loss: 0.0205, Accuracy: 99.38%\n",
            "Epoch [206/250], Step [90/263], Loss: 0.0143, Accuracy: 99.84%\n",
            "Epoch [206/250], Step [100/263], Loss: 0.0185, Accuracy: 99.69%\n",
            "Epoch [206/250], Step [110/263], Loss: 0.0145, Accuracy: 99.69%\n",
            "Epoch [206/250], Step [120/263], Loss: 0.0345, Accuracy: 99.38%\n",
            "Epoch [206/250], Step [130/263], Loss: 0.0225, Accuracy: 99.53%\n",
            "Epoch [206/250], Step [140/263], Loss: 0.0369, Accuracy: 98.91%\n",
            "Epoch [206/250], Step [150/263], Loss: 0.0355, Accuracy: 98.75%\n",
            "Epoch [206/250], Step [160/263], Loss: 0.0511, Accuracy: 98.75%\n",
            "Epoch [206/250], Step [170/263], Loss: 0.0423, Accuracy: 99.22%\n",
            "Epoch [206/250], Step [180/263], Loss: 0.0453, Accuracy: 98.44%\n",
            "Epoch [206/250], Step [190/263], Loss: 0.0227, Accuracy: 99.53%\n",
            "Epoch [206/250], Step [200/263], Loss: 0.0234, Accuracy: 99.53%\n",
            "Epoch [206/250], Step [210/263], Loss: 0.0255, Accuracy: 99.38%\n",
            "Epoch [206/250], Step [220/263], Loss: 0.0469, Accuracy: 98.44%\n",
            "Epoch [206/250], Step [230/263], Loss: 0.0439, Accuracy: 98.91%\n",
            "Epoch [206/250], Step [240/263], Loss: 0.0364, Accuracy: 98.59%\n",
            "Epoch [206/250], Step [250/263], Loss: 0.0281, Accuracy: 99.06%\n",
            "Epoch [206/250], Step [260/263], Loss: 0.0232, Accuracy: 99.06%\n",
            "Epoch [207/250], Step [10/263], Loss: 0.0113, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [20/263], Loss: 0.0126, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [30/263], Loss: 0.0126, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [40/263], Loss: 0.0080, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [50/263], Loss: 0.0076, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [60/263], Loss: 0.0112, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [70/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [80/263], Loss: 0.0262, Accuracy: 99.22%\n",
            "Epoch [207/250], Step [90/263], Loss: 0.0088, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [100/263], Loss: 0.0118, Accuracy: 99.53%\n",
            "Epoch [207/250], Step [110/263], Loss: 0.0172, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [120/263], Loss: 0.0108, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [130/263], Loss: 0.0085, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [140/263], Loss: 0.0115, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [150/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [160/263], Loss: 0.0083, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [170/263], Loss: 0.0105, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [180/263], Loss: 0.0131, Accuracy: 99.53%\n",
            "Epoch [207/250], Step [190/263], Loss: 0.0124, Accuracy: 99.84%\n",
            "Epoch [207/250], Step [200/263], Loss: 0.0098, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [210/263], Loss: 0.0152, Accuracy: 99.22%\n",
            "Epoch [207/250], Step [220/263], Loss: 0.0252, Accuracy: 99.38%\n",
            "Epoch [207/250], Step [230/263], Loss: 0.0063, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [240/263], Loss: 0.0212, Accuracy: 99.38%\n",
            "Epoch [207/250], Step [250/263], Loss: 0.0137, Accuracy: 99.69%\n",
            "Epoch [207/250], Step [260/263], Loss: 0.0172, Accuracy: 99.53%\n",
            "Epoch [208/250], Step [10/263], Loss: 0.0273, Accuracy: 99.53%\n",
            "Epoch [208/250], Step [20/263], Loss: 0.0091, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [30/263], Loss: 0.0104, Accuracy: 99.84%\n",
            "Epoch [208/250], Step [40/263], Loss: 0.0086, Accuracy: 99.84%\n",
            "Epoch [208/250], Step [50/263], Loss: 0.0108, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [60/263], Loss: 0.0140, Accuracy: 99.38%\n",
            "Epoch [208/250], Step [70/263], Loss: 0.0111, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [80/263], Loss: 0.0267, Accuracy: 99.06%\n",
            "Epoch [208/250], Step [90/263], Loss: 0.0242, Accuracy: 99.38%\n",
            "Epoch [208/250], Step [100/263], Loss: 0.0193, Accuracy: 99.53%\n",
            "Epoch [208/250], Step [110/263], Loss: 0.0139, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [120/263], Loss: 0.0085, Accuracy: 99.84%\n",
            "Epoch [208/250], Step [130/263], Loss: 0.0082, Accuracy: 99.84%\n",
            "Epoch [208/250], Step [140/263], Loss: 0.0090, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [150/263], Loss: 0.0119, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [160/263], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [170/263], Loss: 0.0142, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [180/263], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [190/263], Loss: 0.0064, Accuracy: 99.84%\n",
            "Epoch [208/250], Step [200/263], Loss: 0.0113, Accuracy: 99.84%\n",
            "Epoch [208/250], Step [210/263], Loss: 0.0161, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [220/263], Loss: 0.0189, Accuracy: 99.38%\n",
            "Epoch [208/250], Step [230/263], Loss: 0.0113, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [240/263], Loss: 0.0127, Accuracy: 99.84%\n",
            "Epoch [208/250], Step [250/263], Loss: 0.0128, Accuracy: 99.69%\n",
            "Epoch [208/250], Step [260/263], Loss: 0.0145, Accuracy: 99.38%\n",
            "Epoch [209/250], Step [10/263], Loss: 0.0047, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [20/263], Loss: 0.0087, Accuracy: 99.69%\n",
            "Epoch [209/250], Step [30/263], Loss: 0.0076, Accuracy: 99.69%\n",
            "Epoch [209/250], Step [40/263], Loss: 0.0067, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [50/263], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [60/263], Loss: 0.0064, Accuracy: 99.84%\n",
            "Epoch [209/250], Step [70/263], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [80/263], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [90/263], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [100/263], Loss: 0.0082, Accuracy: 99.69%\n",
            "Epoch [209/250], Step [110/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [120/263], Loss: 0.0079, Accuracy: 99.84%\n",
            "Epoch [209/250], Step [130/263], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [140/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [150/263], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [160/263], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [170/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [209/250], Step [180/263], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [190/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [200/263], Loss: 0.0047, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [210/263], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [220/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [230/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [240/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [250/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [260/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [10/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [20/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [30/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [40/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [210/250], Step [50/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [60/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [70/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [80/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [90/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [100/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [110/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [120/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [130/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [140/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [210/250], Step [150/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [160/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [170/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [180/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [190/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [200/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [210/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [220/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [230/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [240/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [250/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [260/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [10/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [20/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [30/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [40/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [50/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [60/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [70/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [80/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [90/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [100/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [110/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [120/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [130/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [140/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [150/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [160/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [170/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [180/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [190/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [211/250], Step [200/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [210/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [220/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [230/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [240/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [250/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [260/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [10/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [20/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [30/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [40/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [50/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [60/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [70/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [80/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [90/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [100/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [110/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [120/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [130/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [140/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [150/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [160/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [170/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [180/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [190/263], Loss: 0.0031, Accuracy: 99.84%\n",
            "Epoch [212/250], Step [200/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [210/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [220/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [230/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [240/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [250/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [260/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [10/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [20/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [30/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [40/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [50/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [60/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [70/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [80/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [90/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [100/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [110/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [120/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [130/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [140/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [150/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [160/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [170/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [180/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [190/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [200/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [210/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [220/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [230/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [240/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [250/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [260/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [10/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [20/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [30/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [40/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [50/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [60/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [70/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [80/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [90/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [100/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [110/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [120/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [130/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [140/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [150/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [160/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [170/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [180/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [190/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [200/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [210/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [220/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [230/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [240/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [250/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [260/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [10/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [20/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [30/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [40/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [50/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [60/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [70/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [80/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [90/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [100/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [110/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [120/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [130/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [140/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [150/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [160/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [170/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [180/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [190/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [200/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [210/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [220/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [230/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [240/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [250/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [260/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [10/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [20/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [30/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [40/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [50/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [60/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [70/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [80/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [90/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [100/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [110/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [120/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [130/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [140/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [150/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [160/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [170/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [180/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [190/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [200/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [210/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [220/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [230/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [240/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [250/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [260/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [10/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [20/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [30/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [40/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [50/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [60/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [70/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [80/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [90/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [100/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [110/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [120/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [130/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [140/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [150/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [160/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [170/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [180/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [190/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [200/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [210/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [220/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [230/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [240/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [250/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [260/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [10/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [20/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [30/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [40/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [50/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [60/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [70/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [80/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [90/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [100/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [110/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [120/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [130/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [140/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [150/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [160/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [180/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [190/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [200/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [210/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [220/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [230/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [240/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [250/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [260/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [10/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [20/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [30/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [40/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [50/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [60/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [70/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [80/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [90/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [100/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [110/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [120/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [130/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [140/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [150/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [160/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [180/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [190/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [200/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [210/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [220/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [230/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [240/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [250/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [260/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [10/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [20/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [30/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [40/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [50/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [60/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [70/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [80/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [90/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [100/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [110/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [120/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [130/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [140/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [150/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [160/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [180/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [200/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [210/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [220/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [230/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [240/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [250/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [260/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [10/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [20/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [40/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [50/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [60/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [70/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [80/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [90/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [100/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [110/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [120/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [130/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [140/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [150/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [160/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [190/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [200/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [210/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [220/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [230/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [240/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [260/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [10/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [20/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [40/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [50/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [60/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [70/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [80/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [90/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [100/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [110/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [120/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [130/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [140/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [150/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [160/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [200/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [210/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [220/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [230/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [240/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [260/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [10/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [20/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [40/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [60/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [70/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [80/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [90/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [100/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [110/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [120/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [130/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [140/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [150/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [160/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [200/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [260/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [10/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [20/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [60/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [70/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [90/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [100/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [120/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [140/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [150/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [160/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [170/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [190/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [200/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [210/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [220/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [230/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [10/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [20/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [60/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [70/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [100/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [120/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [140/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [150/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [160/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [170/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [200/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [250/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [260/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [60/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [120/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [160/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [170/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [250/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [226/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [70/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [140/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [170/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [227/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [170/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [110/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [120/263], Loss: 0.0355, Accuracy: 98.75%\n",
            "Epoch [230/250], Step [130/263], Loss: 0.7393, Accuracy: 85.31%\n",
            "Epoch [230/250], Step [140/263], Loss: 0.7405, Accuracy: 84.84%\n",
            "Epoch [230/250], Step [150/263], Loss: 1.4984, Accuracy: 72.66%\n",
            "Epoch [230/250], Step [160/263], Loss: 1.7319, Accuracy: 63.44%\n",
            "Epoch [230/250], Step [170/263], Loss: 1.2113, Accuracy: 69.84%\n",
            "Epoch [230/250], Step [180/263], Loss: 1.2606, Accuracy: 64.22%\n",
            "Epoch [230/250], Step [190/263], Loss: 0.8520, Accuracy: 72.66%\n",
            "Epoch [230/250], Step [200/263], Loss: 1.0000, Accuracy: 71.25%\n",
            "Epoch [230/250], Step [210/263], Loss: 0.9020, Accuracy: 70.94%\n",
            "Epoch [230/250], Step [220/263], Loss: 0.7268, Accuracy: 76.88%\n",
            "Epoch [230/250], Step [230/263], Loss: 0.7363, Accuracy: 79.22%\n",
            "Epoch [230/250], Step [240/263], Loss: 0.6480, Accuracy: 78.44%\n",
            "Epoch [230/250], Step [250/263], Loss: 0.5800, Accuracy: 81.09%\n",
            "Epoch [230/250], Step [260/263], Loss: 0.5712, Accuracy: 81.56%\n",
            "Epoch [231/250], Step [10/263], Loss: 0.4596, Accuracy: 84.22%\n",
            "Epoch [231/250], Step [20/263], Loss: 0.4892, Accuracy: 85.00%\n",
            "Epoch [231/250], Step [30/263], Loss: 0.3998, Accuracy: 86.41%\n",
            "Epoch [231/250], Step [40/263], Loss: 0.5165, Accuracy: 83.75%\n",
            "Epoch [231/250], Step [50/263], Loss: 0.3961, Accuracy: 85.16%\n",
            "Epoch [231/250], Step [60/263], Loss: 0.4929, Accuracy: 84.38%\n",
            "Epoch [231/250], Step [70/263], Loss: 0.3216, Accuracy: 87.50%\n",
            "Epoch [231/250], Step [80/263], Loss: 0.3647, Accuracy: 88.12%\n",
            "Epoch [231/250], Step [90/263], Loss: 0.3692, Accuracy: 86.56%\n",
            "Epoch [231/250], Step [100/263], Loss: 0.3721, Accuracy: 85.78%\n",
            "Epoch [231/250], Step [110/263], Loss: 0.3028, Accuracy: 89.22%\n",
            "Epoch [231/250], Step [120/263], Loss: 0.3386, Accuracy: 87.97%\n",
            "Epoch [231/250], Step [130/263], Loss: 0.3555, Accuracy: 87.03%\n",
            "Epoch [231/250], Step [140/263], Loss: 0.3879, Accuracy: 85.16%\n",
            "Epoch [231/250], Step [150/263], Loss: 0.3229, Accuracy: 87.19%\n",
            "Epoch [231/250], Step [160/263], Loss: 0.3348, Accuracy: 86.72%\n",
            "Epoch [231/250], Step [170/263], Loss: 0.2780, Accuracy: 90.00%\n",
            "Epoch [231/250], Step [180/263], Loss: 0.2997, Accuracy: 88.44%\n",
            "Epoch [231/250], Step [190/263], Loss: 0.2991, Accuracy: 90.16%\n",
            "Epoch [231/250], Step [200/263], Loss: 0.3128, Accuracy: 87.81%\n",
            "Epoch [231/250], Step [210/263], Loss: 0.3055, Accuracy: 88.44%\n",
            "Epoch [231/250], Step [220/263], Loss: 0.3656, Accuracy: 86.88%\n",
            "Epoch [231/250], Step [230/263], Loss: 0.3873, Accuracy: 86.56%\n",
            "Epoch [231/250], Step [240/263], Loss: 0.2276, Accuracy: 90.62%\n",
            "Epoch [231/250], Step [250/263], Loss: 0.2968, Accuracy: 88.91%\n",
            "Epoch [231/250], Step [260/263], Loss: 0.3177, Accuracy: 87.81%\n",
            "Epoch [232/250], Step [10/263], Loss: 0.2124, Accuracy: 92.19%\n",
            "Epoch [232/250], Step [20/263], Loss: 0.2344, Accuracy: 91.56%\n",
            "Epoch [232/250], Step [30/263], Loss: 0.2186, Accuracy: 89.53%\n",
            "Epoch [232/250], Step [40/263], Loss: 0.1980, Accuracy: 92.19%\n",
            "Epoch [232/250], Step [50/263], Loss: 0.2338, Accuracy: 90.94%\n",
            "Epoch [232/250], Step [60/263], Loss: 0.2145, Accuracy: 91.88%\n",
            "Epoch [232/250], Step [70/263], Loss: 0.2393, Accuracy: 91.25%\n",
            "Epoch [232/250], Step [80/263], Loss: 0.2567, Accuracy: 89.69%\n",
            "Epoch [232/250], Step [90/263], Loss: 0.2729, Accuracy: 89.06%\n",
            "Epoch [232/250], Step [100/263], Loss: 0.2030, Accuracy: 91.09%\n",
            "Epoch [232/250], Step [110/263], Loss: 0.2561, Accuracy: 91.88%\n",
            "Epoch [232/250], Step [120/263], Loss: 0.2227, Accuracy: 91.56%\n",
            "Epoch [232/250], Step [130/263], Loss: 0.2024, Accuracy: 92.03%\n",
            "Epoch [232/250], Step [140/263], Loss: 0.2137, Accuracy: 90.78%\n",
            "Epoch [232/250], Step [150/263], Loss: 0.2232, Accuracy: 91.09%\n",
            "Epoch [232/250], Step [160/263], Loss: 0.2122, Accuracy: 92.19%\n",
            "Epoch [232/250], Step [170/263], Loss: 0.2496, Accuracy: 90.47%\n",
            "Epoch [232/250], Step [180/263], Loss: 0.2183, Accuracy: 91.41%\n",
            "Epoch [232/250], Step [190/263], Loss: 0.2175, Accuracy: 92.03%\n",
            "Epoch [232/250], Step [200/263], Loss: 0.2264, Accuracy: 91.41%\n",
            "Epoch [232/250], Step [210/263], Loss: 0.1834, Accuracy: 91.88%\n",
            "Epoch [232/250], Step [220/263], Loss: 0.1935, Accuracy: 91.88%\n",
            "Epoch [232/250], Step [230/263], Loss: 0.2066, Accuracy: 90.94%\n",
            "Epoch [232/250], Step [240/263], Loss: 0.1583, Accuracy: 93.12%\n",
            "Epoch [232/250], Step [250/263], Loss: 0.1885, Accuracy: 93.75%\n",
            "Epoch [232/250], Step [260/263], Loss: 0.1915, Accuracy: 92.34%\n",
            "Epoch [233/250], Step [10/263], Loss: 0.1776, Accuracy: 92.34%\n",
            "Epoch [233/250], Step [20/263], Loss: 0.1516, Accuracy: 93.59%\n",
            "Epoch [233/250], Step [30/263], Loss: 0.1684, Accuracy: 93.91%\n",
            "Epoch [233/250], Step [40/263], Loss: 0.1329, Accuracy: 94.84%\n",
            "Epoch [233/250], Step [50/263], Loss: 0.1729, Accuracy: 92.50%\n",
            "Epoch [233/250], Step [60/263], Loss: 0.1519, Accuracy: 94.84%\n",
            "Epoch [233/250], Step [70/263], Loss: 0.1174, Accuracy: 94.22%\n",
            "Epoch [233/250], Step [80/263], Loss: 0.1496, Accuracy: 94.38%\n",
            "Epoch [233/250], Step [90/263], Loss: 0.1606, Accuracy: 93.91%\n",
            "Epoch [233/250], Step [100/263], Loss: 0.1386, Accuracy: 95.00%\n",
            "Epoch [233/250], Step [110/263], Loss: 0.1309, Accuracy: 94.22%\n",
            "Epoch [233/250], Step [120/263], Loss: 0.1170, Accuracy: 95.62%\n",
            "Epoch [233/250], Step [130/263], Loss: 0.1761, Accuracy: 93.59%\n",
            "Epoch [233/250], Step [140/263], Loss: 0.1153, Accuracy: 95.47%\n",
            "Epoch [233/250], Step [150/263], Loss: 0.1577, Accuracy: 93.75%\n",
            "Epoch [233/250], Step [160/263], Loss: 0.1182, Accuracy: 95.78%\n",
            "Epoch [233/250], Step [170/263], Loss: 0.1596, Accuracy: 92.97%\n",
            "Epoch [233/250], Step [180/263], Loss: 0.1684, Accuracy: 93.75%\n",
            "Epoch [233/250], Step [190/263], Loss: 0.1691, Accuracy: 93.44%\n",
            "Epoch [233/250], Step [200/263], Loss: 0.1395, Accuracy: 94.06%\n",
            "Epoch [233/250], Step [210/263], Loss: 0.1541, Accuracy: 94.84%\n",
            "Epoch [233/250], Step [220/263], Loss: 0.1251, Accuracy: 95.31%\n",
            "Epoch [233/250], Step [230/263], Loss: 0.1390, Accuracy: 94.06%\n",
            "Epoch [233/250], Step [240/263], Loss: 0.1427, Accuracy: 94.38%\n",
            "Epoch [233/250], Step [250/263], Loss: 0.1475, Accuracy: 94.38%\n",
            "Epoch [233/250], Step [260/263], Loss: 0.1546, Accuracy: 93.59%\n",
            "Epoch [234/250], Step [10/263], Loss: 0.0999, Accuracy: 96.41%\n",
            "Epoch [234/250], Step [20/263], Loss: 0.1021, Accuracy: 96.88%\n",
            "Epoch [234/250], Step [30/263], Loss: 0.1188, Accuracy: 95.16%\n",
            "Epoch [234/250], Step [40/263], Loss: 0.0812, Accuracy: 96.88%\n",
            "Epoch [234/250], Step [50/263], Loss: 0.1298, Accuracy: 95.31%\n",
            "Epoch [234/250], Step [60/263], Loss: 0.1110, Accuracy: 95.47%\n",
            "Epoch [234/250], Step [70/263], Loss: 0.1012, Accuracy: 95.94%\n",
            "Epoch [234/250], Step [80/263], Loss: 0.1013, Accuracy: 96.09%\n",
            "Epoch [234/250], Step [90/263], Loss: 0.0952, Accuracy: 96.09%\n",
            "Epoch [234/250], Step [100/263], Loss: 0.0904, Accuracy: 96.09%\n",
            "Epoch [234/250], Step [110/263], Loss: 0.0883, Accuracy: 96.88%\n",
            "Epoch [234/250], Step [120/263], Loss: 0.1048, Accuracy: 96.09%\n",
            "Epoch [234/250], Step [130/263], Loss: 0.0856, Accuracy: 96.09%\n",
            "Epoch [234/250], Step [140/263], Loss: 0.1174, Accuracy: 96.09%\n",
            "Epoch [234/250], Step [150/263], Loss: 0.1155, Accuracy: 95.47%\n",
            "Epoch [234/250], Step [160/263], Loss: 0.1199, Accuracy: 95.62%\n",
            "Epoch [234/250], Step [170/263], Loss: 0.0901, Accuracy: 97.50%\n",
            "Epoch [234/250], Step [180/263], Loss: 0.0905, Accuracy: 96.88%\n",
            "Epoch [234/250], Step [190/263], Loss: 0.0805, Accuracy: 97.34%\n",
            "Epoch [234/250], Step [200/263], Loss: 0.0878, Accuracy: 97.50%\n",
            "Epoch [234/250], Step [210/263], Loss: 0.1202, Accuracy: 96.41%\n",
            "Epoch [234/250], Step [220/263], Loss: 0.1127, Accuracy: 95.31%\n",
            "Epoch [234/250], Step [230/263], Loss: 0.1216, Accuracy: 95.00%\n",
            "Epoch [234/250], Step [240/263], Loss: 0.0870, Accuracy: 96.56%\n",
            "Epoch [234/250], Step [250/263], Loss: 0.0877, Accuracy: 96.56%\n",
            "Epoch [234/250], Step [260/263], Loss: 0.1195, Accuracy: 95.62%\n",
            "Epoch [235/250], Step [10/263], Loss: 0.0485, Accuracy: 98.59%\n",
            "Epoch [235/250], Step [20/263], Loss: 0.0581, Accuracy: 97.66%\n",
            "Epoch [235/250], Step [30/263], Loss: 0.0875, Accuracy: 97.19%\n",
            "Epoch [235/250], Step [40/263], Loss: 0.0555, Accuracy: 97.97%\n",
            "Epoch [235/250], Step [50/263], Loss: 0.0636, Accuracy: 97.34%\n",
            "Epoch [235/250], Step [60/263], Loss: 0.0551, Accuracy: 98.44%\n",
            "Epoch [235/250], Step [70/263], Loss: 0.0520, Accuracy: 98.12%\n",
            "Epoch [235/250], Step [80/263], Loss: 0.0504, Accuracy: 97.81%\n",
            "Epoch [235/250], Step [90/263], Loss: 0.0582, Accuracy: 98.12%\n",
            "Epoch [235/250], Step [100/263], Loss: 0.0580, Accuracy: 98.12%\n",
            "Epoch [235/250], Step [110/263], Loss: 0.0620, Accuracy: 98.12%\n",
            "Epoch [235/250], Step [120/263], Loss: 0.0522, Accuracy: 98.75%\n",
            "Epoch [235/250], Step [130/263], Loss: 0.0495, Accuracy: 98.59%\n",
            "Epoch [235/250], Step [140/263], Loss: 0.0635, Accuracy: 97.03%\n",
            "Epoch [235/250], Step [150/263], Loss: 0.0662, Accuracy: 97.81%\n",
            "Epoch [235/250], Step [160/263], Loss: 0.0593, Accuracy: 98.28%\n",
            "Epoch [235/250], Step [170/263], Loss: 0.0795, Accuracy: 97.19%\n",
            "Epoch [235/250], Step [180/263], Loss: 0.1076, Accuracy: 95.47%\n",
            "Epoch [235/250], Step [190/263], Loss: 0.0774, Accuracy: 97.34%\n",
            "Epoch [235/250], Step [200/263], Loss: 0.0805, Accuracy: 97.19%\n",
            "Epoch [235/250], Step [210/263], Loss: 0.0638, Accuracy: 97.03%\n",
            "Epoch [235/250], Step [220/263], Loss: 0.0970, Accuracy: 96.41%\n",
            "Epoch [235/250], Step [230/263], Loss: 0.0571, Accuracy: 98.12%\n",
            "Epoch [235/250], Step [240/263], Loss: 0.0685, Accuracy: 97.97%\n",
            "Epoch [235/250], Step [250/263], Loss: 0.0764, Accuracy: 97.19%\n",
            "Epoch [235/250], Step [260/263], Loss: 0.0719, Accuracy: 97.81%\n",
            "Epoch [236/250], Step [10/263], Loss: 0.0397, Accuracy: 98.75%\n",
            "Epoch [236/250], Step [20/263], Loss: 0.0541, Accuracy: 98.12%\n",
            "Epoch [236/250], Step [30/263], Loss: 0.0488, Accuracy: 98.59%\n",
            "Epoch [236/250], Step [40/263], Loss: 0.0504, Accuracy: 98.44%\n",
            "Epoch [236/250], Step [50/263], Loss: 0.0482, Accuracy: 98.59%\n",
            "Epoch [236/250], Step [60/263], Loss: 0.0480, Accuracy: 98.28%\n",
            "Epoch [236/250], Step [70/263], Loss: 0.0638, Accuracy: 97.97%\n",
            "Epoch [236/250], Step [80/263], Loss: 0.0575, Accuracy: 98.12%\n",
            "Epoch [236/250], Step [90/263], Loss: 0.0436, Accuracy: 98.91%\n",
            "Epoch [236/250], Step [100/263], Loss: 0.0471, Accuracy: 98.12%\n",
            "Epoch [236/250], Step [110/263], Loss: 0.0392, Accuracy: 99.22%\n",
            "Epoch [236/250], Step [120/263], Loss: 0.0544, Accuracy: 98.12%\n",
            "Epoch [236/250], Step [130/263], Loss: 0.0507, Accuracy: 98.28%\n",
            "Epoch [236/250], Step [140/263], Loss: 0.0326, Accuracy: 98.91%\n",
            "Epoch [236/250], Step [150/263], Loss: 0.0415, Accuracy: 98.75%\n",
            "Epoch [236/250], Step [160/263], Loss: 0.0333, Accuracy: 99.06%\n",
            "Epoch [236/250], Step [170/263], Loss: 0.0501, Accuracy: 97.97%\n",
            "Epoch [236/250], Step [180/263], Loss: 0.0491, Accuracy: 98.12%\n",
            "Epoch [236/250], Step [190/263], Loss: 0.0509, Accuracy: 98.28%\n",
            "Epoch [236/250], Step [200/263], Loss: 0.0471, Accuracy: 99.06%\n",
            "Epoch [236/250], Step [210/263], Loss: 0.0348, Accuracy: 98.28%\n",
            "Epoch [236/250], Step [220/263], Loss: 0.0521, Accuracy: 97.97%\n",
            "Epoch [236/250], Step [230/263], Loss: 0.0448, Accuracy: 98.59%\n",
            "Epoch [236/250], Step [240/263], Loss: 0.0327, Accuracy: 99.22%\n",
            "Epoch [236/250], Step [250/263], Loss: 0.0398, Accuracy: 98.44%\n",
            "Epoch [236/250], Step [260/263], Loss: 0.0334, Accuracy: 98.59%\n",
            "Epoch [237/250], Step [10/263], Loss: 0.0214, Accuracy: 99.69%\n",
            "Epoch [237/250], Step [20/263], Loss: 0.0415, Accuracy: 99.06%\n",
            "Epoch [237/250], Step [30/263], Loss: 0.0268, Accuracy: 99.22%\n",
            "Epoch [237/250], Step [40/263], Loss: 0.0194, Accuracy: 99.53%\n",
            "Epoch [237/250], Step [50/263], Loss: 0.0271, Accuracy: 99.38%\n",
            "Epoch [237/250], Step [60/263], Loss: 0.0302, Accuracy: 98.91%\n",
            "Epoch [237/250], Step [70/263], Loss: 0.0315, Accuracy: 98.91%\n",
            "Epoch [237/250], Step [80/263], Loss: 0.0398, Accuracy: 98.59%\n",
            "Epoch [237/250], Step [90/263], Loss: 0.0306, Accuracy: 99.22%\n",
            "Epoch [237/250], Step [100/263], Loss: 0.0395, Accuracy: 98.91%\n",
            "Epoch [237/250], Step [110/263], Loss: 0.0475, Accuracy: 98.75%\n",
            "Epoch [237/250], Step [120/263], Loss: 0.0364, Accuracy: 99.06%\n",
            "Epoch [237/250], Step [130/263], Loss: 0.0512, Accuracy: 98.28%\n",
            "Epoch [237/250], Step [140/263], Loss: 0.0482, Accuracy: 98.44%\n",
            "Epoch [237/250], Step [150/263], Loss: 0.0533, Accuracy: 98.59%\n",
            "Epoch [237/250], Step [160/263], Loss: 0.0300, Accuracy: 99.22%\n",
            "Epoch [237/250], Step [170/263], Loss: 0.0506, Accuracy: 98.59%\n",
            "Epoch [237/250], Step [180/263], Loss: 0.0318, Accuracy: 98.91%\n",
            "Epoch [237/250], Step [190/263], Loss: 0.0252, Accuracy: 99.06%\n",
            "Epoch [237/250], Step [200/263], Loss: 0.0315, Accuracy: 99.22%\n",
            "Epoch [237/250], Step [210/263], Loss: 0.0297, Accuracy: 99.22%\n",
            "Epoch [237/250], Step [220/263], Loss: 0.0378, Accuracy: 98.91%\n",
            "Epoch [237/250], Step [230/263], Loss: 0.0323, Accuracy: 99.22%\n",
            "Epoch [237/250], Step [240/263], Loss: 0.0317, Accuracy: 98.91%\n",
            "Epoch [237/250], Step [250/263], Loss: 0.0441, Accuracy: 99.06%\n",
            "Epoch [237/250], Step [260/263], Loss: 0.0405, Accuracy: 99.38%\n",
            "Epoch [238/250], Step [10/263], Loss: 0.0364, Accuracy: 98.59%\n",
            "Epoch [238/250], Step [20/263], Loss: 0.0233, Accuracy: 99.22%\n",
            "Epoch [238/250], Step [30/263], Loss: 0.0424, Accuracy: 98.91%\n",
            "Epoch [238/250], Step [40/263], Loss: 0.0473, Accuracy: 98.75%\n",
            "Epoch [238/250], Step [50/263], Loss: 0.0368, Accuracy: 98.59%\n",
            "Epoch [238/250], Step [60/263], Loss: 0.0260, Accuracy: 99.22%\n",
            "Epoch [238/250], Step [70/263], Loss: 0.0308, Accuracy: 99.22%\n",
            "Epoch [238/250], Step [80/263], Loss: 0.0240, Accuracy: 99.22%\n",
            "Epoch [238/250], Step [90/263], Loss: 0.0380, Accuracy: 98.75%\n",
            "Epoch [238/250], Step [100/263], Loss: 0.0369, Accuracy: 98.44%\n",
            "Epoch [238/250], Step [110/263], Loss: 0.0410, Accuracy: 98.44%\n",
            "Epoch [238/250], Step [120/263], Loss: 0.0404, Accuracy: 98.75%\n",
            "Epoch [238/250], Step [130/263], Loss: 0.0408, Accuracy: 98.59%\n",
            "Epoch [238/250], Step [140/263], Loss: 0.0368, Accuracy: 99.22%\n",
            "Epoch [238/250], Step [150/263], Loss: 0.0320, Accuracy: 99.06%\n",
            "Epoch [238/250], Step [160/263], Loss: 0.0379, Accuracy: 98.91%\n",
            "Epoch [238/250], Step [170/263], Loss: 0.0613, Accuracy: 98.44%\n",
            "Epoch [238/250], Step [180/263], Loss: 0.0518, Accuracy: 98.28%\n",
            "Epoch [238/250], Step [190/263], Loss: 0.0454, Accuracy: 98.44%\n",
            "Epoch [238/250], Step [200/263], Loss: 0.0368, Accuracy: 99.06%\n",
            "Epoch [238/250], Step [210/263], Loss: 0.0439, Accuracy: 98.59%\n",
            "Epoch [238/250], Step [220/263], Loss: 0.0350, Accuracy: 99.06%\n",
            "Epoch [238/250], Step [230/263], Loss: 0.0235, Accuracy: 99.53%\n",
            "Epoch [238/250], Step [240/263], Loss: 0.0285, Accuracy: 99.38%\n",
            "Epoch [238/250], Step [250/263], Loss: 0.0347, Accuracy: 98.75%\n",
            "Epoch [238/250], Step [260/263], Loss: 0.0306, Accuracy: 99.06%\n",
            "Epoch [239/250], Step [10/263], Loss: 0.0158, Accuracy: 99.69%\n",
            "Epoch [239/250], Step [20/263], Loss: 0.0242, Accuracy: 99.22%\n",
            "Epoch [239/250], Step [30/263], Loss: 0.0242, Accuracy: 99.22%\n",
            "Epoch [239/250], Step [40/263], Loss: 0.0270, Accuracy: 99.22%\n",
            "Epoch [239/250], Step [50/263], Loss: 0.0189, Accuracy: 99.38%\n",
            "Epoch [239/250], Step [60/263], Loss: 0.0115, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [70/263], Loss: 0.0246, Accuracy: 99.69%\n",
            "Epoch [239/250], Step [80/263], Loss: 0.0138, Accuracy: 99.84%\n",
            "Epoch [239/250], Step [90/263], Loss: 0.0183, Accuracy: 99.38%\n",
            "Epoch [239/250], Step [100/263], Loss: 0.0199, Accuracy: 99.53%\n",
            "Epoch [239/250], Step [110/263], Loss: 0.0197, Accuracy: 99.53%\n",
            "Epoch [239/250], Step [120/263], Loss: 0.0138, Accuracy: 99.84%\n",
            "Epoch [239/250], Step [130/263], Loss: 0.0183, Accuracy: 99.53%\n",
            "Epoch [239/250], Step [140/263], Loss: 0.0190, Accuracy: 99.38%\n",
            "Epoch [239/250], Step [150/263], Loss: 0.0140, Accuracy: 99.84%\n",
            "Epoch [239/250], Step [160/263], Loss: 0.0189, Accuracy: 99.53%\n",
            "Epoch [239/250], Step [170/263], Loss: 0.0259, Accuracy: 99.38%\n",
            "Epoch [239/250], Step [180/263], Loss: 0.0240, Accuracy: 99.22%\n",
            "Epoch [239/250], Step [190/263], Loss: 0.0124, Accuracy: 99.84%\n",
            "Epoch [239/250], Step [200/263], Loss: 0.0149, Accuracy: 99.69%\n",
            "Epoch [239/250], Step [210/263], Loss: 0.0240, Accuracy: 99.53%\n",
            "Epoch [239/250], Step [220/263], Loss: 0.0139, Accuracy: 99.84%\n",
            "Epoch [239/250], Step [230/263], Loss: 0.0212, Accuracy: 99.38%\n",
            "Epoch [239/250], Step [240/263], Loss: 0.0135, Accuracy: 99.84%\n",
            "Epoch [239/250], Step [250/263], Loss: 0.0127, Accuracy: 99.84%\n",
            "Epoch [239/250], Step [260/263], Loss: 0.0145, Accuracy: 99.69%\n",
            "Epoch [240/250], Step [10/263], Loss: 0.0129, Accuracy: 99.84%\n",
            "Epoch [240/250], Step [20/263], Loss: 0.0116, Accuracy: 99.53%\n",
            "Epoch [240/250], Step [30/263], Loss: 0.0167, Accuracy: 99.69%\n",
            "Epoch [240/250], Step [40/263], Loss: 0.0235, Accuracy: 99.53%\n",
            "Epoch [240/250], Step [50/263], Loss: 0.0368, Accuracy: 99.53%\n",
            "Epoch [240/250], Step [60/263], Loss: 0.0331, Accuracy: 98.75%\n",
            "Epoch [240/250], Step [70/263], Loss: 0.0303, Accuracy: 99.06%\n",
            "Epoch [240/250], Step [80/263], Loss: 0.0370, Accuracy: 98.59%\n",
            "Epoch [240/250], Step [90/263], Loss: 0.0239, Accuracy: 99.53%\n",
            "Epoch [240/250], Step [100/263], Loss: 0.0268, Accuracy: 99.22%\n",
            "Epoch [240/250], Step [110/263], Loss: 0.0295, Accuracy: 99.38%\n",
            "Epoch [240/250], Step [120/263], Loss: 0.0204, Accuracy: 99.53%\n",
            "Epoch [240/250], Step [130/263], Loss: 0.0409, Accuracy: 98.91%\n",
            "Epoch [240/250], Step [140/263], Loss: 0.0241, Accuracy: 99.38%\n",
            "Epoch [240/250], Step [150/263], Loss: 0.0195, Accuracy: 99.69%\n",
            "Epoch [240/250], Step [160/263], Loss: 0.0341, Accuracy: 99.38%\n",
            "Epoch [240/250], Step [170/263], Loss: 0.0249, Accuracy: 99.06%\n",
            "Epoch [240/250], Step [180/263], Loss: 0.0252, Accuracy: 98.91%\n",
            "Epoch [240/250], Step [190/263], Loss: 0.0256, Accuracy: 99.22%\n",
            "Epoch [240/250], Step [200/263], Loss: 0.0175, Accuracy: 99.22%\n",
            "Epoch [240/250], Step [210/263], Loss: 0.0301, Accuracy: 98.91%\n",
            "Epoch [240/250], Step [220/263], Loss: 0.0656, Accuracy: 97.50%\n",
            "Epoch [240/250], Step [230/263], Loss: 0.0483, Accuracy: 98.75%\n",
            "Epoch [240/250], Step [240/263], Loss: 0.0335, Accuracy: 98.91%\n",
            "Epoch [240/250], Step [250/263], Loss: 0.0588, Accuracy: 97.97%\n",
            "Epoch [240/250], Step [260/263], Loss: 0.0421, Accuracy: 99.22%\n",
            "Epoch [241/250], Step [10/263], Loss: 0.0223, Accuracy: 99.22%\n",
            "Epoch [241/250], Step [20/263], Loss: 0.0202, Accuracy: 99.38%\n",
            "Epoch [241/250], Step [30/263], Loss: 0.0363, Accuracy: 99.06%\n",
            "Epoch [241/250], Step [40/263], Loss: 0.0230, Accuracy: 99.06%\n",
            "Epoch [241/250], Step [50/263], Loss: 0.0217, Accuracy: 99.69%\n",
            "Epoch [241/250], Step [60/263], Loss: 0.0359, Accuracy: 99.06%\n",
            "Epoch [241/250], Step [70/263], Loss: 0.0312, Accuracy: 98.91%\n",
            "Epoch [241/250], Step [80/263], Loss: 0.0326, Accuracy: 98.91%\n",
            "Epoch [241/250], Step [90/263], Loss: 0.0441, Accuracy: 98.75%\n",
            "Epoch [241/250], Step [100/263], Loss: 0.0347, Accuracy: 98.91%\n",
            "Epoch [241/250], Step [110/263], Loss: 0.0814, Accuracy: 97.34%\n",
            "Epoch [241/250], Step [120/263], Loss: 0.0703, Accuracy: 97.03%\n",
            "Epoch [241/250], Step [130/263], Loss: 0.0746, Accuracy: 97.66%\n",
            "Epoch [241/250], Step [140/263], Loss: 0.0675, Accuracy: 97.81%\n",
            "Epoch [241/250], Step [150/263], Loss: 0.0662, Accuracy: 97.19%\n",
            "Epoch [241/250], Step [160/263], Loss: 0.0756, Accuracy: 97.66%\n",
            "Epoch [241/250], Step [170/263], Loss: 0.0718, Accuracy: 97.81%\n",
            "Epoch [241/250], Step [180/263], Loss: 0.0662, Accuracy: 97.50%\n",
            "Epoch [241/250], Step [190/263], Loss: 0.0834, Accuracy: 96.72%\n",
            "Epoch [241/250], Step [200/263], Loss: 0.0728, Accuracy: 96.72%\n",
            "Epoch [241/250], Step [210/263], Loss: 0.0761, Accuracy: 97.34%\n",
            "Epoch [241/250], Step [220/263], Loss: 0.0545, Accuracy: 97.97%\n",
            "Epoch [241/250], Step [230/263], Loss: 0.0680, Accuracy: 97.34%\n",
            "Epoch [241/250], Step [240/263], Loss: 0.0598, Accuracy: 97.19%\n",
            "Epoch [241/250], Step [250/263], Loss: 0.0918, Accuracy: 96.09%\n",
            "Epoch [241/250], Step [260/263], Loss: 0.0961, Accuracy: 96.72%\n",
            "Epoch [242/250], Step [10/263], Loss: 0.0948, Accuracy: 97.19%\n",
            "Epoch [242/250], Step [20/263], Loss: 0.1345, Accuracy: 95.00%\n",
            "Epoch [242/250], Step [30/263], Loss: 0.0801, Accuracy: 96.25%\n",
            "Epoch [242/250], Step [40/263], Loss: 0.0870, Accuracy: 96.88%\n",
            "Epoch [242/250], Step [50/263], Loss: 0.0872, Accuracy: 96.41%\n",
            "Epoch [242/250], Step [60/263], Loss: 0.0651, Accuracy: 97.66%\n",
            "Epoch [242/250], Step [70/263], Loss: 0.0509, Accuracy: 98.59%\n",
            "Epoch [242/250], Step [80/263], Loss: 0.0664, Accuracy: 97.50%\n",
            "Epoch [242/250], Step [90/263], Loss: 0.0536, Accuracy: 97.97%\n",
            "Epoch [242/250], Step [100/263], Loss: 0.0749, Accuracy: 97.66%\n",
            "Epoch [242/250], Step [110/263], Loss: 0.0693, Accuracy: 97.19%\n",
            "Epoch [242/250], Step [120/263], Loss: 0.0576, Accuracy: 98.12%\n",
            "Epoch [242/250], Step [130/263], Loss: 0.0554, Accuracy: 97.81%\n",
            "Epoch [242/250], Step [140/263], Loss: 0.0563, Accuracy: 97.66%\n",
            "Epoch [242/250], Step [150/263], Loss: 0.0551, Accuracy: 98.12%\n",
            "Epoch [242/250], Step [160/263], Loss: 0.0439, Accuracy: 98.44%\n",
            "Epoch [242/250], Step [170/263], Loss: 0.0493, Accuracy: 98.12%\n",
            "Epoch [242/250], Step [180/263], Loss: 0.0503, Accuracy: 98.12%\n",
            "Epoch [242/250], Step [190/263], Loss: 0.0427, Accuracy: 98.91%\n",
            "Epoch [242/250], Step [200/263], Loss: 0.0483, Accuracy: 98.44%\n",
            "Epoch [242/250], Step [210/263], Loss: 0.0504, Accuracy: 98.28%\n",
            "Epoch [242/250], Step [220/263], Loss: 0.0374, Accuracy: 98.44%\n",
            "Epoch [242/250], Step [230/263], Loss: 0.0351, Accuracy: 98.12%\n",
            "Epoch [242/250], Step [240/263], Loss: 0.0304, Accuracy: 99.06%\n",
            "Epoch [242/250], Step [250/263], Loss: 0.0361, Accuracy: 99.06%\n",
            "Epoch [242/250], Step [260/263], Loss: 0.0336, Accuracy: 98.44%\n",
            "Epoch [243/250], Step [10/263], Loss: 0.0249, Accuracy: 99.22%\n",
            "Epoch [243/250], Step [20/263], Loss: 0.0194, Accuracy: 99.53%\n",
            "Epoch [243/250], Step [30/263], Loss: 0.0237, Accuracy: 99.38%\n",
            "Epoch [243/250], Step [40/263], Loss: 0.0280, Accuracy: 99.53%\n",
            "Epoch [243/250], Step [50/263], Loss: 0.0244, Accuracy: 99.53%\n",
            "Epoch [243/250], Step [60/263], Loss: 0.0290, Accuracy: 99.22%\n",
            "Epoch [243/250], Step [70/263], Loss: 0.0203, Accuracy: 99.38%\n",
            "Epoch [243/250], Step [80/263], Loss: 0.0133, Accuracy: 99.84%\n",
            "Epoch [243/250], Step [90/263], Loss: 0.0300, Accuracy: 99.69%\n",
            "Epoch [243/250], Step [100/263], Loss: 0.0164, Accuracy: 99.38%\n",
            "Epoch [243/250], Step [110/263], Loss: 0.0207, Accuracy: 99.53%\n",
            "Epoch [243/250], Step [120/263], Loss: 0.0301, Accuracy: 99.38%\n",
            "Epoch [243/250], Step [130/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [243/250], Step [140/263], Loss: 0.0139, Accuracy: 99.53%\n",
            "Epoch [243/250], Step [150/263], Loss: 0.0192, Accuracy: 99.38%\n",
            "Epoch [243/250], Step [160/263], Loss: 0.0282, Accuracy: 99.06%\n",
            "Epoch [243/250], Step [170/263], Loss: 0.0141, Accuracy: 99.84%\n",
            "Epoch [243/250], Step [180/263], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [190/263], Loss: 0.0108, Accuracy: 99.84%\n",
            "Epoch [243/250], Step [200/263], Loss: 0.0120, Accuracy: 99.69%\n",
            "Epoch [243/250], Step [210/263], Loss: 0.0107, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [220/263], Loss: 0.0111, Accuracy: 99.84%\n",
            "Epoch [243/250], Step [230/263], Loss: 0.0199, Accuracy: 99.53%\n",
            "Epoch [243/250], Step [240/263], Loss: 0.0210, Accuracy: 99.38%\n",
            "Epoch [243/250], Step [250/263], Loss: 0.0736, Accuracy: 97.66%\n",
            "Epoch [243/250], Step [260/263], Loss: 0.0317, Accuracy: 98.75%\n",
            "Epoch [244/250], Step [10/263], Loss: 0.0252, Accuracy: 98.75%\n",
            "Epoch [244/250], Step [20/263], Loss: 0.0308, Accuracy: 99.22%\n",
            "Epoch [244/250], Step [30/263], Loss: 0.0172, Accuracy: 99.38%\n",
            "Epoch [244/250], Step [40/263], Loss: 0.0150, Accuracy: 99.53%\n",
            "Epoch [244/250], Step [50/263], Loss: 0.0233, Accuracy: 99.53%\n",
            "Epoch [244/250], Step [60/263], Loss: 0.0227, Accuracy: 99.69%\n",
            "Epoch [244/250], Step [70/263], Loss: 0.0105, Accuracy: 99.84%\n",
            "Epoch [244/250], Step [80/263], Loss: 0.0130, Accuracy: 99.69%\n",
            "Epoch [244/250], Step [90/263], Loss: 0.0110, Accuracy: 99.69%\n",
            "Epoch [244/250], Step [100/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [244/250], Step [110/263], Loss: 0.0149, Accuracy: 99.69%\n",
            "Epoch [244/250], Step [120/263], Loss: 0.0213, Accuracy: 99.38%\n",
            "Epoch [244/250], Step [130/263], Loss: 0.0383, Accuracy: 98.75%\n",
            "Epoch [244/250], Step [140/263], Loss: 0.0229, Accuracy: 99.22%\n",
            "Epoch [244/250], Step [150/263], Loss: 0.0195, Accuracy: 99.22%\n",
            "Epoch [244/250], Step [160/263], Loss: 0.0174, Accuracy: 99.69%\n",
            "Epoch [244/250], Step [170/263], Loss: 0.0188, Accuracy: 99.53%\n",
            "Epoch [244/250], Step [180/263], Loss: 0.0147, Accuracy: 99.69%\n",
            "Epoch [244/250], Step [190/263], Loss: 0.0196, Accuracy: 99.22%\n",
            "Epoch [244/250], Step [200/263], Loss: 0.0202, Accuracy: 99.53%\n",
            "Epoch [244/250], Step [210/263], Loss: 0.0194, Accuracy: 99.22%\n",
            "Epoch [244/250], Step [220/263], Loss: 0.0092, Accuracy: 99.84%\n",
            "Epoch [244/250], Step [230/263], Loss: 0.0169, Accuracy: 99.38%\n",
            "Epoch [244/250], Step [240/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [244/250], Step [250/263], Loss: 0.0149, Accuracy: 99.69%\n",
            "Epoch [244/250], Step [260/263], Loss: 0.0126, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [10/263], Loss: 0.0105, Accuracy: 99.84%\n",
            "Epoch [245/250], Step [20/263], Loss: 0.0079, Accuracy: 99.84%\n",
            "Epoch [245/250], Step [30/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [40/263], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [50/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [60/263], Loss: 0.0083, Accuracy: 99.84%\n",
            "Epoch [245/250], Step [70/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [80/263], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [90/263], Loss: 0.0081, Accuracy: 99.84%\n",
            "Epoch [245/250], Step [100/263], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [110/263], Loss: 0.0121, Accuracy: 99.69%\n",
            "Epoch [245/250], Step [120/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [245/250], Step [130/263], Loss: 0.0114, Accuracy: 99.69%\n",
            "Epoch [245/250], Step [140/263], Loss: 0.0060, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [150/263], Loss: 0.0137, Accuracy: 99.69%\n",
            "Epoch [245/250], Step [160/263], Loss: 0.0096, Accuracy: 99.53%\n",
            "Epoch [245/250], Step [170/263], Loss: 0.0071, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [180/263], Loss: 0.0118, Accuracy: 99.69%\n",
            "Epoch [245/250], Step [190/263], Loss: 0.0077, Accuracy: 99.84%\n",
            "Epoch [245/250], Step [200/263], Loss: 0.0056, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [210/263], Loss: 0.0156, Accuracy: 99.53%\n",
            "Epoch [245/250], Step [220/263], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [230/263], Loss: 0.0055, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [240/263], Loss: 0.0064, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [250/263], Loss: 0.0070, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [260/263], Loss: 0.0068, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [10/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [20/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [30/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [40/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [50/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [60/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [70/263], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [80/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [90/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [100/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [110/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [120/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [130/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [140/263], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [150/263], Loss: 0.0045, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [160/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [170/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [180/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [190/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [200/263], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [210/263], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [220/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [230/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [240/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [250/263], Loss: 0.0045, Accuracy: 99.84%\n",
            "Epoch [246/250], Step [260/263], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [10/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [20/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [30/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [40/263], Loss: 0.0090, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [50/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [60/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [70/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [80/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [90/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [100/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [110/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [120/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [130/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [140/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [150/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [160/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [170/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [180/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [190/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [200/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [210/263], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [220/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [230/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [240/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [250/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [260/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [10/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [20/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [30/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [40/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [50/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [60/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [70/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [80/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [90/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [100/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [110/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [120/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [130/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [140/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [150/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [160/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [170/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [180/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [190/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [200/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [210/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [220/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [230/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [240/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [250/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [260/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [10/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [20/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [30/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [40/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [50/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [60/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [70/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [80/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [90/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [100/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [110/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [120/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [130/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [140/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [150/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [160/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [170/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [180/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [190/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [200/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [210/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [220/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [230/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [240/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [250/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [260/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [10/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [20/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [30/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [40/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [50/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [60/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [70/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [80/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [90/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [100/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [110/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [120/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [130/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [140/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [150/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [160/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [170/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [180/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [190/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [200/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [210/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [220/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [230/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [240/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [250/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [260/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Training completed for model:  CNN_LSTM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(models: List, train_loader: DataLoader, epochs: int):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss_history = {model.__class__.__name__: [] for model in models}\n",
        "    accuracy_history = {model.__class__.__name__: [] for model in models}\n",
        "\n",
        "    for model in models:\n",
        "        print(\"Training model: \", model.__class__.__name__)\n",
        "        model.train()\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            for i, (x, y) in enumerate(train_loader):\n",
        "                x = x.to(device)\n",
        "                y = y.squeeze().to(device)  # Remove the extra dimension from the target tensor\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x)\n",
        "                loss = criterion(y_pred, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Compute accuracy\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                correct_predictions += (predicted_labels == y).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    avg_loss = total_loss / 10\n",
        "                    accuracy = correct_predictions / total_samples * 100\n",
        "                    print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], '\n",
        "                          f'Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "                    total_loss = 0.0\n",
        "                    correct_predictions = 0\n",
        "                    total_samples = 0\n",
        "\n",
        "            # Store loss and accuracy values for plotting\n",
        "            loss_history[model.__class__.__name__].append(avg_loss)\n",
        "            accuracy_history[model.__class__.__name__].append(accuracy)\n",
        "\n",
        "        print(\"Training completed for model: \", model.__class__.__name__)\n",
        "\n",
        "    # Plot loss and accuracy for each model\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for model in models:\n",
        "        plt.subplot(1, 2, 1)  # Loss plot\n",
        "        plt.plot(range(1, epochs + 1), loss_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)  # Accuracy plot\n",
        "        plt.plot(range(1, epochs + 1), accuracy_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# train\n",
        "models = [cnn_lstm]#, cnn_lstm_parallel]\n",
        "num_epochs = 250\n",
        "train(models, train_loader, epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N90oNV-ZKUU9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI6EWl_yKUEJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJNKlTC4rgX9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W76U_hC11HrF",
        "outputId": "156a0e97-9321-4758-ab27-b845f48cbb6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [58/250], Step [200/263], Loss: 0.0492, Accuracy: 97.97%\n",
            "Epoch [58/250], Step [210/263], Loss: 0.0348, Accuracy: 98.75%\n",
            "Epoch [58/250], Step [220/263], Loss: 0.0553, Accuracy: 98.28%\n",
            "Epoch [58/250], Step [230/263], Loss: 0.0438, Accuracy: 98.12%\n",
            "Epoch [58/250], Step [240/263], Loss: 0.0528, Accuracy: 98.28%\n",
            "Epoch [58/250], Step [250/263], Loss: 0.0449, Accuracy: 98.28%\n",
            "Epoch [58/250], Step [260/263], Loss: 0.0299, Accuracy: 99.06%\n",
            "Epoch [59/250], Step [10/263], Loss: 0.0325, Accuracy: 99.22%\n",
            "Epoch [59/250], Step [20/263], Loss: 0.0350, Accuracy: 99.53%\n",
            "Epoch [59/250], Step [30/263], Loss: 0.0341, Accuracy: 99.06%\n",
            "Epoch [59/250], Step [40/263], Loss: 0.0323, Accuracy: 98.59%\n",
            "Epoch [59/250], Step [50/263], Loss: 0.0243, Accuracy: 99.38%\n",
            "Epoch [59/250], Step [60/263], Loss: 0.0325, Accuracy: 99.06%\n",
            "Epoch [59/250], Step [70/263], Loss: 0.0396, Accuracy: 98.44%\n",
            "Epoch [59/250], Step [80/263], Loss: 0.0341, Accuracy: 99.06%\n",
            "Epoch [59/250], Step [90/263], Loss: 0.0315, Accuracy: 98.91%\n",
            "Epoch [59/250], Step [100/263], Loss: 0.0354, Accuracy: 98.28%\n",
            "Epoch [59/250], Step [110/263], Loss: 0.0429, Accuracy: 98.44%\n",
            "Epoch [59/250], Step [120/263], Loss: 0.0283, Accuracy: 99.06%\n",
            "Epoch [59/250], Step [130/263], Loss: 0.0449, Accuracy: 98.75%\n",
            "Epoch [59/250], Step [140/263], Loss: 0.0569, Accuracy: 97.81%\n",
            "Epoch [59/250], Step [150/263], Loss: 0.0453, Accuracy: 98.75%\n",
            "Epoch [59/250], Step [160/263], Loss: 0.0371, Accuracy: 98.28%\n",
            "Epoch [59/250], Step [170/263], Loss: 0.0280, Accuracy: 99.22%\n",
            "Epoch [59/250], Step [180/263], Loss: 0.0586, Accuracy: 98.28%\n",
            "Epoch [59/250], Step [190/263], Loss: 0.0562, Accuracy: 98.12%\n",
            "Epoch [59/250], Step [200/263], Loss: 0.0491, Accuracy: 97.97%\n",
            "Epoch [59/250], Step [210/263], Loss: 0.0352, Accuracy: 99.06%\n",
            "Epoch [59/250], Step [220/263], Loss: 0.0394, Accuracy: 98.44%\n",
            "Epoch [59/250], Step [230/263], Loss: 0.0524, Accuracy: 97.97%\n",
            "Epoch [59/250], Step [240/263], Loss: 0.0429, Accuracy: 98.59%\n",
            "Epoch [59/250], Step [250/263], Loss: 0.0575, Accuracy: 97.81%\n",
            "Epoch [59/250], Step [260/263], Loss: 0.0483, Accuracy: 98.75%\n",
            "Epoch [60/250], Step [10/263], Loss: 0.0295, Accuracy: 99.22%\n",
            "Epoch [60/250], Step [20/263], Loss: 0.0326, Accuracy: 98.12%\n",
            "Epoch [60/250], Step [30/263], Loss: 0.0344, Accuracy: 98.75%\n",
            "Epoch [60/250], Step [40/263], Loss: 0.0481, Accuracy: 97.81%\n",
            "Epoch [60/250], Step [50/263], Loss: 0.0474, Accuracy: 98.28%\n",
            "Epoch [60/250], Step [60/263], Loss: 0.0311, Accuracy: 99.22%\n",
            "Epoch [60/250], Step [70/263], Loss: 0.0425, Accuracy: 98.91%\n",
            "Epoch [60/250], Step [80/263], Loss: 0.0352, Accuracy: 98.91%\n",
            "Epoch [60/250], Step [90/263], Loss: 0.0409, Accuracy: 98.91%\n",
            "Epoch [60/250], Step [100/263], Loss: 0.0398, Accuracy: 98.75%\n",
            "Epoch [60/250], Step [110/263], Loss: 0.0493, Accuracy: 97.81%\n",
            "Epoch [60/250], Step [120/263], Loss: 0.0588, Accuracy: 97.66%\n",
            "Epoch [60/250], Step [130/263], Loss: 0.0293, Accuracy: 98.91%\n",
            "Epoch [60/250], Step [140/263], Loss: 0.0419, Accuracy: 98.12%\n",
            "Epoch [60/250], Step [150/263], Loss: 0.0338, Accuracy: 98.59%\n",
            "Epoch [60/250], Step [160/263], Loss: 0.0337, Accuracy: 99.22%\n",
            "Epoch [60/250], Step [170/263], Loss: 0.0503, Accuracy: 97.97%\n",
            "Epoch [60/250], Step [180/263], Loss: 0.0398, Accuracy: 98.75%\n",
            "Epoch [60/250], Step [190/263], Loss: 0.0623, Accuracy: 97.81%\n",
            "Epoch [60/250], Step [200/263], Loss: 0.0417, Accuracy: 98.12%\n",
            "Epoch [60/250], Step [210/263], Loss: 0.0558, Accuracy: 98.12%\n",
            "Epoch [60/250], Step [220/263], Loss: 0.0479, Accuracy: 98.28%\n",
            "Epoch [60/250], Step [230/263], Loss: 0.0364, Accuracy: 97.97%\n",
            "Epoch [60/250], Step [240/263], Loss: 0.0400, Accuracy: 99.06%\n",
            "Epoch [60/250], Step [250/263], Loss: 0.0470, Accuracy: 98.28%\n",
            "Epoch [60/250], Step [260/263], Loss: 0.0431, Accuracy: 98.59%\n",
            "Epoch [61/250], Step [10/263], Loss: 0.0403, Accuracy: 98.44%\n",
            "Epoch [61/250], Step [20/263], Loss: 0.0377, Accuracy: 98.75%\n",
            "Epoch [61/250], Step [30/263], Loss: 0.0297, Accuracy: 99.22%\n",
            "Epoch [61/250], Step [40/263], Loss: 0.0444, Accuracy: 98.75%\n",
            "Epoch [61/250], Step [50/263], Loss: 0.0555, Accuracy: 97.50%\n",
            "Epoch [61/250], Step [60/263], Loss: 0.0470, Accuracy: 98.12%\n",
            "Epoch [61/250], Step [70/263], Loss: 0.0557, Accuracy: 97.81%\n",
            "Epoch [61/250], Step [80/263], Loss: 0.0613, Accuracy: 97.66%\n",
            "Epoch [61/250], Step [90/263], Loss: 0.0475, Accuracy: 98.44%\n",
            "Epoch [61/250], Step [100/263], Loss: 0.0513, Accuracy: 98.28%\n",
            "Epoch [61/250], Step [110/263], Loss: 0.0281, Accuracy: 99.06%\n",
            "Epoch [61/250], Step [120/263], Loss: 0.0567, Accuracy: 98.28%\n",
            "Epoch [61/250], Step [130/263], Loss: 0.0564, Accuracy: 98.59%\n",
            "Epoch [61/250], Step [140/263], Loss: 0.0534, Accuracy: 97.81%\n",
            "Epoch [61/250], Step [150/263], Loss: 0.0332, Accuracy: 99.38%\n",
            "Epoch [61/250], Step [160/263], Loss: 0.0412, Accuracy: 98.28%\n",
            "Epoch [61/250], Step [170/263], Loss: 0.0522, Accuracy: 97.97%\n",
            "Epoch [61/250], Step [180/263], Loss: 0.0473, Accuracy: 98.28%\n",
            "Epoch [61/250], Step [190/263], Loss: 0.0419, Accuracy: 98.44%\n",
            "Epoch [61/250], Step [200/263], Loss: 0.0391, Accuracy: 98.59%\n",
            "Epoch [61/250], Step [210/263], Loss: 0.0425, Accuracy: 98.59%\n",
            "Epoch [61/250], Step [220/263], Loss: 0.0328, Accuracy: 98.28%\n",
            "Epoch [61/250], Step [230/263], Loss: 0.0414, Accuracy: 98.75%\n",
            "Epoch [61/250], Step [240/263], Loss: 0.0437, Accuracy: 98.44%\n",
            "Epoch [61/250], Step [250/263], Loss: 0.0536, Accuracy: 98.28%\n",
            "Epoch [61/250], Step [260/263], Loss: 0.0485, Accuracy: 98.12%\n",
            "Epoch [62/250], Step [10/263], Loss: 0.0404, Accuracy: 98.44%\n",
            "Epoch [62/250], Step [20/263], Loss: 0.0306, Accuracy: 98.91%\n",
            "Epoch [62/250], Step [30/263], Loss: 0.0291, Accuracy: 98.59%\n",
            "Epoch [62/250], Step [40/263], Loss: 0.0312, Accuracy: 98.28%\n",
            "Epoch [62/250], Step [50/263], Loss: 0.0522, Accuracy: 98.28%\n",
            "Epoch [62/250], Step [60/263], Loss: 0.0489, Accuracy: 99.38%\n",
            "Epoch [62/250], Step [70/263], Loss: 0.0600, Accuracy: 98.12%\n",
            "Epoch [62/250], Step [80/263], Loss: 0.0329, Accuracy: 98.59%\n",
            "Epoch [62/250], Step [90/263], Loss: 0.0454, Accuracy: 98.44%\n",
            "Epoch [62/250], Step [100/263], Loss: 0.0259, Accuracy: 99.38%\n",
            "Epoch [62/250], Step [110/263], Loss: 0.0314, Accuracy: 98.59%\n",
            "Epoch [62/250], Step [120/263], Loss: 0.0385, Accuracy: 98.75%\n",
            "Epoch [62/250], Step [130/263], Loss: 0.0326, Accuracy: 98.91%\n",
            "Epoch [62/250], Step [140/263], Loss: 0.0287, Accuracy: 98.75%\n",
            "Epoch [62/250], Step [150/263], Loss: 0.0325, Accuracy: 98.75%\n",
            "Epoch [62/250], Step [160/263], Loss: 0.0339, Accuracy: 98.91%\n",
            "Epoch [62/250], Step [170/263], Loss: 0.0338, Accuracy: 98.44%\n",
            "Epoch [62/250], Step [180/263], Loss: 0.0434, Accuracy: 98.75%\n",
            "Epoch [62/250], Step [190/263], Loss: 0.0368, Accuracy: 98.75%\n",
            "Epoch [62/250], Step [200/263], Loss: 0.0571, Accuracy: 98.75%\n",
            "Epoch [62/250], Step [210/263], Loss: 0.0397, Accuracy: 98.75%\n",
            "Epoch [62/250], Step [220/263], Loss: 0.0296, Accuracy: 98.91%\n",
            "Epoch [62/250], Step [230/263], Loss: 0.0481, Accuracy: 97.97%\n",
            "Epoch [62/250], Step [240/263], Loss: 0.0605, Accuracy: 98.44%\n",
            "Epoch [62/250], Step [250/263], Loss: 0.0311, Accuracy: 98.91%\n",
            "Epoch [62/250], Step [260/263], Loss: 0.0656, Accuracy: 97.50%\n",
            "Epoch [63/250], Step [10/263], Loss: 0.0488, Accuracy: 98.59%\n",
            "Epoch [63/250], Step [20/263], Loss: 0.0237, Accuracy: 99.06%\n",
            "Epoch [63/250], Step [30/263], Loss: 0.0271, Accuracy: 99.06%\n",
            "Epoch [63/250], Step [40/263], Loss: 0.0200, Accuracy: 99.69%\n",
            "Epoch [63/250], Step [50/263], Loss: 0.0197, Accuracy: 99.38%\n",
            "Epoch [63/250], Step [60/263], Loss: 0.0190, Accuracy: 99.53%\n",
            "Epoch [63/250], Step [70/263], Loss: 0.0236, Accuracy: 98.91%\n",
            "Epoch [63/250], Step [80/263], Loss: 0.0210, Accuracy: 99.22%\n",
            "Epoch [63/250], Step [90/263], Loss: 0.0289, Accuracy: 98.59%\n",
            "Epoch [63/250], Step [100/263], Loss: 0.0265, Accuracy: 99.22%\n",
            "Epoch [63/250], Step [110/263], Loss: 0.0293, Accuracy: 98.59%\n",
            "Epoch [63/250], Step [120/263], Loss: 0.0272, Accuracy: 99.53%\n",
            "Epoch [63/250], Step [130/263], Loss: 0.0239, Accuracy: 99.69%\n",
            "Epoch [63/250], Step [140/263], Loss: 0.0442, Accuracy: 98.75%\n",
            "Epoch [63/250], Step [150/263], Loss: 0.0240, Accuracy: 99.38%\n",
            "Epoch [63/250], Step [160/263], Loss: 0.0355, Accuracy: 98.75%\n",
            "Epoch [63/250], Step [170/263], Loss: 0.0283, Accuracy: 98.91%\n",
            "Epoch [63/250], Step [180/263], Loss: 0.0397, Accuracy: 98.59%\n",
            "Epoch [63/250], Step [190/263], Loss: 0.0342, Accuracy: 98.59%\n",
            "Epoch [63/250], Step [200/263], Loss: 0.0515, Accuracy: 97.97%\n",
            "Epoch [63/250], Step [210/263], Loss: 0.0792, Accuracy: 97.50%\n",
            "Epoch [63/250], Step [220/263], Loss: 0.0615, Accuracy: 97.97%\n",
            "Epoch [63/250], Step [230/263], Loss: 0.0554, Accuracy: 98.12%\n",
            "Epoch [63/250], Step [240/263], Loss: 0.0409, Accuracy: 98.28%\n",
            "Epoch [63/250], Step [250/263], Loss: 0.0256, Accuracy: 98.91%\n",
            "Epoch [63/250], Step [260/263], Loss: 0.0241, Accuracy: 99.22%\n",
            "Epoch [64/250], Step [10/263], Loss: 0.0266, Accuracy: 99.38%\n",
            "Epoch [64/250], Step [20/263], Loss: 0.0157, Accuracy: 99.69%\n",
            "Epoch [64/250], Step [30/263], Loss: 0.0203, Accuracy: 99.22%\n",
            "Epoch [64/250], Step [40/263], Loss: 0.0367, Accuracy: 98.44%\n",
            "Epoch [64/250], Step [50/263], Loss: 0.0234, Accuracy: 99.06%\n",
            "Epoch [64/250], Step [60/263], Loss: 0.0308, Accuracy: 99.06%\n",
            "Epoch [64/250], Step [70/263], Loss: 0.0349, Accuracy: 98.75%\n",
            "Epoch [64/250], Step [80/263], Loss: 0.0227, Accuracy: 99.22%\n",
            "Epoch [64/250], Step [90/263], Loss: 0.0286, Accuracy: 98.91%\n",
            "Epoch [64/250], Step [100/263], Loss: 0.0193, Accuracy: 99.53%\n",
            "Epoch [64/250], Step [110/263], Loss: 0.0376, Accuracy: 98.44%\n",
            "Epoch [64/250], Step [120/263], Loss: 0.0249, Accuracy: 98.91%\n",
            "Epoch [64/250], Step [130/263], Loss: 0.0404, Accuracy: 98.44%\n",
            "Epoch [64/250], Step [140/263], Loss: 0.0346, Accuracy: 99.06%\n",
            "Epoch [64/250], Step [150/263], Loss: 0.0522, Accuracy: 97.81%\n",
            "Epoch [64/250], Step [160/263], Loss: 0.0350, Accuracy: 98.59%\n",
            "Epoch [64/250], Step [170/263], Loss: 0.0391, Accuracy: 98.59%\n",
            "Epoch [64/250], Step [180/263], Loss: 0.0265, Accuracy: 99.38%\n",
            "Epoch [64/250], Step [190/263], Loss: 0.0274, Accuracy: 99.22%\n",
            "Epoch [64/250], Step [200/263], Loss: 0.0212, Accuracy: 99.53%\n",
            "Epoch [64/250], Step [210/263], Loss: 0.0272, Accuracy: 99.06%\n",
            "Epoch [64/250], Step [220/263], Loss: 0.0301, Accuracy: 98.75%\n",
            "Epoch [64/250], Step [230/263], Loss: 0.0177, Accuracy: 99.53%\n",
            "Epoch [64/250], Step [240/263], Loss: 0.0239, Accuracy: 99.22%\n",
            "Epoch [64/250], Step [250/263], Loss: 0.0276, Accuracy: 98.75%\n",
            "Epoch [64/250], Step [260/263], Loss: 0.0247, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [10/263], Loss: 0.0280, Accuracy: 99.06%\n",
            "Epoch [65/250], Step [20/263], Loss: 0.0193, Accuracy: 99.84%\n",
            "Epoch [65/250], Step [30/263], Loss: 0.0385, Accuracy: 98.91%\n",
            "Epoch [65/250], Step [40/263], Loss: 0.0203, Accuracy: 99.38%\n",
            "Epoch [65/250], Step [50/263], Loss: 0.0243, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [60/263], Loss: 0.0241, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [70/263], Loss: 0.0258, Accuracy: 98.75%\n",
            "Epoch [65/250], Step [80/263], Loss: 0.0209, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [90/263], Loss: 0.0249, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [100/263], Loss: 0.0291, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [110/263], Loss: 0.0125, Accuracy: 100.00%\n",
            "Epoch [65/250], Step [120/263], Loss: 0.0295, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [130/263], Loss: 0.0236, Accuracy: 99.06%\n",
            "Epoch [65/250], Step [140/263], Loss: 0.0435, Accuracy: 98.91%\n",
            "Epoch [65/250], Step [150/263], Loss: 0.0271, Accuracy: 99.06%\n",
            "Epoch [65/250], Step [160/263], Loss: 0.0262, Accuracy: 99.06%\n",
            "Epoch [65/250], Step [170/263], Loss: 0.0179, Accuracy: 99.53%\n",
            "Epoch [65/250], Step [180/263], Loss: 0.0231, Accuracy: 99.38%\n",
            "Epoch [65/250], Step [190/263], Loss: 0.0208, Accuracy: 99.69%\n",
            "Epoch [65/250], Step [200/263], Loss: 0.0224, Accuracy: 99.22%\n",
            "Epoch [65/250], Step [210/263], Loss: 0.0455, Accuracy: 98.12%\n",
            "Epoch [65/250], Step [220/263], Loss: 0.0391, Accuracy: 98.59%\n",
            "Epoch [65/250], Step [230/263], Loss: 0.0400, Accuracy: 98.44%\n",
            "Epoch [65/250], Step [240/263], Loss: 0.0312, Accuracy: 98.91%\n",
            "Epoch [65/250], Step [250/263], Loss: 0.0548, Accuracy: 98.12%\n",
            "Epoch [65/250], Step [260/263], Loss: 0.0482, Accuracy: 98.12%\n",
            "Epoch [66/250], Step [10/263], Loss: 0.0495, Accuracy: 98.75%\n",
            "Epoch [66/250], Step [20/263], Loss: 0.0214, Accuracy: 99.38%\n",
            "Epoch [66/250], Step [30/263], Loss: 0.0315, Accuracy: 98.75%\n",
            "Epoch [66/250], Step [40/263], Loss: 0.0669, Accuracy: 98.28%\n",
            "Epoch [66/250], Step [50/263], Loss: 0.0479, Accuracy: 98.44%\n",
            "Epoch [66/250], Step [60/263], Loss: 0.0365, Accuracy: 98.59%\n",
            "Epoch [66/250], Step [70/263], Loss: 0.0431, Accuracy: 98.12%\n",
            "Epoch [66/250], Step [80/263], Loss: 0.0240, Accuracy: 99.22%\n",
            "Epoch [66/250], Step [90/263], Loss: 0.0252, Accuracy: 99.53%\n",
            "Epoch [66/250], Step [100/263], Loss: 0.0249, Accuracy: 99.53%\n",
            "Epoch [66/250], Step [110/263], Loss: 0.0239, Accuracy: 99.22%\n",
            "Epoch [66/250], Step [120/263], Loss: 0.0152, Accuracy: 100.00%\n",
            "Epoch [66/250], Step [130/263], Loss: 0.0285, Accuracy: 99.38%\n",
            "Epoch [66/250], Step [140/263], Loss: 0.0426, Accuracy: 98.44%\n",
            "Epoch [66/250], Step [150/263], Loss: 0.0314, Accuracy: 98.44%\n",
            "Epoch [66/250], Step [160/263], Loss: 0.0384, Accuracy: 98.91%\n",
            "Epoch [66/250], Step [170/263], Loss: 0.0366, Accuracy: 99.22%\n",
            "Epoch [66/250], Step [180/263], Loss: 0.0315, Accuracy: 98.75%\n",
            "Epoch [66/250], Step [190/263], Loss: 0.0576, Accuracy: 97.81%\n",
            "Epoch [66/250], Step [200/263], Loss: 0.0306, Accuracy: 99.22%\n",
            "Epoch [66/250], Step [210/263], Loss: 0.0434, Accuracy: 97.97%\n",
            "Epoch [66/250], Step [220/263], Loss: 0.0291, Accuracy: 98.75%\n",
            "Epoch [66/250], Step [230/263], Loss: 0.0247, Accuracy: 98.91%\n",
            "Epoch [66/250], Step [240/263], Loss: 0.0252, Accuracy: 99.06%\n",
            "Epoch [66/250], Step [250/263], Loss: 0.0489, Accuracy: 98.59%\n",
            "Epoch [66/250], Step [260/263], Loss: 0.0284, Accuracy: 98.91%\n",
            "Epoch [67/250], Step [10/263], Loss: 0.0498, Accuracy: 98.44%\n",
            "Epoch [67/250], Step [20/263], Loss: 0.0389, Accuracy: 99.06%\n",
            "Epoch [67/250], Step [30/263], Loss: 0.0200, Accuracy: 99.53%\n",
            "Epoch [67/250], Step [40/263], Loss: 0.0225, Accuracy: 98.75%\n",
            "Epoch [67/250], Step [50/263], Loss: 0.0510, Accuracy: 98.44%\n",
            "Epoch [67/250], Step [60/263], Loss: 0.0296, Accuracy: 99.22%\n",
            "Epoch [67/250], Step [70/263], Loss: 0.0254, Accuracy: 99.22%\n",
            "Epoch [67/250], Step [80/263], Loss: 0.0198, Accuracy: 99.53%\n",
            "Epoch [67/250], Step [90/263], Loss: 0.0333, Accuracy: 98.75%\n",
            "Epoch [67/250], Step [100/263], Loss: 0.0263, Accuracy: 99.22%\n",
            "Epoch [67/250], Step [110/263], Loss: 0.0218, Accuracy: 99.22%\n",
            "Epoch [67/250], Step [120/263], Loss: 0.0217, Accuracy: 99.22%\n",
            "Epoch [67/250], Step [130/263], Loss: 0.0226, Accuracy: 99.06%\n",
            "Epoch [67/250], Step [140/263], Loss: 0.0166, Accuracy: 99.69%\n",
            "Epoch [67/250], Step [150/263], Loss: 0.0260, Accuracy: 99.06%\n",
            "Epoch [67/250], Step [160/263], Loss: 0.0268, Accuracy: 99.22%\n",
            "Epoch [67/250], Step [170/263], Loss: 0.0199, Accuracy: 99.53%\n",
            "Epoch [67/250], Step [180/263], Loss: 0.0177, Accuracy: 99.22%\n",
            "Epoch [67/250], Step [190/263], Loss: 0.0309, Accuracy: 98.91%\n",
            "Epoch [67/250], Step [200/263], Loss: 0.0349, Accuracy: 98.75%\n",
            "Epoch [67/250], Step [210/263], Loss: 0.0206, Accuracy: 99.53%\n",
            "Epoch [67/250], Step [220/263], Loss: 0.0190, Accuracy: 99.53%\n",
            "Epoch [67/250], Step [230/263], Loss: 0.0239, Accuracy: 99.38%\n",
            "Epoch [67/250], Step [240/263], Loss: 0.0321, Accuracy: 98.91%\n",
            "Epoch [67/250], Step [250/263], Loss: 0.0288, Accuracy: 98.75%\n",
            "Epoch [67/250], Step [260/263], Loss: 0.0232, Accuracy: 99.38%\n",
            "Epoch [68/250], Step [10/263], Loss: 0.0319, Accuracy: 99.06%\n",
            "Epoch [68/250], Step [20/263], Loss: 0.0210, Accuracy: 99.22%\n",
            "Epoch [68/250], Step [30/263], Loss: 0.0338, Accuracy: 98.75%\n",
            "Epoch [68/250], Step [40/263], Loss: 0.0135, Accuracy: 99.38%\n",
            "Epoch [68/250], Step [50/263], Loss: 0.0127, Accuracy: 99.69%\n",
            "Epoch [68/250], Step [60/263], Loss: 0.0309, Accuracy: 98.91%\n",
            "Epoch [68/250], Step [70/263], Loss: 0.0303, Accuracy: 98.59%\n",
            "Epoch [68/250], Step [80/263], Loss: 0.0367, Accuracy: 98.75%\n",
            "Epoch [68/250], Step [90/263], Loss: 0.0375, Accuracy: 98.91%\n",
            "Epoch [68/250], Step [100/263], Loss: 0.0394, Accuracy: 98.28%\n",
            "Epoch [68/250], Step [110/263], Loss: 0.0204, Accuracy: 99.22%\n",
            "Epoch [68/250], Step [120/263], Loss: 0.0424, Accuracy: 98.75%\n",
            "Epoch [68/250], Step [130/263], Loss: 0.0373, Accuracy: 98.75%\n",
            "Epoch [68/250], Step [140/263], Loss: 0.0605, Accuracy: 98.12%\n",
            "Epoch [68/250], Step [150/263], Loss: 0.0628, Accuracy: 98.28%\n",
            "Epoch [68/250], Step [160/263], Loss: 0.0683, Accuracy: 97.03%\n",
            "Epoch [68/250], Step [170/263], Loss: 0.0985, Accuracy: 97.50%\n",
            "Epoch [68/250], Step [180/263], Loss: 0.0480, Accuracy: 97.66%\n",
            "Epoch [68/250], Step [190/263], Loss: 0.0642, Accuracy: 97.50%\n",
            "Epoch [68/250], Step [200/263], Loss: 0.0643, Accuracy: 97.50%\n",
            "Epoch [68/250], Step [210/263], Loss: 0.0330, Accuracy: 98.91%\n",
            "Epoch [68/250], Step [220/263], Loss: 0.0317, Accuracy: 98.75%\n",
            "Epoch [68/250], Step [230/263], Loss: 0.0397, Accuracy: 98.59%\n",
            "Epoch [68/250], Step [240/263], Loss: 0.0544, Accuracy: 97.66%\n",
            "Epoch [68/250], Step [250/263], Loss: 0.0395, Accuracy: 98.44%\n",
            "Epoch [68/250], Step [260/263], Loss: 0.0332, Accuracy: 98.59%\n",
            "Epoch [69/250], Step [10/263], Loss: 0.0364, Accuracy: 98.91%\n",
            "Epoch [69/250], Step [20/263], Loss: 0.0229, Accuracy: 99.22%\n",
            "Epoch [69/250], Step [30/263], Loss: 0.0150, Accuracy: 99.69%\n",
            "Epoch [69/250], Step [40/263], Loss: 0.0213, Accuracy: 99.06%\n",
            "Epoch [69/250], Step [50/263], Loss: 0.0294, Accuracy: 99.22%\n",
            "Epoch [69/250], Step [60/263], Loss: 0.0194, Accuracy: 99.38%\n",
            "Epoch [69/250], Step [70/263], Loss: 0.0388, Accuracy: 98.59%\n",
            "Epoch [69/250], Step [80/263], Loss: 0.0256, Accuracy: 99.06%\n",
            "Epoch [69/250], Step [90/263], Loss: 0.0201, Accuracy: 99.53%\n",
            "Epoch [69/250], Step [100/263], Loss: 0.0401, Accuracy: 98.44%\n",
            "Epoch [69/250], Step [110/263], Loss: 0.0425, Accuracy: 98.59%\n",
            "Epoch [69/250], Step [120/263], Loss: 0.0399, Accuracy: 98.59%\n",
            "Epoch [69/250], Step [130/263], Loss: 0.0206, Accuracy: 99.22%\n",
            "Epoch [69/250], Step [140/263], Loss: 0.0250, Accuracy: 99.38%\n",
            "Epoch [69/250], Step [150/263], Loss: 0.0433, Accuracy: 98.59%\n",
            "Epoch [69/250], Step [160/263], Loss: 0.0398, Accuracy: 98.28%\n",
            "Epoch [69/250], Step [170/263], Loss: 0.0311, Accuracy: 99.06%\n",
            "Epoch [69/250], Step [180/263], Loss: 0.0233, Accuracy: 99.06%\n",
            "Epoch [69/250], Step [190/263], Loss: 0.0212, Accuracy: 99.38%\n",
            "Epoch [69/250], Step [200/263], Loss: 0.0349, Accuracy: 98.91%\n",
            "Epoch [69/250], Step [210/263], Loss: 0.0232, Accuracy: 99.38%\n",
            "Epoch [69/250], Step [220/263], Loss: 0.0280, Accuracy: 98.59%\n",
            "Epoch [69/250], Step [230/263], Loss: 0.0340, Accuracy: 98.91%\n",
            "Epoch [69/250], Step [240/263], Loss: 0.0461, Accuracy: 98.12%\n",
            "Epoch [69/250], Step [250/263], Loss: 0.0666, Accuracy: 97.81%\n",
            "Epoch [69/250], Step [260/263], Loss: 0.0464, Accuracy: 98.91%\n",
            "Epoch [70/250], Step [10/263], Loss: 0.0410, Accuracy: 98.59%\n",
            "Epoch [70/250], Step [20/263], Loss: 0.0177, Accuracy: 99.06%\n",
            "Epoch [70/250], Step [30/263], Loss: 0.0179, Accuracy: 99.53%\n",
            "Epoch [70/250], Step [40/263], Loss: 0.0217, Accuracy: 99.22%\n",
            "Epoch [70/250], Step [50/263], Loss: 0.0231, Accuracy: 99.22%\n",
            "Epoch [70/250], Step [60/263], Loss: 0.0159, Accuracy: 99.38%\n",
            "Epoch [70/250], Step [70/263], Loss: 0.0289, Accuracy: 98.75%\n",
            "Epoch [70/250], Step [80/263], Loss: 0.0422, Accuracy: 99.22%\n",
            "Epoch [70/250], Step [90/263], Loss: 0.0350, Accuracy: 99.22%\n",
            "Epoch [70/250], Step [100/263], Loss: 0.0277, Accuracy: 99.06%\n",
            "Epoch [70/250], Step [110/263], Loss: 0.0160, Accuracy: 99.53%\n",
            "Epoch [70/250], Step [120/263], Loss: 0.0230, Accuracy: 99.06%\n",
            "Epoch [70/250], Step [130/263], Loss: 0.0201, Accuracy: 99.22%\n",
            "Epoch [70/250], Step [140/263], Loss: 0.0169, Accuracy: 99.38%\n",
            "Epoch [70/250], Step [150/263], Loss: 0.0369, Accuracy: 98.44%\n",
            "Epoch [70/250], Step [160/263], Loss: 0.0218, Accuracy: 98.91%\n",
            "Epoch [70/250], Step [170/263], Loss: 0.0225, Accuracy: 98.91%\n",
            "Epoch [70/250], Step [180/263], Loss: 0.0137, Accuracy: 99.53%\n",
            "Epoch [70/250], Step [190/263], Loss: 0.0213, Accuracy: 99.38%\n",
            "Epoch [70/250], Step [200/263], Loss: 0.0289, Accuracy: 98.91%\n",
            "Epoch [70/250], Step [210/263], Loss: 0.0219, Accuracy: 98.75%\n",
            "Epoch [70/250], Step [220/263], Loss: 0.0136, Accuracy: 99.69%\n",
            "Epoch [70/250], Step [230/263], Loss: 0.0163, Accuracy: 99.69%\n",
            "Epoch [70/250], Step [240/263], Loss: 0.0212, Accuracy: 99.38%\n",
            "Epoch [70/250], Step [250/263], Loss: 0.0152, Accuracy: 99.38%\n",
            "Epoch [70/250], Step [260/263], Loss: 0.0206, Accuracy: 99.22%\n",
            "Epoch [71/250], Step [10/263], Loss: 0.0150, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [20/263], Loss: 0.0190, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [30/263], Loss: 0.0137, Accuracy: 99.53%\n",
            "Epoch [71/250], Step [40/263], Loss: 0.0196, Accuracy: 99.38%\n",
            "Epoch [71/250], Step [50/263], Loss: 0.0142, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [60/263], Loss: 0.0217, Accuracy: 99.38%\n",
            "Epoch [71/250], Step [70/263], Loss: 0.0135, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [80/263], Loss: 0.0119, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [90/263], Loss: 0.0183, Accuracy: 99.53%\n",
            "Epoch [71/250], Step [100/263], Loss: 0.0206, Accuracy: 99.38%\n",
            "Epoch [71/250], Step [110/263], Loss: 0.0142, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [120/263], Loss: 0.0149, Accuracy: 99.53%\n",
            "Epoch [71/250], Step [130/263], Loss: 0.0149, Accuracy: 99.53%\n",
            "Epoch [71/250], Step [140/263], Loss: 0.0258, Accuracy: 99.06%\n",
            "Epoch [71/250], Step [150/263], Loss: 0.0134, Accuracy: 99.38%\n",
            "Epoch [71/250], Step [160/263], Loss: 0.0117, Accuracy: 99.53%\n",
            "Epoch [71/250], Step [170/263], Loss: 0.0124, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [180/263], Loss: 0.0174, Accuracy: 99.22%\n",
            "Epoch [71/250], Step [190/263], Loss: 0.0168, Accuracy: 99.22%\n",
            "Epoch [71/250], Step [200/263], Loss: 0.0107, Accuracy: 99.69%\n",
            "Epoch [71/250], Step [210/263], Loss: 0.0131, Accuracy: 99.22%\n",
            "Epoch [71/250], Step [220/263], Loss: 0.0099, Accuracy: 99.84%\n",
            "Epoch [71/250], Step [230/263], Loss: 0.0079, Accuracy: 99.84%\n",
            "Epoch [71/250], Step [240/263], Loss: 0.0095, Accuracy: 99.84%\n",
            "Epoch [71/250], Step [250/263], Loss: 0.0208, Accuracy: 99.22%\n",
            "Epoch [71/250], Step [260/263], Loss: 0.0290, Accuracy: 98.44%\n",
            "Epoch [72/250], Step [10/263], Loss: 0.0101, Accuracy: 99.84%\n",
            "Epoch [72/250], Step [20/263], Loss: 0.0177, Accuracy: 99.38%\n",
            "Epoch [72/250], Step [30/263], Loss: 0.0088, Accuracy: 99.84%\n",
            "Epoch [72/250], Step [40/263], Loss: 0.0129, Accuracy: 99.38%\n",
            "Epoch [72/250], Step [50/263], Loss: 0.0112, Accuracy: 99.69%\n",
            "Epoch [72/250], Step [60/263], Loss: 0.0128, Accuracy: 99.84%\n",
            "Epoch [72/250], Step [70/263], Loss: 0.0061, Accuracy: 100.00%\n",
            "Epoch [72/250], Step [80/263], Loss: 0.0115, Accuracy: 99.53%\n",
            "Epoch [72/250], Step [90/263], Loss: 0.0099, Accuracy: 99.84%\n",
            "Epoch [72/250], Step [100/263], Loss: 0.0069, Accuracy: 100.00%\n",
            "Epoch [72/250], Step [110/263], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [72/250], Step [120/263], Loss: 0.0069, Accuracy: 99.84%\n",
            "Epoch [72/250], Step [130/263], Loss: 0.0076, Accuracy: 99.84%\n",
            "Epoch [72/250], Step [140/263], Loss: 0.0125, Accuracy: 99.53%\n",
            "Epoch [72/250], Step [150/263], Loss: 0.0103, Accuracy: 99.69%\n",
            "Epoch [72/250], Step [160/263], Loss: 0.0115, Accuracy: 99.53%\n",
            "Epoch [72/250], Step [170/263], Loss: 0.0155, Accuracy: 99.53%\n",
            "Epoch [72/250], Step [180/263], Loss: 0.0156, Accuracy: 99.53%\n",
            "Epoch [72/250], Step [190/263], Loss: 0.0175, Accuracy: 99.69%\n",
            "Epoch [72/250], Step [200/263], Loss: 0.0413, Accuracy: 98.75%\n",
            "Epoch [72/250], Step [210/263], Loss: 0.0247, Accuracy: 99.38%\n",
            "Epoch [72/250], Step [220/263], Loss: 0.0323, Accuracy: 98.91%\n",
            "Epoch [72/250], Step [230/263], Loss: 0.0125, Accuracy: 99.69%\n",
            "Epoch [72/250], Step [240/263], Loss: 0.0237, Accuracy: 98.91%\n",
            "Epoch [72/250], Step [250/263], Loss: 0.0293, Accuracy: 98.91%\n",
            "Epoch [72/250], Step [260/263], Loss: 0.0273, Accuracy: 98.75%\n",
            "Epoch [73/250], Step [10/263], Loss: 0.0229, Accuracy: 99.22%\n",
            "Epoch [73/250], Step [20/263], Loss: 0.0461, Accuracy: 98.59%\n",
            "Epoch [73/250], Step [30/263], Loss: 0.0309, Accuracy: 98.75%\n",
            "Epoch [73/250], Step [40/263], Loss: 0.0276, Accuracy: 99.38%\n",
            "Epoch [73/250], Step [50/263], Loss: 0.0253, Accuracy: 98.91%\n",
            "Epoch [73/250], Step [60/263], Loss: 0.0150, Accuracy: 99.38%\n",
            "Epoch [73/250], Step [70/263], Loss: 0.0285, Accuracy: 99.06%\n",
            "Epoch [73/250], Step [80/263], Loss: 0.0302, Accuracy: 98.75%\n",
            "Epoch [73/250], Step [90/263], Loss: 0.0167, Accuracy: 99.53%\n",
            "Epoch [73/250], Step [100/263], Loss: 0.0179, Accuracy: 99.22%\n",
            "Epoch [73/250], Step [110/263], Loss: 0.0088, Accuracy: 99.84%\n",
            "Epoch [73/250], Step [120/263], Loss: 0.0188, Accuracy: 99.22%\n",
            "Epoch [73/250], Step [130/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [73/250], Step [140/263], Loss: 0.0428, Accuracy: 98.75%\n",
            "Epoch [73/250], Step [150/263], Loss: 0.0214, Accuracy: 99.38%\n",
            "Epoch [73/250], Step [160/263], Loss: 0.0269, Accuracy: 98.91%\n",
            "Epoch [73/250], Step [170/263], Loss: 0.0242, Accuracy: 99.22%\n",
            "Epoch [73/250], Step [180/263], Loss: 0.0294, Accuracy: 99.22%\n",
            "Epoch [73/250], Step [190/263], Loss: 0.0376, Accuracy: 98.75%\n",
            "Epoch [73/250], Step [200/263], Loss: 0.0431, Accuracy: 98.59%\n",
            "Epoch [73/250], Step [210/263], Loss: 0.0414, Accuracy: 98.44%\n",
            "Epoch [73/250], Step [220/263], Loss: 0.0310, Accuracy: 99.06%\n",
            "Epoch [73/250], Step [230/263], Loss: 0.0529, Accuracy: 97.97%\n",
            "Epoch [73/250], Step [240/263], Loss: 0.0237, Accuracy: 99.06%\n",
            "Epoch [73/250], Step [250/263], Loss: 0.0209, Accuracy: 99.38%\n",
            "Epoch [73/250], Step [260/263], Loss: 0.0180, Accuracy: 99.22%\n",
            "Epoch [74/250], Step [10/263], Loss: 0.0202, Accuracy: 98.91%\n",
            "Epoch [74/250], Step [20/263], Loss: 0.0150, Accuracy: 99.69%\n",
            "Epoch [74/250], Step [30/263], Loss: 0.0120, Accuracy: 99.84%\n",
            "Epoch [74/250], Step [40/263], Loss: 0.0114, Accuracy: 99.53%\n",
            "Epoch [74/250], Step [50/263], Loss: 0.0127, Accuracy: 99.53%\n",
            "Epoch [74/250], Step [60/263], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [74/250], Step [70/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [74/250], Step [80/263], Loss: 0.0156, Accuracy: 99.38%\n",
            "Epoch [74/250], Step [90/263], Loss: 0.0152, Accuracy: 99.22%\n",
            "Epoch [74/250], Step [100/263], Loss: 0.0179, Accuracy: 99.53%\n",
            "Epoch [74/250], Step [110/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [74/250], Step [120/263], Loss: 0.0310, Accuracy: 99.06%\n",
            "Epoch [74/250], Step [130/263], Loss: 0.0097, Accuracy: 99.69%\n",
            "Epoch [74/250], Step [140/263], Loss: 0.0375, Accuracy: 98.75%\n",
            "Epoch [74/250], Step [150/263], Loss: 0.0285, Accuracy: 98.75%\n",
            "Epoch [74/250], Step [160/263], Loss: 0.0215, Accuracy: 99.38%\n",
            "Epoch [74/250], Step [170/263], Loss: 0.0164, Accuracy: 99.38%\n",
            "Epoch [74/250], Step [180/263], Loss: 0.0609, Accuracy: 98.28%\n",
            "Epoch [74/250], Step [190/263], Loss: 0.0317, Accuracy: 98.91%\n",
            "Epoch [74/250], Step [200/263], Loss: 0.0686, Accuracy: 97.97%\n",
            "Epoch [74/250], Step [210/263], Loss: 0.0456, Accuracy: 98.59%\n",
            "Epoch [74/250], Step [220/263], Loss: 0.0457, Accuracy: 98.12%\n",
            "Epoch [74/250], Step [230/263], Loss: 0.0171, Accuracy: 99.53%\n",
            "Epoch [74/250], Step [240/263], Loss: 0.0403, Accuracy: 98.59%\n",
            "Epoch [74/250], Step [250/263], Loss: 0.0285, Accuracy: 99.06%\n",
            "Epoch [74/250], Step [260/263], Loss: 0.0220, Accuracy: 99.22%\n",
            "Epoch [75/250], Step [10/263], Loss: 0.0454, Accuracy: 98.44%\n",
            "Epoch [75/250], Step [20/263], Loss: 0.0424, Accuracy: 98.44%\n",
            "Epoch [75/250], Step [30/263], Loss: 0.0462, Accuracy: 98.44%\n",
            "Epoch [75/250], Step [40/263], Loss: 0.0618, Accuracy: 98.28%\n",
            "Epoch [75/250], Step [50/263], Loss: 0.0428, Accuracy: 98.28%\n",
            "Epoch [75/250], Step [60/263], Loss: 0.0437, Accuracy: 98.59%\n",
            "Epoch [75/250], Step [70/263], Loss: 0.0365, Accuracy: 98.44%\n",
            "Epoch [75/250], Step [80/263], Loss: 0.0333, Accuracy: 99.06%\n",
            "Epoch [75/250], Step [90/263], Loss: 0.0262, Accuracy: 99.06%\n",
            "Epoch [75/250], Step [100/263], Loss: 0.0205, Accuracy: 99.53%\n",
            "Epoch [75/250], Step [110/263], Loss: 0.0168, Accuracy: 99.53%\n",
            "Epoch [75/250], Step [120/263], Loss: 0.0210, Accuracy: 99.06%\n",
            "Epoch [75/250], Step [130/263], Loss: 0.0352, Accuracy: 97.97%\n",
            "Epoch [75/250], Step [140/263], Loss: 0.0362, Accuracy: 98.59%\n",
            "Epoch [75/250], Step [150/263], Loss: 0.0525, Accuracy: 98.59%\n",
            "Epoch [75/250], Step [160/263], Loss: 0.0332, Accuracy: 98.44%\n",
            "Epoch [75/250], Step [170/263], Loss: 0.0253, Accuracy: 99.53%\n",
            "Epoch [75/250], Step [180/263], Loss: 0.0231, Accuracy: 99.53%\n",
            "Epoch [75/250], Step [190/263], Loss: 0.0249, Accuracy: 99.69%\n",
            "Epoch [75/250], Step [200/263], Loss: 0.0231, Accuracy: 99.53%\n",
            "Epoch [75/250], Step [210/263], Loss: 0.0375, Accuracy: 99.06%\n",
            "Epoch [75/250], Step [220/263], Loss: 0.0395, Accuracy: 98.91%\n",
            "Epoch [75/250], Step [230/263], Loss: 0.0294, Accuracy: 99.06%\n",
            "Epoch [75/250], Step [240/263], Loss: 0.0146, Accuracy: 99.84%\n",
            "Epoch [75/250], Step [250/263], Loss: 0.0196, Accuracy: 99.69%\n",
            "Epoch [75/250], Step [260/263], Loss: 0.0285, Accuracy: 98.59%\n",
            "Epoch [76/250], Step [10/263], Loss: 0.0296, Accuracy: 98.75%\n",
            "Epoch [76/250], Step [20/263], Loss: 0.0255, Accuracy: 98.91%\n",
            "Epoch [76/250], Step [30/263], Loss: 0.0135, Accuracy: 99.69%\n",
            "Epoch [76/250], Step [40/263], Loss: 0.0113, Accuracy: 99.84%\n",
            "Epoch [76/250], Step [50/263], Loss: 0.0162, Accuracy: 99.38%\n",
            "Epoch [76/250], Step [60/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [76/250], Step [70/263], Loss: 0.0185, Accuracy: 99.53%\n",
            "Epoch [76/250], Step [80/263], Loss: 0.0256, Accuracy: 99.22%\n",
            "Epoch [76/250], Step [90/263], Loss: 0.0258, Accuracy: 98.91%\n",
            "Epoch [76/250], Step [100/263], Loss: 0.0181, Accuracy: 99.38%\n",
            "Epoch [76/250], Step [110/263], Loss: 0.0079, Accuracy: 100.00%\n",
            "Epoch [76/250], Step [120/263], Loss: 0.0153, Accuracy: 99.53%\n",
            "Epoch [76/250], Step [130/263], Loss: 0.0273, Accuracy: 98.44%\n",
            "Epoch [76/250], Step [140/263], Loss: 0.0259, Accuracy: 99.06%\n",
            "Epoch [76/250], Step [150/263], Loss: 0.0327, Accuracy: 98.28%\n",
            "Epoch [76/250], Step [160/263], Loss: 0.0305, Accuracy: 98.75%\n",
            "Epoch [76/250], Step [170/263], Loss: 0.0219, Accuracy: 99.38%\n",
            "Epoch [76/250], Step [180/263], Loss: 0.0306, Accuracy: 99.06%\n",
            "Epoch [76/250], Step [190/263], Loss: 0.0473, Accuracy: 98.91%\n",
            "Epoch [76/250], Step [200/263], Loss: 0.0598, Accuracy: 98.59%\n",
            "Epoch [76/250], Step [210/263], Loss: 0.1485, Accuracy: 97.34%\n",
            "Epoch [76/250], Step [220/263], Loss: 0.0527, Accuracy: 97.66%\n",
            "Epoch [76/250], Step [230/263], Loss: 0.0727, Accuracy: 98.75%\n",
            "Epoch [76/250], Step [240/263], Loss: 0.0449, Accuracy: 98.12%\n",
            "Epoch [76/250], Step [250/263], Loss: 0.0526, Accuracy: 97.81%\n",
            "Epoch [76/250], Step [260/263], Loss: 0.0725, Accuracy: 97.66%\n",
            "Epoch [77/250], Step [10/263], Loss: 0.0490, Accuracy: 97.97%\n",
            "Epoch [77/250], Step [20/263], Loss: 0.0269, Accuracy: 98.91%\n",
            "Epoch [77/250], Step [30/263], Loss: 0.0276, Accuracy: 98.91%\n",
            "Epoch [77/250], Step [40/263], Loss: 0.0284, Accuracy: 98.91%\n",
            "Epoch [77/250], Step [50/263], Loss: 0.0258, Accuracy: 99.38%\n",
            "Epoch [77/250], Step [60/263], Loss: 0.0207, Accuracy: 99.38%\n",
            "Epoch [77/250], Step [70/263], Loss: 0.0164, Accuracy: 99.84%\n",
            "Epoch [77/250], Step [80/263], Loss: 0.0266, Accuracy: 98.75%\n",
            "Epoch [77/250], Step [90/263], Loss: 0.0211, Accuracy: 99.06%\n",
            "Epoch [77/250], Step [100/263], Loss: 0.0203, Accuracy: 99.38%\n",
            "Epoch [77/250], Step [110/263], Loss: 0.0227, Accuracy: 98.75%\n",
            "Epoch [77/250], Step [120/263], Loss: 0.0176, Accuracy: 99.38%\n",
            "Epoch [77/250], Step [130/263], Loss: 0.0176, Accuracy: 99.22%\n",
            "Epoch [77/250], Step [140/263], Loss: 0.0220, Accuracy: 99.38%\n",
            "Epoch [77/250], Step [150/263], Loss: 0.0267, Accuracy: 99.38%\n",
            "Epoch [77/250], Step [160/263], Loss: 0.0208, Accuracy: 98.91%\n",
            "Epoch [77/250], Step [170/263], Loss: 0.0199, Accuracy: 99.53%\n",
            "Epoch [77/250], Step [180/263], Loss: 0.0134, Accuracy: 99.53%\n",
            "Epoch [77/250], Step [190/263], Loss: 0.0288, Accuracy: 98.75%\n",
            "Epoch [77/250], Step [200/263], Loss: 0.0441, Accuracy: 98.44%\n",
            "Epoch [77/250], Step [210/263], Loss: 0.0248, Accuracy: 98.91%\n",
            "Epoch [77/250], Step [220/263], Loss: 0.0228, Accuracy: 99.06%\n",
            "Epoch [77/250], Step [230/263], Loss: 0.0244, Accuracy: 99.22%\n",
            "Epoch [77/250], Step [240/263], Loss: 0.0250, Accuracy: 99.69%\n",
            "Epoch [77/250], Step [250/263], Loss: 0.0350, Accuracy: 98.44%\n",
            "Epoch [77/250], Step [260/263], Loss: 0.0189, Accuracy: 99.22%\n",
            "Epoch [78/250], Step [10/263], Loss: 0.0239, Accuracy: 99.22%\n",
            "Epoch [78/250], Step [20/263], Loss: 0.0157, Accuracy: 99.38%\n",
            "Epoch [78/250], Step [30/263], Loss: 0.0250, Accuracy: 99.38%\n",
            "Epoch [78/250], Step [40/263], Loss: 0.0440, Accuracy: 97.66%\n",
            "Epoch [78/250], Step [50/263], Loss: 0.0292, Accuracy: 99.06%\n",
            "Epoch [78/250], Step [60/263], Loss: 0.0104, Accuracy: 99.84%\n",
            "Epoch [78/250], Step [70/263], Loss: 0.0189, Accuracy: 99.53%\n",
            "Epoch [78/250], Step [80/263], Loss: 0.0195, Accuracy: 99.69%\n",
            "Epoch [78/250], Step [90/263], Loss: 0.0113, Accuracy: 99.69%\n",
            "Epoch [78/250], Step [100/263], Loss: 0.0064, Accuracy: 99.69%\n",
            "Epoch [78/250], Step [110/263], Loss: 0.0150, Accuracy: 99.38%\n",
            "Epoch [78/250], Step [120/263], Loss: 0.0139, Accuracy: 99.53%\n",
            "Epoch [78/250], Step [130/263], Loss: 0.0248, Accuracy: 99.06%\n",
            "Epoch [78/250], Step [140/263], Loss: 0.0162, Accuracy: 99.53%\n",
            "Epoch [78/250], Step [150/263], Loss: 0.0125, Accuracy: 99.69%\n",
            "Epoch [78/250], Step [160/263], Loss: 0.0152, Accuracy: 99.53%\n",
            "Epoch [78/250], Step [170/263], Loss: 0.0190, Accuracy: 99.69%\n",
            "Epoch [78/250], Step [180/263], Loss: 0.0113, Accuracy: 99.69%\n",
            "Epoch [78/250], Step [190/263], Loss: 0.0113, Accuracy: 99.38%\n",
            "Epoch [78/250], Step [200/263], Loss: 0.0120, Accuracy: 99.38%\n",
            "Epoch [78/250], Step [210/263], Loss: 0.0086, Accuracy: 99.84%\n",
            "Epoch [78/250], Step [220/263], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [78/250], Step [230/263], Loss: 0.0130, Accuracy: 99.53%\n",
            "Epoch [78/250], Step [240/263], Loss: 0.0099, Accuracy: 99.53%\n",
            "Epoch [78/250], Step [250/263], Loss: 0.0184, Accuracy: 99.53%\n",
            "Epoch [78/250], Step [260/263], Loss: 0.0178, Accuracy: 99.38%\n",
            "Epoch [79/250], Step [10/263], Loss: 0.0104, Accuracy: 99.53%\n",
            "Epoch [79/250], Step [20/263], Loss: 0.0177, Accuracy: 99.38%\n",
            "Epoch [79/250], Step [30/263], Loss: 0.0055, Accuracy: 99.84%\n",
            "Epoch [79/250], Step [40/263], Loss: 0.0044, Accuracy: 100.00%\n",
            "Epoch [79/250], Step [50/263], Loss: 0.0130, Accuracy: 99.69%\n",
            "Epoch [79/250], Step [60/263], Loss: 0.0122, Accuracy: 99.84%\n",
            "Epoch [79/250], Step [70/263], Loss: 0.0114, Accuracy: 99.69%\n",
            "Epoch [79/250], Step [80/263], Loss: 0.0091, Accuracy: 99.53%\n",
            "Epoch [79/250], Step [90/263], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [79/250], Step [100/263], Loss: 0.0215, Accuracy: 99.53%\n",
            "Epoch [79/250], Step [110/263], Loss: 0.0050, Accuracy: 99.84%\n",
            "Epoch [79/250], Step [120/263], Loss: 0.0186, Accuracy: 99.38%\n",
            "Epoch [79/250], Step [130/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [79/250], Step [140/263], Loss: 0.0207, Accuracy: 99.38%\n",
            "Epoch [79/250], Step [150/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [79/250], Step [160/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [79/250], Step [170/263], Loss: 0.0531, Accuracy: 99.38%\n",
            "Epoch [79/250], Step [180/263], Loss: 0.0066, Accuracy: 100.00%\n",
            "Epoch [79/250], Step [190/263], Loss: 0.0109, Accuracy: 99.84%\n",
            "Epoch [79/250], Step [200/263], Loss: 0.0111, Accuracy: 99.53%\n",
            "Epoch [79/250], Step [210/263], Loss: 0.0091, Accuracy: 99.69%\n",
            "Epoch [79/250], Step [220/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [79/250], Step [230/263], Loss: 0.0181, Accuracy: 99.53%\n",
            "Epoch [79/250], Step [240/263], Loss: 0.0092, Accuracy: 99.84%\n",
            "Epoch [79/250], Step [250/263], Loss: 0.0146, Accuracy: 99.84%\n",
            "Epoch [79/250], Step [260/263], Loss: 0.0162, Accuracy: 99.53%\n",
            "Epoch [80/250], Step [10/263], Loss: 0.0081, Accuracy: 99.69%\n",
            "Epoch [80/250], Step [20/263], Loss: 0.0045, Accuracy: 100.00%\n",
            "Epoch [80/250], Step [30/263], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [80/250], Step [40/263], Loss: 0.0209, Accuracy: 99.22%\n",
            "Epoch [80/250], Step [50/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [60/263], Loss: 0.0128, Accuracy: 99.53%\n",
            "Epoch [80/250], Step [70/263], Loss: 0.0078, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [80/263], Loss: 0.0185, Accuracy: 99.53%\n",
            "Epoch [80/250], Step [90/263], Loss: 0.0081, Accuracy: 99.69%\n",
            "Epoch [80/250], Step [100/263], Loss: 0.0086, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [110/263], Loss: 0.0131, Accuracy: 99.38%\n",
            "Epoch [80/250], Step [120/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [80/250], Step [130/263], Loss: 0.0105, Accuracy: 99.38%\n",
            "Epoch [80/250], Step [140/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [150/263], Loss: 0.0091, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [160/263], Loss: 0.0091, Accuracy: 99.69%\n",
            "Epoch [80/250], Step [170/263], Loss: 0.0080, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [180/263], Loss: 0.0090, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [190/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [200/263], Loss: 0.0086, Accuracy: 99.69%\n",
            "Epoch [80/250], Step [210/263], Loss: 0.0168, Accuracy: 99.53%\n",
            "Epoch [80/250], Step [220/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [80/250], Step [230/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [80/250], Step [240/263], Loss: 0.0125, Accuracy: 99.84%\n",
            "Epoch [80/250], Step [250/263], Loss: 0.0090, Accuracy: 99.69%\n",
            "Epoch [80/250], Step [260/263], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [81/250], Step [10/263], Loss: 0.0079, Accuracy: 99.69%\n",
            "Epoch [81/250], Step [20/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [81/250], Step [30/263], Loss: 0.0102, Accuracy: 99.69%\n",
            "Epoch [81/250], Step [40/263], Loss: 0.0119, Accuracy: 99.84%\n",
            "Epoch [81/250], Step [50/263], Loss: 0.0202, Accuracy: 99.38%\n",
            "Epoch [81/250], Step [60/263], Loss: 0.0259, Accuracy: 99.06%\n",
            "Epoch [81/250], Step [70/263], Loss: 0.0324, Accuracy: 98.44%\n",
            "Epoch [81/250], Step [80/263], Loss: 0.0289, Accuracy: 99.06%\n",
            "Epoch [81/250], Step [90/263], Loss: 0.0250, Accuracy: 99.06%\n",
            "Epoch [81/250], Step [100/263], Loss: 0.0201, Accuracy: 99.22%\n",
            "Epoch [81/250], Step [110/263], Loss: 0.0221, Accuracy: 98.91%\n",
            "Epoch [81/250], Step [120/263], Loss: 0.0316, Accuracy: 98.75%\n",
            "Epoch [81/250], Step [130/263], Loss: 0.0109, Accuracy: 99.69%\n",
            "Epoch [81/250], Step [140/263], Loss: 0.0248, Accuracy: 99.06%\n",
            "Epoch [81/250], Step [150/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [81/250], Step [160/263], Loss: 0.0119, Accuracy: 99.69%\n",
            "Epoch [81/250], Step [170/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [81/250], Step [180/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [81/250], Step [190/263], Loss: 0.0126, Accuracy: 99.69%\n",
            "Epoch [81/250], Step [200/263], Loss: 0.0290, Accuracy: 99.06%\n",
            "Epoch [81/250], Step [210/263], Loss: 0.0322, Accuracy: 98.91%\n",
            "Epoch [81/250], Step [220/263], Loss: 0.0330, Accuracy: 98.75%\n",
            "Epoch [81/250], Step [230/263], Loss: 0.0262, Accuracy: 99.06%\n",
            "Epoch [81/250], Step [240/263], Loss: 0.0455, Accuracy: 98.44%\n",
            "Epoch [81/250], Step [250/263], Loss: 0.0340, Accuracy: 98.75%\n",
            "Epoch [81/250], Step [260/263], Loss: 0.0194, Accuracy: 99.69%\n",
            "Epoch [82/250], Step [10/263], Loss: 0.0214, Accuracy: 99.22%\n",
            "Epoch [82/250], Step [20/263], Loss: 0.0174, Accuracy: 99.22%\n",
            "Epoch [82/250], Step [30/263], Loss: 0.0215, Accuracy: 99.06%\n",
            "Epoch [82/250], Step [40/263], Loss: 0.0106, Accuracy: 99.84%\n",
            "Epoch [82/250], Step [50/263], Loss: 0.0271, Accuracy: 99.53%\n",
            "Epoch [82/250], Step [60/263], Loss: 0.0219, Accuracy: 99.38%\n",
            "Epoch [82/250], Step [70/263], Loss: 0.0143, Accuracy: 99.69%\n",
            "Epoch [82/250], Step [80/263], Loss: 0.0305, Accuracy: 99.06%\n",
            "Epoch [82/250], Step [90/263], Loss: 0.0146, Accuracy: 99.38%\n",
            "Epoch [82/250], Step [100/263], Loss: 0.0327, Accuracy: 98.91%\n",
            "Epoch [82/250], Step [110/263], Loss: 0.0286, Accuracy: 99.22%\n",
            "Epoch [82/250], Step [120/263], Loss: 0.0228, Accuracy: 98.91%\n",
            "Epoch [82/250], Step [130/263], Loss: 0.0130, Accuracy: 100.00%\n",
            "Epoch [82/250], Step [140/263], Loss: 0.0221, Accuracy: 99.22%\n",
            "Epoch [82/250], Step [150/263], Loss: 0.0247, Accuracy: 98.91%\n",
            "Epoch [82/250], Step [160/263], Loss: 0.0239, Accuracy: 98.91%\n",
            "Epoch [82/250], Step [170/263], Loss: 0.0185, Accuracy: 99.38%\n",
            "Epoch [82/250], Step [180/263], Loss: 0.0195, Accuracy: 99.22%\n",
            "Epoch [82/250], Step [190/263], Loss: 0.0138, Accuracy: 99.38%\n",
            "Epoch [82/250], Step [200/263], Loss: 0.0134, Accuracy: 99.84%\n",
            "Epoch [82/250], Step [210/263], Loss: 0.0164, Accuracy: 99.38%\n",
            "Epoch [82/250], Step [220/263], Loss: 0.0171, Accuracy: 99.53%\n",
            "Epoch [82/250], Step [230/263], Loss: 0.0162, Accuracy: 99.53%\n",
            "Epoch [82/250], Step [240/263], Loss: 0.0190, Accuracy: 99.38%\n",
            "Epoch [82/250], Step [250/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [82/250], Step [260/263], Loss: 0.0327, Accuracy: 98.44%\n",
            "Epoch [83/250], Step [10/263], Loss: 0.0156, Accuracy: 99.53%\n",
            "Epoch [83/250], Step [20/263], Loss: 0.0188, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [30/263], Loss: 0.0160, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [40/263], Loss: 0.0079, Accuracy: 99.84%\n",
            "Epoch [83/250], Step [50/263], Loss: 0.0088, Accuracy: 99.69%\n",
            "Epoch [83/250], Step [60/263], Loss: 0.0067, Accuracy: 99.69%\n",
            "Epoch [83/250], Step [70/263], Loss: 0.0071, Accuracy: 99.53%\n",
            "Epoch [83/250], Step [80/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [83/250], Step [90/263], Loss: 0.0156, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [100/263], Loss: 0.0083, Accuracy: 99.84%\n",
            "Epoch [83/250], Step [110/263], Loss: 0.0132, Accuracy: 99.22%\n",
            "Epoch [83/250], Step [120/263], Loss: 0.0176, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [130/263], Loss: 0.0113, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [140/263], Loss: 0.0072, Accuracy: 99.69%\n",
            "Epoch [83/250], Step [150/263], Loss: 0.0234, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [160/263], Loss: 0.0294, Accuracy: 99.53%\n",
            "Epoch [83/250], Step [170/263], Loss: 0.0241, Accuracy: 99.22%\n",
            "Epoch [83/250], Step [180/263], Loss: 0.0159, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [190/263], Loss: 0.0379, Accuracy: 99.06%\n",
            "Epoch [83/250], Step [200/263], Loss: 0.0184, Accuracy: 99.69%\n",
            "Epoch [83/250], Step [210/263], Loss: 0.0156, Accuracy: 99.22%\n",
            "Epoch [83/250], Step [220/263], Loss: 0.0256, Accuracy: 98.91%\n",
            "Epoch [83/250], Step [230/263], Loss: 0.0294, Accuracy: 98.91%\n",
            "Epoch [83/250], Step [240/263], Loss: 0.0294, Accuracy: 99.38%\n",
            "Epoch [83/250], Step [250/263], Loss: 0.0339, Accuracy: 99.22%\n",
            "Epoch [83/250], Step [260/263], Loss: 0.0374, Accuracy: 98.44%\n",
            "Epoch [84/250], Step [10/263], Loss: 0.0363, Accuracy: 98.91%\n",
            "Epoch [84/250], Step [20/263], Loss: 0.0206, Accuracy: 99.38%\n",
            "Epoch [84/250], Step [30/263], Loss: 0.0322, Accuracy: 98.59%\n",
            "Epoch [84/250], Step [40/263], Loss: 0.0335, Accuracy: 98.91%\n",
            "Epoch [84/250], Step [50/263], Loss: 0.0580, Accuracy: 97.97%\n",
            "Epoch [84/250], Step [60/263], Loss: 0.0437, Accuracy: 98.44%\n",
            "Epoch [84/250], Step [70/263], Loss: 0.0334, Accuracy: 98.75%\n",
            "Epoch [84/250], Step [80/263], Loss: 0.0356, Accuracy: 99.06%\n",
            "Epoch [84/250], Step [90/263], Loss: 0.0313, Accuracy: 99.22%\n",
            "Epoch [84/250], Step [100/263], Loss: 0.0340, Accuracy: 98.75%\n",
            "Epoch [84/250], Step [110/263], Loss: 0.0393, Accuracy: 99.06%\n",
            "Epoch [84/250], Step [120/263], Loss: 0.0389, Accuracy: 98.91%\n",
            "Epoch [84/250], Step [130/263], Loss: 0.0561, Accuracy: 97.66%\n",
            "Epoch [84/250], Step [140/263], Loss: 0.0244, Accuracy: 99.22%\n",
            "Epoch [84/250], Step [150/263], Loss: 0.0176, Accuracy: 99.38%\n",
            "Epoch [84/250], Step [160/263], Loss: 0.0150, Accuracy: 99.38%\n",
            "Epoch [84/250], Step [170/263], Loss: 0.0157, Accuracy: 99.53%\n",
            "Epoch [84/250], Step [180/263], Loss: 0.0145, Accuracy: 99.84%\n",
            "Epoch [84/250], Step [190/263], Loss: 0.0270, Accuracy: 99.06%\n",
            "Epoch [84/250], Step [200/263], Loss: 0.0274, Accuracy: 99.22%\n",
            "Epoch [84/250], Step [210/263], Loss: 0.0351, Accuracy: 98.59%\n",
            "Epoch [84/250], Step [220/263], Loss: 0.0243, Accuracy: 99.06%\n",
            "Epoch [84/250], Step [230/263], Loss: 0.0179, Accuracy: 99.69%\n",
            "Epoch [84/250], Step [240/263], Loss: 0.0493, Accuracy: 98.44%\n",
            "Epoch [84/250], Step [250/263], Loss: 0.0223, Accuracy: 99.22%\n",
            "Epoch [84/250], Step [260/263], Loss: 0.0316, Accuracy: 98.75%\n",
            "Epoch [85/250], Step [10/263], Loss: 0.0436, Accuracy: 98.44%\n",
            "Epoch [85/250], Step [20/263], Loss: 0.0264, Accuracy: 98.59%\n",
            "Epoch [85/250], Step [30/263], Loss: 0.0416, Accuracy: 98.59%\n",
            "Epoch [85/250], Step [40/263], Loss: 0.0367, Accuracy: 98.59%\n",
            "Epoch [85/250], Step [50/263], Loss: 0.0263, Accuracy: 99.22%\n",
            "Epoch [85/250], Step [60/263], Loss: 0.0240, Accuracy: 99.69%\n",
            "Epoch [85/250], Step [70/263], Loss: 0.0121, Accuracy: 99.84%\n",
            "Epoch [85/250], Step [80/263], Loss: 0.0105, Accuracy: 99.69%\n",
            "Epoch [85/250], Step [90/263], Loss: 0.0124, Accuracy: 99.69%\n",
            "Epoch [85/250], Step [100/263], Loss: 0.0124, Accuracy: 99.53%\n",
            "Epoch [85/250], Step [110/263], Loss: 0.0180, Accuracy: 99.06%\n",
            "Epoch [85/250], Step [120/263], Loss: 0.0066, Accuracy: 99.69%\n",
            "Epoch [85/250], Step [130/263], Loss: 0.0089, Accuracy: 99.84%\n",
            "Epoch [85/250], Step [140/263], Loss: 0.0091, Accuracy: 99.84%\n",
            "Epoch [85/250], Step [150/263], Loss: 0.0202, Accuracy: 99.53%\n",
            "Epoch [85/250], Step [160/263], Loss: 0.0098, Accuracy: 99.69%\n",
            "Epoch [85/250], Step [170/263], Loss: 0.0186, Accuracy: 99.22%\n",
            "Epoch [85/250], Step [180/263], Loss: 0.0262, Accuracy: 98.75%\n",
            "Epoch [85/250], Step [190/263], Loss: 0.0202, Accuracy: 99.38%\n",
            "Epoch [85/250], Step [200/263], Loss: 0.0198, Accuracy: 99.22%\n",
            "Epoch [85/250], Step [210/263], Loss: 0.0148, Accuracy: 99.53%\n",
            "Epoch [85/250], Step [220/263], Loss: 0.0339, Accuracy: 99.38%\n",
            "Epoch [85/250], Step [230/263], Loss: 0.0134, Accuracy: 99.69%\n",
            "Epoch [85/250], Step [240/263], Loss: 0.0108, Accuracy: 99.53%\n",
            "Epoch [85/250], Step [250/263], Loss: 0.0224, Accuracy: 98.91%\n",
            "Epoch [85/250], Step [260/263], Loss: 0.0145, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [10/263], Loss: 0.0131, Accuracy: 99.69%\n",
            "Epoch [86/250], Step [20/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [86/250], Step [30/263], Loss: 0.0281, Accuracy: 99.22%\n",
            "Epoch [86/250], Step [40/263], Loss: 0.0279, Accuracy: 99.38%\n",
            "Epoch [86/250], Step [50/263], Loss: 0.0151, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [60/263], Loss: 0.0103, Accuracy: 99.84%\n",
            "Epoch [86/250], Step [70/263], Loss: 0.0186, Accuracy: 99.22%\n",
            "Epoch [86/250], Step [80/263], Loss: 0.0193, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [90/263], Loss: 0.0192, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [100/263], Loss: 0.0107, Accuracy: 99.69%\n",
            "Epoch [86/250], Step [110/263], Loss: 0.0090, Accuracy: 99.84%\n",
            "Epoch [86/250], Step [120/263], Loss: 0.0135, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [130/263], Loss: 0.0157, Accuracy: 99.22%\n",
            "Epoch [86/250], Step [140/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [150/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [160/263], Loss: 0.0090, Accuracy: 99.69%\n",
            "Epoch [86/250], Step [170/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [86/250], Step [180/263], Loss: 0.0061, Accuracy: 99.84%\n",
            "Epoch [86/250], Step [190/263], Loss: 0.0143, Accuracy: 99.38%\n",
            "Epoch [86/250], Step [200/263], Loss: 0.0067, Accuracy: 100.00%\n",
            "Epoch [86/250], Step [210/263], Loss: 0.0048, Accuracy: 100.00%\n",
            "Epoch [86/250], Step [220/263], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [86/250], Step [230/263], Loss: 0.0136, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [240/263], Loss: 0.0063, Accuracy: 99.69%\n",
            "Epoch [86/250], Step [250/263], Loss: 0.0198, Accuracy: 99.53%\n",
            "Epoch [86/250], Step [260/263], Loss: 0.0224, Accuracy: 99.22%\n",
            "Epoch [87/250], Step [10/263], Loss: 0.0192, Accuracy: 99.06%\n",
            "Epoch [87/250], Step [20/263], Loss: 0.0100, Accuracy: 99.69%\n",
            "Epoch [87/250], Step [30/263], Loss: 0.0077, Accuracy: 99.84%\n",
            "Epoch [87/250], Step [40/263], Loss: 0.0100, Accuracy: 99.69%\n",
            "Epoch [87/250], Step [50/263], Loss: 0.0293, Accuracy: 99.38%\n",
            "Epoch [87/250], Step [60/263], Loss: 0.0122, Accuracy: 99.53%\n",
            "Epoch [87/250], Step [70/263], Loss: 0.0098, Accuracy: 99.69%\n",
            "Epoch [87/250], Step [80/263], Loss: 0.0138, Accuracy: 99.69%\n",
            "Epoch [87/250], Step [90/263], Loss: 0.0158, Accuracy: 99.53%\n",
            "Epoch [87/250], Step [100/263], Loss: 0.0098, Accuracy: 99.69%\n",
            "Epoch [87/250], Step [110/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [87/250], Step [120/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [87/250], Step [130/263], Loss: 0.0217, Accuracy: 99.38%\n",
            "Epoch [87/250], Step [140/263], Loss: 0.0098, Accuracy: 99.53%\n",
            "Epoch [87/250], Step [150/263], Loss: 0.0166, Accuracy: 99.38%\n",
            "Epoch [87/250], Step [160/263], Loss: 0.0567, Accuracy: 98.91%\n",
            "Epoch [87/250], Step [170/263], Loss: 0.0436, Accuracy: 99.53%\n",
            "Epoch [87/250], Step [180/263], Loss: 0.0299, Accuracy: 99.22%\n",
            "Epoch [87/250], Step [190/263], Loss: 0.0365, Accuracy: 98.75%\n",
            "Epoch [87/250], Step [200/263], Loss: 0.0366, Accuracy: 98.91%\n",
            "Epoch [87/250], Step [210/263], Loss: 0.0195, Accuracy: 99.22%\n",
            "Epoch [87/250], Step [220/263], Loss: 0.0326, Accuracy: 99.06%\n",
            "Epoch [87/250], Step [230/263], Loss: 0.0153, Accuracy: 99.53%\n",
            "Epoch [87/250], Step [240/263], Loss: 0.0157, Accuracy: 99.38%\n",
            "Epoch [87/250], Step [250/263], Loss: 0.0205, Accuracy: 99.38%\n",
            "Epoch [87/250], Step [260/263], Loss: 0.0155, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [10/263], Loss: 0.0105, Accuracy: 99.53%\n",
            "Epoch [88/250], Step [20/263], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [88/250], Step [30/263], Loss: 0.0242, Accuracy: 99.38%\n",
            "Epoch [88/250], Step [40/263], Loss: 0.0140, Accuracy: 99.38%\n",
            "Epoch [88/250], Step [50/263], Loss: 0.0089, Accuracy: 99.84%\n",
            "Epoch [88/250], Step [60/263], Loss: 0.0129, Accuracy: 99.84%\n",
            "Epoch [88/250], Step [70/263], Loss: 0.0138, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [80/263], Loss: 0.0084, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [90/263], Loss: 0.0098, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [100/263], Loss: 0.0137, Accuracy: 99.38%\n",
            "Epoch [88/250], Step [110/263], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [88/250], Step [120/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [88/250], Step [130/263], Loss: 0.0256, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [140/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [88/250], Step [150/263], Loss: 0.0116, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [160/263], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [88/250], Step [170/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [88/250], Step [180/263], Loss: 0.0089, Accuracy: 99.84%\n",
            "Epoch [88/250], Step [190/263], Loss: 0.0080, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [200/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [88/250], Step [210/263], Loss: 0.0121, Accuracy: 99.53%\n",
            "Epoch [88/250], Step [220/263], Loss: 0.0072, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [230/263], Loss: 0.0061, Accuracy: 99.69%\n",
            "Epoch [88/250], Step [240/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [88/250], Step [250/263], Loss: 0.0050, Accuracy: 99.84%\n",
            "Epoch [88/250], Step [260/263], Loss: 0.0072, Accuracy: 99.69%\n",
            "Epoch [89/250], Step [10/263], Loss: 0.0167, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [20/263], Loss: 0.0083, Accuracy: 99.53%\n",
            "Epoch [89/250], Step [30/263], Loss: 0.0043, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [40/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [50/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [60/263], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [70/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [80/263], Loss: 0.0106, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [90/263], Loss: 0.0038, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [100/263], Loss: 0.0056, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [110/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [120/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [130/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [140/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [150/263], Loss: 0.0062, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [160/263], Loss: 0.0056, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [170/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [180/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [190/263], Loss: 0.0054, Accuracy: 99.69%\n",
            "Epoch [89/250], Step [200/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [210/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [220/263], Loss: 0.0087, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [230/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [240/263], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [89/250], Step [250/263], Loss: 0.0090, Accuracy: 99.84%\n",
            "Epoch [89/250], Step [260/263], Loss: 0.0055, Accuracy: 99.84%\n",
            "Epoch [90/250], Step [10/263], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [90/250], Step [20/263], Loss: 0.0071, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [30/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [90/250], Step [40/263], Loss: 0.0058, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [50/263], Loss: 0.0051, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [60/263], Loss: 0.0081, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [70/263], Loss: 0.0129, Accuracy: 99.53%\n",
            "Epoch [90/250], Step [80/263], Loss: 0.0055, Accuracy: 100.00%\n",
            "Epoch [90/250], Step [90/263], Loss: 0.0094, Accuracy: 99.84%\n",
            "Epoch [90/250], Step [100/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [90/250], Step [110/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [90/250], Step [120/263], Loss: 0.0136, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [130/263], Loss: 0.0112, Accuracy: 99.38%\n",
            "Epoch [90/250], Step [140/263], Loss: 0.0105, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [150/263], Loss: 0.0216, Accuracy: 99.38%\n",
            "Epoch [90/250], Step [160/263], Loss: 0.0078, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [170/263], Loss: 0.0086, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [180/263], Loss: 0.0116, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [190/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [90/250], Step [200/263], Loss: 0.0124, Accuracy: 99.53%\n",
            "Epoch [90/250], Step [210/263], Loss: 0.0195, Accuracy: 99.06%\n",
            "Epoch [90/250], Step [220/263], Loss: 0.0544, Accuracy: 98.28%\n",
            "Epoch [90/250], Step [230/263], Loss: 0.0458, Accuracy: 98.75%\n",
            "Epoch [90/250], Step [240/263], Loss: 0.0370, Accuracy: 98.75%\n",
            "Epoch [90/250], Step [250/263], Loss: 0.0334, Accuracy: 99.06%\n",
            "Epoch [90/250], Step [260/263], Loss: 0.0651, Accuracy: 98.59%\n",
            "Epoch [91/250], Step [10/263], Loss: 0.0339, Accuracy: 98.75%\n",
            "Epoch [91/250], Step [20/263], Loss: 0.0169, Accuracy: 99.38%\n",
            "Epoch [91/250], Step [30/263], Loss: 0.0142, Accuracy: 99.38%\n",
            "Epoch [91/250], Step [40/263], Loss: 0.0454, Accuracy: 98.91%\n",
            "Epoch [91/250], Step [50/263], Loss: 0.0208, Accuracy: 99.38%\n",
            "Epoch [91/250], Step [60/263], Loss: 0.0166, Accuracy: 99.69%\n",
            "Epoch [91/250], Step [70/263], Loss: 0.0104, Accuracy: 99.53%\n",
            "Epoch [91/250], Step [80/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [91/250], Step [90/263], Loss: 0.0101, Accuracy: 99.84%\n",
            "Epoch [91/250], Step [100/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [91/250], Step [110/263], Loss: 0.0390, Accuracy: 98.91%\n",
            "Epoch [91/250], Step [120/263], Loss: 0.0209, Accuracy: 98.91%\n",
            "Epoch [91/250], Step [130/263], Loss: 0.0146, Accuracy: 99.69%\n",
            "Epoch [91/250], Step [140/263], Loss: 0.0235, Accuracy: 99.06%\n",
            "Epoch [91/250], Step [150/263], Loss: 0.0395, Accuracy: 98.75%\n",
            "Epoch [91/250], Step [160/263], Loss: 0.0274, Accuracy: 99.06%\n",
            "Epoch [91/250], Step [170/263], Loss: 0.0415, Accuracy: 98.44%\n",
            "Epoch [91/250], Step [180/263], Loss: 0.0223, Accuracy: 99.22%\n",
            "Epoch [91/250], Step [190/263], Loss: 0.0189, Accuracy: 99.38%\n",
            "Epoch [91/250], Step [200/263], Loss: 0.0392, Accuracy: 98.75%\n",
            "Epoch [91/250], Step [210/263], Loss: 0.0264, Accuracy: 99.06%\n",
            "Epoch [91/250], Step [220/263], Loss: 0.0105, Accuracy: 99.84%\n",
            "Epoch [91/250], Step [230/263], Loss: 0.0215, Accuracy: 99.38%\n",
            "Epoch [91/250], Step [240/263], Loss: 0.0301, Accuracy: 99.06%\n",
            "Epoch [91/250], Step [250/263], Loss: 0.0413, Accuracy: 98.44%\n",
            "Epoch [91/250], Step [260/263], Loss: 0.0293, Accuracy: 98.75%\n",
            "Epoch [92/250], Step [10/263], Loss: 0.0259, Accuracy: 98.91%\n",
            "Epoch [92/250], Step [20/263], Loss: 0.0257, Accuracy: 98.91%\n",
            "Epoch [92/250], Step [30/263], Loss: 0.0277, Accuracy: 99.22%\n",
            "Epoch [92/250], Step [40/263], Loss: 0.0276, Accuracy: 99.06%\n",
            "Epoch [92/250], Step [50/263], Loss: 0.0426, Accuracy: 98.91%\n",
            "Epoch [92/250], Step [60/263], Loss: 0.0235, Accuracy: 99.22%\n",
            "Epoch [92/250], Step [70/263], Loss: 0.0327, Accuracy: 98.75%\n",
            "Epoch [92/250], Step [80/263], Loss: 0.0279, Accuracy: 99.06%\n",
            "Epoch [92/250], Step [90/263], Loss: 0.0310, Accuracy: 98.75%\n",
            "Epoch [92/250], Step [100/263], Loss: 0.0288, Accuracy: 99.06%\n",
            "Epoch [92/250], Step [110/263], Loss: 0.0198, Accuracy: 99.06%\n",
            "Epoch [92/250], Step [120/263], Loss: 0.0271, Accuracy: 99.06%\n",
            "Epoch [92/250], Step [130/263], Loss: 0.0371, Accuracy: 98.75%\n",
            "Epoch [92/250], Step [140/263], Loss: 0.0148, Accuracy: 99.53%\n",
            "Epoch [92/250], Step [150/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [92/250], Step [160/263], Loss: 0.0145, Accuracy: 99.38%\n",
            "Epoch [92/250], Step [170/263], Loss: 0.0162, Accuracy: 99.84%\n",
            "Epoch [92/250], Step [180/263], Loss: 0.0144, Accuracy: 99.38%\n",
            "Epoch [92/250], Step [190/263], Loss: 0.0207, Accuracy: 99.38%\n",
            "Epoch [92/250], Step [200/263], Loss: 0.0203, Accuracy: 99.53%\n",
            "Epoch [92/250], Step [210/263], Loss: 0.0176, Accuracy: 99.53%\n",
            "Epoch [92/250], Step [220/263], Loss: 0.0430, Accuracy: 98.44%\n",
            "Epoch [92/250], Step [230/263], Loss: 0.0215, Accuracy: 98.91%\n",
            "Epoch [92/250], Step [240/263], Loss: 0.0199, Accuracy: 99.38%\n",
            "Epoch [92/250], Step [250/263], Loss: 0.0265, Accuracy: 98.75%\n",
            "Epoch [92/250], Step [260/263], Loss: 0.0449, Accuracy: 98.91%\n",
            "Epoch [93/250], Step [10/263], Loss: 0.0269, Accuracy: 98.91%\n",
            "Epoch [93/250], Step [20/263], Loss: 0.0247, Accuracy: 99.06%\n",
            "Epoch [93/250], Step [30/263], Loss: 0.0060, Accuracy: 99.84%\n",
            "Epoch [93/250], Step [40/263], Loss: 0.0151, Accuracy: 99.69%\n",
            "Epoch [93/250], Step [50/263], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [93/250], Step [60/263], Loss: 0.0083, Accuracy: 99.84%\n",
            "Epoch [93/250], Step [70/263], Loss: 0.0149, Accuracy: 99.53%\n",
            "Epoch [93/250], Step [80/263], Loss: 0.0120, Accuracy: 99.69%\n",
            "Epoch [93/250], Step [90/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [93/250], Step [100/263], Loss: 0.0134, Accuracy: 99.53%\n",
            "Epoch [93/250], Step [110/263], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [93/250], Step [120/263], Loss: 0.0137, Accuracy: 99.53%\n",
            "Epoch [93/250], Step [130/263], Loss: 0.0047, Accuracy: 100.00%\n",
            "Epoch [93/250], Step [140/263], Loss: 0.0099, Accuracy: 99.84%\n",
            "Epoch [93/250], Step [150/263], Loss: 0.0088, Accuracy: 99.69%\n",
            "Epoch [93/250], Step [160/263], Loss: 0.0074, Accuracy: 100.00%\n",
            "Epoch [93/250], Step [170/263], Loss: 0.0059, Accuracy: 99.84%\n",
            "Epoch [93/250], Step [180/263], Loss: 0.0077, Accuracy: 99.69%\n",
            "Epoch [93/250], Step [190/263], Loss: 0.0080, Accuracy: 99.84%\n",
            "Epoch [93/250], Step [200/263], Loss: 0.0086, Accuracy: 99.53%\n",
            "Epoch [93/250], Step [210/263], Loss: 0.0123, Accuracy: 99.38%\n",
            "Epoch [93/250], Step [220/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [93/250], Step [230/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [93/250], Step [240/263], Loss: 0.0063, Accuracy: 99.69%\n",
            "Epoch [93/250], Step [250/263], Loss: 0.0058, Accuracy: 99.69%\n",
            "Epoch [93/250], Step [260/263], Loss: 0.0068, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [10/263], Loss: 0.0065, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [20/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [30/263], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [40/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [50/263], Loss: 0.0031, Accuracy: 99.84%\n",
            "Epoch [94/250], Step [60/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [70/263], Loss: 0.0079, Accuracy: 99.84%\n",
            "Epoch [94/250], Step [80/263], Loss: 0.0084, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [90/263], Loss: 0.0083, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [100/263], Loss: 0.0060, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [110/263], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [120/263], Loss: 0.0028, Accuracy: 99.84%\n",
            "Epoch [94/250], Step [130/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [140/263], Loss: 0.0043, Accuracy: 99.84%\n",
            "Epoch [94/250], Step [150/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [160/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [170/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [180/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [94/250], Step [190/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [200/263], Loss: 0.0121, Accuracy: 99.84%\n",
            "Epoch [94/250], Step [210/263], Loss: 0.0023, Accuracy: 99.84%\n",
            "Epoch [94/250], Step [220/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [94/250], Step [230/263], Loss: 0.0041, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [240/263], Loss: 0.0123, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [250/263], Loss: 0.0117, Accuracy: 99.69%\n",
            "Epoch [94/250], Step [260/263], Loss: 0.0143, Accuracy: 99.53%\n",
            "Epoch [95/250], Step [10/263], Loss: 0.0095, Accuracy: 99.69%\n",
            "Epoch [95/250], Step [20/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [95/250], Step [30/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [95/250], Step [40/263], Loss: 0.0087, Accuracy: 99.69%\n",
            "Epoch [95/250], Step [50/263], Loss: 0.0085, Accuracy: 99.84%\n",
            "Epoch [95/250], Step [60/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [95/250], Step [70/263], Loss: 0.0079, Accuracy: 99.69%\n",
            "Epoch [95/250], Step [80/263], Loss: 0.0068, Accuracy: 99.69%\n",
            "Epoch [95/250], Step [90/263], Loss: 0.0085, Accuracy: 99.53%\n",
            "Epoch [95/250], Step [100/263], Loss: 0.0160, Accuracy: 99.53%\n",
            "Epoch [95/250], Step [110/263], Loss: 0.0031, Accuracy: 99.84%\n",
            "Epoch [95/250], Step [120/263], Loss: 0.0200, Accuracy: 99.22%\n",
            "Epoch [95/250], Step [130/263], Loss: 0.0218, Accuracy: 99.38%\n",
            "Epoch [95/250], Step [140/263], Loss: 0.0246, Accuracy: 99.22%\n",
            "Epoch [95/250], Step [150/263], Loss: 0.0213, Accuracy: 99.38%\n",
            "Epoch [95/250], Step [160/263], Loss: 0.0230, Accuracy: 99.22%\n",
            "Epoch [95/250], Step [170/263], Loss: 0.0161, Accuracy: 99.22%\n",
            "Epoch [95/250], Step [180/263], Loss: 0.0168, Accuracy: 99.38%\n",
            "Epoch [95/250], Step [190/263], Loss: 0.0136, Accuracy: 99.38%\n",
            "Epoch [95/250], Step [200/263], Loss: 0.0250, Accuracy: 99.22%\n",
            "Epoch [95/250], Step [210/263], Loss: 0.0080, Accuracy: 99.84%\n",
            "Epoch [95/250], Step [220/263], Loss: 0.0224, Accuracy: 99.53%\n",
            "Epoch [95/250], Step [230/263], Loss: 0.0183, Accuracy: 99.06%\n",
            "Epoch [95/250], Step [240/263], Loss: 0.0330, Accuracy: 99.38%\n",
            "Epoch [95/250], Step [250/263], Loss: 0.0745, Accuracy: 98.91%\n",
            "Epoch [95/250], Step [260/263], Loss: 0.0359, Accuracy: 99.06%\n",
            "Epoch [96/250], Step [10/263], Loss: 0.0247, Accuracy: 99.06%\n",
            "Epoch [96/250], Step [20/263], Loss: 0.0196, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [30/263], Loss: 0.0289, Accuracy: 99.22%\n",
            "Epoch [96/250], Step [40/263], Loss: 0.0228, Accuracy: 99.22%\n",
            "Epoch [96/250], Step [50/263], Loss: 0.0169, Accuracy: 99.53%\n",
            "Epoch [96/250], Step [60/263], Loss: 0.0305, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [70/263], Loss: 0.0201, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [80/263], Loss: 0.0184, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [90/263], Loss: 0.0125, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [100/263], Loss: 0.0217, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [110/263], Loss: 0.0109, Accuracy: 99.84%\n",
            "Epoch [96/250], Step [120/263], Loss: 0.0169, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [130/263], Loss: 0.0072, Accuracy: 100.00%\n",
            "Epoch [96/250], Step [140/263], Loss: 0.0196, Accuracy: 99.22%\n",
            "Epoch [96/250], Step [150/263], Loss: 0.0119, Accuracy: 99.53%\n",
            "Epoch [96/250], Step [160/263], Loss: 0.0082, Accuracy: 99.84%\n",
            "Epoch [96/250], Step [170/263], Loss: 0.0384, Accuracy: 99.69%\n",
            "Epoch [96/250], Step [180/263], Loss: 0.0119, Accuracy: 99.53%\n",
            "Epoch [96/250], Step [190/263], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [96/250], Step [200/263], Loss: 0.0059, Accuracy: 100.00%\n",
            "Epoch [96/250], Step [210/263], Loss: 0.0084, Accuracy: 99.53%\n",
            "Epoch [96/250], Step [220/263], Loss: 0.0149, Accuracy: 99.53%\n",
            "Epoch [96/250], Step [230/263], Loss: 0.0118, Accuracy: 99.38%\n",
            "Epoch [96/250], Step [240/263], Loss: 0.0161, Accuracy: 99.53%\n",
            "Epoch [96/250], Step [250/263], Loss: 0.0088, Accuracy: 99.69%\n",
            "Epoch [96/250], Step [260/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [97/250], Step [10/263], Loss: 0.0093, Accuracy: 99.38%\n",
            "Epoch [97/250], Step [20/263], Loss: 0.0120, Accuracy: 99.53%\n",
            "Epoch [97/250], Step [30/263], Loss: 0.0094, Accuracy: 99.84%\n",
            "Epoch [97/250], Step [40/263], Loss: 0.0079, Accuracy: 99.69%\n",
            "Epoch [97/250], Step [50/263], Loss: 0.0223, Accuracy: 99.22%\n",
            "Epoch [97/250], Step [60/263], Loss: 0.0236, Accuracy: 99.38%\n",
            "Epoch [97/250], Step [70/263], Loss: 0.0140, Accuracy: 99.53%\n",
            "Epoch [97/250], Step [80/263], Loss: 0.0261, Accuracy: 99.22%\n",
            "Epoch [97/250], Step [90/263], Loss: 0.0226, Accuracy: 99.38%\n",
            "Epoch [97/250], Step [100/263], Loss: 0.0118, Accuracy: 99.53%\n",
            "Epoch [97/250], Step [110/263], Loss: 0.0343, Accuracy: 99.22%\n",
            "Epoch [97/250], Step [120/263], Loss: 0.0420, Accuracy: 98.59%\n",
            "Epoch [97/250], Step [130/263], Loss: 0.0282, Accuracy: 98.91%\n",
            "Epoch [97/250], Step [140/263], Loss: 0.0355, Accuracy: 98.91%\n",
            "Epoch [97/250], Step [150/263], Loss: 0.0454, Accuracy: 98.91%\n",
            "Epoch [97/250], Step [160/263], Loss: 0.0093, Accuracy: 99.53%\n",
            "Epoch [97/250], Step [170/263], Loss: 0.0080, Accuracy: 99.69%\n",
            "Epoch [97/250], Step [180/263], Loss: 0.0145, Accuracy: 99.38%\n",
            "Epoch [97/250], Step [190/263], Loss: 0.0736, Accuracy: 99.06%\n",
            "Epoch [97/250], Step [200/263], Loss: 0.0431, Accuracy: 99.06%\n",
            "Epoch [97/250], Step [210/263], Loss: 0.0314, Accuracy: 98.44%\n",
            "Epoch [97/250], Step [220/263], Loss: 0.0403, Accuracy: 98.75%\n",
            "Epoch [97/250], Step [230/263], Loss: 0.0470, Accuracy: 98.75%\n",
            "Epoch [97/250], Step [240/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [97/250], Step [250/263], Loss: 0.0141, Accuracy: 99.69%\n",
            "Epoch [97/250], Step [260/263], Loss: 0.0124, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [10/263], Loss: 0.0146, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [20/263], Loss: 0.0154, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [30/263], Loss: 0.0333, Accuracy: 99.06%\n",
            "Epoch [98/250], Step [40/263], Loss: 0.0423, Accuracy: 98.59%\n",
            "Epoch [98/250], Step [50/263], Loss: 0.0194, Accuracy: 99.06%\n",
            "Epoch [98/250], Step [60/263], Loss: 0.0163, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [70/263], Loss: 0.0124, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [80/263], Loss: 0.0154, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [90/263], Loss: 0.0233, Accuracy: 99.06%\n",
            "Epoch [98/250], Step [100/263], Loss: 0.0186, Accuracy: 99.22%\n",
            "Epoch [98/250], Step [110/263], Loss: 0.0246, Accuracy: 99.06%\n",
            "Epoch [98/250], Step [120/263], Loss: 0.0123, Accuracy: 99.84%\n",
            "Epoch [98/250], Step [130/263], Loss: 0.0296, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [140/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [150/263], Loss: 0.0154, Accuracy: 99.38%\n",
            "Epoch [98/250], Step [160/263], Loss: 0.0107, Accuracy: 99.69%\n",
            "Epoch [98/250], Step [170/263], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [98/250], Step [180/263], Loss: 0.0065, Accuracy: 100.00%\n",
            "Epoch [98/250], Step [190/263], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [98/250], Step [200/263], Loss: 0.0073, Accuracy: 100.00%\n",
            "Epoch [98/250], Step [210/263], Loss: 0.0062, Accuracy: 100.00%\n",
            "Epoch [98/250], Step [220/263], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [98/250], Step [230/263], Loss: 0.0082, Accuracy: 99.69%\n",
            "Epoch [98/250], Step [240/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [98/250], Step [250/263], Loss: 0.0249, Accuracy: 99.53%\n",
            "Epoch [98/250], Step [260/263], Loss: 0.0107, Accuracy: 99.38%\n",
            "Epoch [99/250], Step [10/263], Loss: 0.0148, Accuracy: 99.53%\n",
            "Epoch [99/250], Step [20/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [30/263], Loss: 0.0093, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [40/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [50/263], Loss: 0.0082, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [60/263], Loss: 0.0103, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [70/263], Loss: 0.0063, Accuracy: 99.69%\n",
            "Epoch [99/250], Step [80/263], Loss: 0.0045, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [90/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [100/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [110/263], Loss: 0.0029, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [120/263], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [130/263], Loss: 0.0066, Accuracy: 99.69%\n",
            "Epoch [99/250], Step [140/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [150/263], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [160/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [170/263], Loss: 0.0055, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [180/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [190/263], Loss: 0.0089, Accuracy: 99.69%\n",
            "Epoch [99/250], Step [200/263], Loss: 0.0092, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [210/263], Loss: 0.0189, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [220/263], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [230/263], Loss: 0.0152, Accuracy: 99.69%\n",
            "Epoch [99/250], Step [240/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [99/250], Step [250/263], Loss: 0.0055, Accuracy: 99.84%\n",
            "Epoch [99/250], Step [260/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [100/250], Step [10/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [100/250], Step [20/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [100/250], Step [30/263], Loss: 0.0057, Accuracy: 99.69%\n",
            "Epoch [100/250], Step [40/263], Loss: 0.0021, Accuracy: 99.84%\n",
            "Epoch [100/250], Step [50/263], Loss: 0.0063, Accuracy: 99.69%\n",
            "Epoch [100/250], Step [60/263], Loss: 0.0088, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [70/263], Loss: 0.0089, Accuracy: 99.38%\n",
            "Epoch [100/250], Step [80/263], Loss: 0.0128, Accuracy: 99.69%\n",
            "Epoch [100/250], Step [90/263], Loss: 0.0128, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [100/263], Loss: 0.0085, Accuracy: 99.84%\n",
            "Epoch [100/250], Step [110/263], Loss: 0.0118, Accuracy: 99.38%\n",
            "Epoch [100/250], Step [120/263], Loss: 0.0074, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [130/263], Loss: 0.0266, Accuracy: 99.69%\n",
            "Epoch [100/250], Step [140/263], Loss: 0.0201, Accuracy: 99.38%\n",
            "Epoch [100/250], Step [150/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [100/250], Step [160/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [100/250], Step [170/263], Loss: 0.0134, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [180/263], Loss: 0.0177, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [190/263], Loss: 0.0203, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [200/263], Loss: 0.0076, Accuracy: 99.84%\n",
            "Epoch [100/250], Step [210/263], Loss: 0.0116, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [220/263], Loss: 0.0231, Accuracy: 99.69%\n",
            "Epoch [100/250], Step [230/263], Loss: 0.0152, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [240/263], Loss: 0.0146, Accuracy: 99.53%\n",
            "Epoch [100/250], Step [250/263], Loss: 0.0063, Accuracy: 99.84%\n",
            "Epoch [100/250], Step [260/263], Loss: 0.0079, Accuracy: 99.84%\n",
            "Epoch [101/250], Step [10/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [101/250], Step [20/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [101/250], Step [30/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [101/250], Step [40/263], Loss: 0.0056, Accuracy: 99.84%\n",
            "Epoch [101/250], Step [50/263], Loss: 0.0138, Accuracy: 99.69%\n",
            "Epoch [101/250], Step [60/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [101/250], Step [70/263], Loss: 0.0174, Accuracy: 99.69%\n",
            "Epoch [101/250], Step [80/263], Loss: 0.0069, Accuracy: 99.84%\n",
            "Epoch [101/250], Step [90/263], Loss: 0.0218, Accuracy: 99.53%\n",
            "Epoch [101/250], Step [100/263], Loss: 0.0105, Accuracy: 99.69%\n",
            "Epoch [101/250], Step [110/263], Loss: 0.0135, Accuracy: 99.84%\n",
            "Epoch [101/250], Step [120/263], Loss: 0.0101, Accuracy: 99.53%\n",
            "Epoch [101/250], Step [130/263], Loss: 0.0269, Accuracy: 99.22%\n",
            "Epoch [101/250], Step [140/263], Loss: 0.0112, Accuracy: 99.69%\n",
            "Epoch [101/250], Step [150/263], Loss: 0.0254, Accuracy: 99.06%\n",
            "Epoch [101/250], Step [160/263], Loss: 0.0153, Accuracy: 99.06%\n",
            "Epoch [101/250], Step [170/263], Loss: 0.0215, Accuracy: 99.38%\n",
            "Epoch [101/250], Step [180/263], Loss: 0.0205, Accuracy: 99.38%\n",
            "Epoch [101/250], Step [190/263], Loss: 0.0199, Accuracy: 99.22%\n",
            "Epoch [101/250], Step [200/263], Loss: 0.0151, Accuracy: 99.22%\n",
            "Epoch [101/250], Step [210/263], Loss: 0.0217, Accuracy: 99.38%\n",
            "Epoch [101/250], Step [220/263], Loss: 0.0272, Accuracy: 99.06%\n",
            "Epoch [101/250], Step [230/263], Loss: 0.0197, Accuracy: 99.06%\n",
            "Epoch [101/250], Step [240/263], Loss: 0.0150, Accuracy: 99.38%\n",
            "Epoch [101/250], Step [250/263], Loss: 0.0118, Accuracy: 99.38%\n",
            "Epoch [101/250], Step [260/263], Loss: 0.0064, Accuracy: 99.84%\n",
            "Epoch [102/250], Step [10/263], Loss: 0.0212, Accuracy: 99.22%\n",
            "Epoch [102/250], Step [20/263], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [102/250], Step [30/263], Loss: 0.0108, Accuracy: 99.53%\n",
            "Epoch [102/250], Step [40/263], Loss: 0.0315, Accuracy: 98.75%\n",
            "Epoch [102/250], Step [50/263], Loss: 0.0112, Accuracy: 99.53%\n",
            "Epoch [102/250], Step [60/263], Loss: 0.0107, Accuracy: 99.53%\n",
            "Epoch [102/250], Step [70/263], Loss: 0.0144, Accuracy: 99.69%\n",
            "Epoch [102/250], Step [80/263], Loss: 0.0104, Accuracy: 99.53%\n",
            "Epoch [102/250], Step [90/263], Loss: 0.0227, Accuracy: 98.91%\n",
            "Epoch [102/250], Step [100/263], Loss: 0.0186, Accuracy: 99.38%\n",
            "Epoch [102/250], Step [110/263], Loss: 0.0313, Accuracy: 98.75%\n",
            "Epoch [102/250], Step [120/263], Loss: 0.0242, Accuracy: 99.06%\n",
            "Epoch [102/250], Step [130/263], Loss: 0.0210, Accuracy: 99.53%\n",
            "Epoch [102/250], Step [140/263], Loss: 0.0505, Accuracy: 98.44%\n",
            "Epoch [102/250], Step [150/263], Loss: 0.0431, Accuracy: 98.44%\n",
            "Epoch [102/250], Step [160/263], Loss: 0.0306, Accuracy: 98.75%\n",
            "Epoch [102/250], Step [170/263], Loss: 0.0261, Accuracy: 98.91%\n",
            "Epoch [102/250], Step [180/263], Loss: 0.0295, Accuracy: 98.75%\n",
            "Epoch [102/250], Step [190/263], Loss: 0.0290, Accuracy: 99.53%\n",
            "Epoch [102/250], Step [200/263], Loss: 0.0084, Accuracy: 99.84%\n",
            "Epoch [102/250], Step [210/263], Loss: 0.0195, Accuracy: 98.91%\n",
            "Epoch [102/250], Step [220/263], Loss: 0.0199, Accuracy: 98.91%\n",
            "Epoch [102/250], Step [230/263], Loss: 0.0231, Accuracy: 99.38%\n",
            "Epoch [102/250], Step [240/263], Loss: 0.0200, Accuracy: 99.22%\n",
            "Epoch [102/250], Step [250/263], Loss: 0.0227, Accuracy: 99.38%\n",
            "Epoch [102/250], Step [260/263], Loss: 0.0153, Accuracy: 99.38%\n",
            "Epoch [103/250], Step [10/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [103/250], Step [20/263], Loss: 0.0133, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [30/263], Loss: 0.0139, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [40/263], Loss: 0.0192, Accuracy: 99.38%\n",
            "Epoch [103/250], Step [50/263], Loss: 0.0121, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [60/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [103/250], Step [70/263], Loss: 0.0240, Accuracy: 99.06%\n",
            "Epoch [103/250], Step [80/263], Loss: 0.0214, Accuracy: 99.69%\n",
            "Epoch [103/250], Step [90/263], Loss: 0.0195, Accuracy: 99.38%\n",
            "Epoch [103/250], Step [100/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [110/263], Loss: 0.0125, Accuracy: 99.38%\n",
            "Epoch [103/250], Step [120/263], Loss: 0.0108, Accuracy: 99.84%\n",
            "Epoch [103/250], Step [130/263], Loss: 0.0186, Accuracy: 98.91%\n",
            "Epoch [103/250], Step [140/263], Loss: 0.0121, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [150/263], Loss: 0.0207, Accuracy: 99.38%\n",
            "Epoch [103/250], Step [160/263], Loss: 0.0176, Accuracy: 99.38%\n",
            "Epoch [103/250], Step [170/263], Loss: 0.0095, Accuracy: 99.84%\n",
            "Epoch [103/250], Step [180/263], Loss: 0.0126, Accuracy: 99.69%\n",
            "Epoch [103/250], Step [190/263], Loss: 0.0078, Accuracy: 99.84%\n",
            "Epoch [103/250], Step [200/263], Loss: 0.0068, Accuracy: 99.69%\n",
            "Epoch [103/250], Step [210/263], Loss: 0.0158, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [220/263], Loss: 0.0171, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [230/263], Loss: 0.0130, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [240/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [103/250], Step [250/263], Loss: 0.0090, Accuracy: 99.69%\n",
            "Epoch [103/250], Step [260/263], Loss: 0.0093, Accuracy: 99.53%\n",
            "Epoch [104/250], Step [10/263], Loss: 0.0070, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [20/263], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [30/263], Loss: 0.0061, Accuracy: 99.69%\n",
            "Epoch [104/250], Step [40/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [50/263], Loss: 0.0113, Accuracy: 99.38%\n",
            "Epoch [104/250], Step [60/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [70/263], Loss: 0.0046, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [80/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [90/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [100/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [110/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [120/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [130/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [140/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [150/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [160/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [170/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [180/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [190/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [200/263], Loss: 0.0027, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [210/263], Loss: 0.0028, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [220/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [230/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [104/250], Step [240/263], Loss: 0.0065, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [250/263], Loss: 0.0088, Accuracy: 99.84%\n",
            "Epoch [104/250], Step [260/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [10/263], Loss: 0.0038, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [20/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [30/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [40/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [50/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [60/263], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [70/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [80/263], Loss: 0.0060, Accuracy: 99.69%\n",
            "Epoch [105/250], Step [90/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [100/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [110/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [120/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [130/263], Loss: 0.0106, Accuracy: 99.69%\n",
            "Epoch [105/250], Step [140/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [150/263], Loss: 0.0094, Accuracy: 99.53%\n",
            "Epoch [105/250], Step [160/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [170/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [105/250], Step [180/263], Loss: 0.0036, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [190/263], Loss: 0.0104, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [200/263], Loss: 0.0050, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [210/263], Loss: 0.0080, Accuracy: 99.69%\n",
            "Epoch [105/250], Step [220/263], Loss: 0.0138, Accuracy: 99.53%\n",
            "Epoch [105/250], Step [230/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [240/263], Loss: 0.0084, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [250/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [105/250], Step [260/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [106/250], Step [10/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [106/250], Step [20/263], Loss: 0.0293, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [30/263], Loss: 0.0225, Accuracy: 99.38%\n",
            "Epoch [106/250], Step [40/263], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [106/250], Step [50/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [60/263], Loss: 0.0066, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [70/263], Loss: 0.0210, Accuracy: 99.22%\n",
            "Epoch [106/250], Step [80/263], Loss: 0.0073, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [90/263], Loss: 0.0057, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [100/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [106/250], Step [110/263], Loss: 0.0184, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [120/263], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [106/250], Step [130/263], Loss: 0.0145, Accuracy: 99.38%\n",
            "Epoch [106/250], Step [140/263], Loss: 0.0314, Accuracy: 99.53%\n",
            "Epoch [106/250], Step [150/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [160/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [170/263], Loss: 0.0094, Accuracy: 99.53%\n",
            "Epoch [106/250], Step [180/263], Loss: 0.0041, Accuracy: 99.84%\n",
            "Epoch [106/250], Step [190/263], Loss: 0.0075, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [200/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [210/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [106/250], Step [220/263], Loss: 0.0086, Accuracy: 99.53%\n",
            "Epoch [106/250], Step [230/263], Loss: 0.0086, Accuracy: 99.53%\n",
            "Epoch [106/250], Step [240/263], Loss: 0.0141, Accuracy: 99.22%\n",
            "Epoch [106/250], Step [250/263], Loss: 0.0102, Accuracy: 99.69%\n",
            "Epoch [106/250], Step [260/263], Loss: 0.0123, Accuracy: 99.38%\n",
            "Epoch [107/250], Step [10/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [107/250], Step [20/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [107/250], Step [30/263], Loss: 0.0068, Accuracy: 99.69%\n",
            "Epoch [107/250], Step [40/263], Loss: 0.0078, Accuracy: 99.69%\n",
            "Epoch [107/250], Step [50/263], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [107/250], Step [60/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [107/250], Step [70/263], Loss: 0.0082, Accuracy: 99.84%\n",
            "Epoch [107/250], Step [80/263], Loss: 0.0083, Accuracy: 99.84%\n",
            "Epoch [107/250], Step [90/263], Loss: 0.0234, Accuracy: 99.38%\n",
            "Epoch [107/250], Step [100/263], Loss: 0.0209, Accuracy: 99.38%\n",
            "Epoch [107/250], Step [110/263], Loss: 0.0412, Accuracy: 98.59%\n",
            "Epoch [107/250], Step [120/263], Loss: 0.0498, Accuracy: 98.12%\n",
            "Epoch [107/250], Step [130/263], Loss: 0.0367, Accuracy: 98.91%\n",
            "Epoch [107/250], Step [140/263], Loss: 0.0533, Accuracy: 97.66%\n",
            "Epoch [107/250], Step [150/263], Loss: 0.0389, Accuracy: 99.06%\n",
            "Epoch [107/250], Step [160/263], Loss: 0.0459, Accuracy: 98.28%\n",
            "Epoch [107/250], Step [170/263], Loss: 0.0436, Accuracy: 98.44%\n",
            "Epoch [107/250], Step [180/263], Loss: 0.0662, Accuracy: 97.81%\n",
            "Epoch [107/250], Step [190/263], Loss: 0.0802, Accuracy: 97.34%\n",
            "Epoch [107/250], Step [200/263], Loss: 0.0476, Accuracy: 98.91%\n",
            "Epoch [107/250], Step [210/263], Loss: 0.0364, Accuracy: 98.75%\n",
            "Epoch [107/250], Step [220/263], Loss: 0.0295, Accuracy: 99.06%\n",
            "Epoch [107/250], Step [230/263], Loss: 0.0282, Accuracy: 99.22%\n",
            "Epoch [107/250], Step [240/263], Loss: 0.0255, Accuracy: 99.06%\n",
            "Epoch [107/250], Step [250/263], Loss: 0.0284, Accuracy: 99.06%\n",
            "Epoch [107/250], Step [260/263], Loss: 0.0147, Accuracy: 99.38%\n",
            "Epoch [108/250], Step [10/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [108/250], Step [20/263], Loss: 0.0231, Accuracy: 99.22%\n",
            "Epoch [108/250], Step [30/263], Loss: 0.0175, Accuracy: 99.53%\n",
            "Epoch [108/250], Step [40/263], Loss: 0.0124, Accuracy: 99.69%\n",
            "Epoch [108/250], Step [50/263], Loss: 0.0149, Accuracy: 99.38%\n",
            "Epoch [108/250], Step [60/263], Loss: 0.0236, Accuracy: 99.22%\n",
            "Epoch [108/250], Step [70/263], Loss: 0.0193, Accuracy: 99.38%\n",
            "Epoch [108/250], Step [80/263], Loss: 0.0072, Accuracy: 99.69%\n",
            "Epoch [108/250], Step [90/263], Loss: 0.0104, Accuracy: 99.53%\n",
            "Epoch [108/250], Step [100/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [108/250], Step [110/263], Loss: 0.0219, Accuracy: 98.91%\n",
            "Epoch [108/250], Step [120/263], Loss: 0.0125, Accuracy: 99.38%\n",
            "Epoch [108/250], Step [130/263], Loss: 0.0107, Accuracy: 99.69%\n",
            "Epoch [108/250], Step [140/263], Loss: 0.0214, Accuracy: 99.84%\n",
            "Epoch [108/250], Step [150/263], Loss: 0.0194, Accuracy: 99.53%\n",
            "Epoch [108/250], Step [160/263], Loss: 0.0238, Accuracy: 99.06%\n",
            "Epoch [108/250], Step [170/263], Loss: 0.0105, Accuracy: 99.69%\n",
            "Epoch [108/250], Step [180/263], Loss: 0.0091, Accuracy: 99.69%\n",
            "Epoch [108/250], Step [190/263], Loss: 0.0081, Accuracy: 99.84%\n",
            "Epoch [108/250], Step [200/263], Loss: 0.0059, Accuracy: 99.84%\n",
            "Epoch [108/250], Step [210/263], Loss: 0.0098, Accuracy: 99.69%\n",
            "Epoch [108/250], Step [220/263], Loss: 0.0089, Accuracy: 99.53%\n",
            "Epoch [108/250], Step [230/263], Loss: 0.0126, Accuracy: 99.69%\n",
            "Epoch [108/250], Step [240/263], Loss: 0.0135, Accuracy: 99.38%\n",
            "Epoch [108/250], Step [250/263], Loss: 0.0123, Accuracy: 99.53%\n",
            "Epoch [108/250], Step [260/263], Loss: 0.0147, Accuracy: 99.53%\n",
            "Epoch [109/250], Step [10/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [20/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [30/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [40/263], Loss: 0.0051, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [50/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [60/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [70/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [80/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [90/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [100/263], Loss: 0.0061, Accuracy: 99.69%\n",
            "Epoch [109/250], Step [110/263], Loss: 0.0050, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [120/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [130/263], Loss: 0.0289, Accuracy: 99.69%\n",
            "Epoch [109/250], Step [140/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [109/250], Step [150/263], Loss: 0.0137, Accuracy: 99.53%\n",
            "Epoch [109/250], Step [160/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [170/263], Loss: 0.0108, Accuracy: 99.69%\n",
            "Epoch [109/250], Step [180/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [190/263], Loss: 0.0072, Accuracy: 99.69%\n",
            "Epoch [109/250], Step [200/263], Loss: 0.0140, Accuracy: 99.38%\n",
            "Epoch [109/250], Step [210/263], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [220/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [230/263], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [240/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [250/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [109/250], Step [260/263], Loss: 0.0046, Accuracy: 99.84%\n",
            "Epoch [110/250], Step [10/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [20/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [110/250], Step [30/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [40/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [50/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [110/250], Step [60/263], Loss: 0.0047, Accuracy: 99.84%\n",
            "Epoch [110/250], Step [70/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [80/263], Loss: 0.0064, Accuracy: 99.84%\n",
            "Epoch [110/250], Step [90/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [100/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [110/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [120/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [130/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [140/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [150/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [160/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [170/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [180/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [190/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [200/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [210/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [220/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [230/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [240/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [250/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [110/250], Step [260/263], Loss: 0.0022, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [10/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [20/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [40/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [50/263], Loss: 0.0024, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [60/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [70/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [80/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [90/263], Loss: 0.0016, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [100/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [110/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [120/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [130/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [140/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [150/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [160/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [170/263], Loss: 0.0032, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [190/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [200/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [210/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [220/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [230/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [111/250], Step [240/263], Loss: 0.0065, Accuracy: 99.69%\n",
            "Epoch [111/250], Step [250/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [111/250], Step [260/263], Loss: 0.0031, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [10/263], Loss: 0.0020, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [20/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [40/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [50/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [60/263], Loss: 0.0094, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [70/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [80/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [90/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [100/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [110/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [120/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [130/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [140/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [160/263], Loss: 0.0048, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [170/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [180/263], Loss: 0.0023, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [190/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [200/263], Loss: 0.0109, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [210/263], Loss: 0.0027, Accuracy: 99.84%\n",
            "Epoch [112/250], Step [220/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [230/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [240/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [250/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [112/250], Step [260/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [113/250], Step [10/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [113/250], Step [20/263], Loss: 0.0059, Accuracy: 99.84%\n",
            "Epoch [113/250], Step [30/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [40/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [50/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [60/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [70/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [80/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [90/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [100/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [110/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [120/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [140/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [150/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [170/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [113/250], Step [180/263], Loss: 0.0100, Accuracy: 99.69%\n",
            "Epoch [113/250], Step [190/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [200/263], Loss: 0.0064, Accuracy: 99.84%\n",
            "Epoch [113/250], Step [210/263], Loss: 0.0204, Accuracy: 99.69%\n",
            "Epoch [113/250], Step [220/263], Loss: 0.0207, Accuracy: 99.53%\n",
            "Epoch [113/250], Step [230/263], Loss: 0.0205, Accuracy: 99.38%\n",
            "Epoch [113/250], Step [240/263], Loss: 0.0139, Accuracy: 99.69%\n",
            "Epoch [113/250], Step [250/263], Loss: 0.0047, Accuracy: 100.00%\n",
            "Epoch [113/250], Step [260/263], Loss: 0.0078, Accuracy: 99.69%\n",
            "Epoch [114/250], Step [10/263], Loss: 0.0036, Accuracy: 99.84%\n",
            "Epoch [114/250], Step [20/263], Loss: 0.0113, Accuracy: 99.84%\n",
            "Epoch [114/250], Step [30/263], Loss: 0.0323, Accuracy: 99.53%\n",
            "Epoch [114/250], Step [40/263], Loss: 0.0177, Accuracy: 99.22%\n",
            "Epoch [114/250], Step [50/263], Loss: 0.0069, Accuracy: 99.69%\n",
            "Epoch [114/250], Step [60/263], Loss: 0.0284, Accuracy: 98.91%\n",
            "Epoch [114/250], Step [70/263], Loss: 0.0328, Accuracy: 98.91%\n",
            "Epoch [114/250], Step [80/263], Loss: 0.0420, Accuracy: 98.28%\n",
            "Epoch [114/250], Step [90/263], Loss: 0.0261, Accuracy: 98.91%\n",
            "Epoch [114/250], Step [100/263], Loss: 0.0780, Accuracy: 97.50%\n",
            "Epoch [114/250], Step [110/263], Loss: 0.0865, Accuracy: 98.44%\n",
            "Epoch [114/250], Step [120/263], Loss: 0.0902, Accuracy: 96.56%\n",
            "Epoch [114/250], Step [130/263], Loss: 0.0455, Accuracy: 98.28%\n",
            "Epoch [114/250], Step [140/263], Loss: 0.0680, Accuracy: 97.97%\n",
            "Epoch [114/250], Step [150/263], Loss: 0.0533, Accuracy: 97.97%\n",
            "Epoch [114/250], Step [160/263], Loss: 0.0396, Accuracy: 98.12%\n",
            "Epoch [114/250], Step [170/263], Loss: 0.0560, Accuracy: 98.28%\n",
            "Epoch [114/250], Step [180/263], Loss: 0.0314, Accuracy: 98.75%\n",
            "Epoch [114/250], Step [190/263], Loss: 0.0513, Accuracy: 98.12%\n",
            "Epoch [114/250], Step [200/263], Loss: 0.0203, Accuracy: 99.22%\n",
            "Epoch [114/250], Step [210/263], Loss: 0.0315, Accuracy: 98.75%\n",
            "Epoch [114/250], Step [220/263], Loss: 0.0416, Accuracy: 98.59%\n",
            "Epoch [114/250], Step [230/263], Loss: 0.0392, Accuracy: 98.75%\n",
            "Epoch [114/250], Step [240/263], Loss: 0.0409, Accuracy: 98.44%\n",
            "Epoch [114/250], Step [250/263], Loss: 0.0435, Accuracy: 98.59%\n",
            "Epoch [114/250], Step [260/263], Loss: 0.0353, Accuracy: 98.75%\n",
            "Epoch [115/250], Step [10/263], Loss: 0.0231, Accuracy: 99.22%\n",
            "Epoch [115/250], Step [20/263], Loss: 0.0168, Accuracy: 99.38%\n",
            "Epoch [115/250], Step [30/263], Loss: 0.0107, Accuracy: 99.84%\n",
            "Epoch [115/250], Step [40/263], Loss: 0.0120, Accuracy: 99.53%\n",
            "Epoch [115/250], Step [50/263], Loss: 0.0130, Accuracy: 99.53%\n",
            "Epoch [115/250], Step [60/263], Loss: 0.0208, Accuracy: 98.75%\n",
            "Epoch [115/250], Step [70/263], Loss: 0.0081, Accuracy: 99.84%\n",
            "Epoch [115/250], Step [80/263], Loss: 0.0062, Accuracy: 99.84%\n",
            "Epoch [115/250], Step [90/263], Loss: 0.0136, Accuracy: 99.53%\n",
            "Epoch [115/250], Step [100/263], Loss: 0.0190, Accuracy: 99.53%\n",
            "Epoch [115/250], Step [110/263], Loss: 0.0067, Accuracy: 99.84%\n",
            "Epoch [115/250], Step [120/263], Loss: 0.0098, Accuracy: 99.38%\n",
            "Epoch [115/250], Step [130/263], Loss: 0.0214, Accuracy: 99.69%\n",
            "Epoch [115/250], Step [140/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [115/250], Step [150/263], Loss: 0.0063, Accuracy: 99.84%\n",
            "Epoch [115/250], Step [160/263], Loss: 0.0154, Accuracy: 99.22%\n",
            "Epoch [115/250], Step [170/263], Loss: 0.0076, Accuracy: 99.69%\n",
            "Epoch [115/250], Step [180/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [115/250], Step [190/263], Loss: 0.0253, Accuracy: 99.38%\n",
            "Epoch [115/250], Step [200/263], Loss: 0.0225, Accuracy: 99.38%\n",
            "Epoch [115/250], Step [210/263], Loss: 0.0102, Accuracy: 99.69%\n",
            "Epoch [115/250], Step [220/263], Loss: 0.0271, Accuracy: 99.53%\n",
            "Epoch [115/250], Step [230/263], Loss: 0.0311, Accuracy: 99.38%\n",
            "Epoch [115/250], Step [240/263], Loss: 0.0453, Accuracy: 99.22%\n",
            "Epoch [115/250], Step [250/263], Loss: 0.0245, Accuracy: 98.75%\n",
            "Epoch [115/250], Step [260/263], Loss: 0.0156, Accuracy: 99.69%\n",
            "Epoch [116/250], Step [10/263], Loss: 0.0109, Accuracy: 99.69%\n",
            "Epoch [116/250], Step [20/263], Loss: 0.0411, Accuracy: 99.38%\n",
            "Epoch [116/250], Step [30/263], Loss: 0.0174, Accuracy: 99.53%\n",
            "Epoch [116/250], Step [40/263], Loss: 0.0790, Accuracy: 98.44%\n",
            "Epoch [116/250], Step [50/263], Loss: 0.0323, Accuracy: 98.91%\n",
            "Epoch [116/250], Step [60/263], Loss: 0.0134, Accuracy: 99.84%\n",
            "Epoch [116/250], Step [70/263], Loss: 0.0238, Accuracy: 99.38%\n",
            "Epoch [116/250], Step [80/263], Loss: 0.0143, Accuracy: 99.22%\n",
            "Epoch [116/250], Step [90/263], Loss: 0.0357, Accuracy: 98.91%\n",
            "Epoch [116/250], Step [100/263], Loss: 0.0308, Accuracy: 99.22%\n",
            "Epoch [116/250], Step [110/263], Loss: 0.0252, Accuracy: 99.06%\n",
            "Epoch [116/250], Step [120/263], Loss: 0.0262, Accuracy: 99.22%\n",
            "Epoch [116/250], Step [130/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [116/250], Step [140/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [116/250], Step [150/263], Loss: 0.0137, Accuracy: 99.38%\n",
            "Epoch [116/250], Step [160/263], Loss: 0.0096, Accuracy: 99.53%\n",
            "Epoch [116/250], Step [170/263], Loss: 0.0070, Accuracy: 99.53%\n",
            "Epoch [116/250], Step [180/263], Loss: 0.0159, Accuracy: 99.38%\n",
            "Epoch [116/250], Step [190/263], Loss: 0.0087, Accuracy: 99.69%\n",
            "Epoch [116/250], Step [200/263], Loss: 0.0076, Accuracy: 99.84%\n",
            "Epoch [116/250], Step [210/263], Loss: 0.0089, Accuracy: 99.53%\n",
            "Epoch [116/250], Step [220/263], Loss: 0.0161, Accuracy: 99.84%\n",
            "Epoch [116/250], Step [230/263], Loss: 0.0308, Accuracy: 98.75%\n",
            "Epoch [116/250], Step [240/263], Loss: 0.0068, Accuracy: 100.00%\n",
            "Epoch [116/250], Step [250/263], Loss: 0.0100, Accuracy: 99.84%\n",
            "Epoch [116/250], Step [260/263], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [117/250], Step [10/263], Loss: 0.0060, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [20/263], Loss: 0.0048, Accuracy: 99.69%\n",
            "Epoch [117/250], Step [30/263], Loss: 0.0114, Accuracy: 99.53%\n",
            "Epoch [117/250], Step [40/263], Loss: 0.0053, Accuracy: 99.69%\n",
            "Epoch [117/250], Step [50/263], Loss: 0.0057, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [60/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [70/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [117/250], Step [80/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [117/250], Step [90/263], Loss: 0.0047, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [100/263], Loss: 0.0036, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [110/263], Loss: 0.0076, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [120/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [117/250], Step [130/263], Loss: 0.0064, Accuracy: 99.69%\n",
            "Epoch [117/250], Step [140/263], Loss: 0.0067, Accuracy: 99.69%\n",
            "Epoch [117/250], Step [150/263], Loss: 0.0135, Accuracy: 99.69%\n",
            "Epoch [117/250], Step [160/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [170/263], Loss: 0.0041, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [180/263], Loss: 0.0129, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [190/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [117/250], Step [200/263], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [117/250], Step [210/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [117/250], Step [220/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [230/263], Loss: 0.0078, Accuracy: 99.84%\n",
            "Epoch [117/250], Step [240/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [117/250], Step [250/263], Loss: 0.0087, Accuracy: 99.69%\n",
            "Epoch [117/250], Step [260/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [118/250], Step [10/263], Loss: 0.0047, Accuracy: 99.84%\n",
            "Epoch [118/250], Step [20/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [30/263], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [40/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [50/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [60/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [70/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [80/263], Loss: 0.0038, Accuracy: 99.84%\n",
            "Epoch [118/250], Step [90/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [100/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [110/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [120/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [130/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [140/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [150/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [160/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [170/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [180/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [200/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [210/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [220/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [230/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [250/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [118/250], Step [260/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [10/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [20/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [40/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [50/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [60/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [70/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [100/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [130/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [140/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [150/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [160/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [170/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [180/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [119/250], Step [190/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [200/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [210/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [119/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [70/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [100/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [140/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [190/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [200/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [220/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [120/250], Step [250/263], Loss: 0.0055, Accuracy: 99.84%\n",
            "Epoch [120/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [10/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [20/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [40/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [60/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [70/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [80/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [90/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [110/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [120/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [190/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [121/250], Step [250/263], Loss: 0.0023, Accuracy: 99.84%\n",
            "Epoch [121/250], Step [260/263], Loss: 0.0032, Accuracy: 99.84%\n",
            "Epoch [122/250], Step [10/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [122/250], Step [20/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [122/250], Step [30/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [122/250], Step [40/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [122/250], Step [50/263], Loss: 0.0036, Accuracy: 99.69%\n",
            "Epoch [122/250], Step [60/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [122/250], Step [70/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [122/250], Step [80/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [122/250], Step [90/263], Loss: 0.0101, Accuracy: 99.69%\n",
            "Epoch [122/250], Step [100/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [122/250], Step [110/263], Loss: 0.0036, Accuracy: 99.84%\n",
            "Epoch [122/250], Step [120/263], Loss: 0.0083, Accuracy: 99.84%\n",
            "Epoch [122/250], Step [130/263], Loss: 0.0202, Accuracy: 99.69%\n",
            "Epoch [122/250], Step [140/263], Loss: 0.0062, Accuracy: 99.84%\n",
            "Epoch [122/250], Step [150/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [122/250], Step [160/263], Loss: 0.0113, Accuracy: 99.53%\n",
            "Epoch [122/250], Step [170/263], Loss: 0.0261, Accuracy: 99.06%\n",
            "Epoch [122/250], Step [180/263], Loss: 0.0390, Accuracy: 98.59%\n",
            "Epoch [122/250], Step [190/263], Loss: 0.0441, Accuracy: 98.59%\n",
            "Epoch [122/250], Step [200/263], Loss: 0.0596, Accuracy: 97.50%\n",
            "Epoch [122/250], Step [210/263], Loss: 0.0648, Accuracy: 97.97%\n",
            "Epoch [122/250], Step [220/263], Loss: 0.0421, Accuracy: 98.91%\n",
            "Epoch [122/250], Step [230/263], Loss: 0.0466, Accuracy: 98.44%\n",
            "Epoch [122/250], Step [240/263], Loss: 0.0618, Accuracy: 98.44%\n",
            "Epoch [122/250], Step [250/263], Loss: 0.0863, Accuracy: 97.19%\n",
            "Epoch [122/250], Step [260/263], Loss: 0.0462, Accuracy: 97.81%\n",
            "Epoch [123/250], Step [10/263], Loss: 0.0543, Accuracy: 97.81%\n",
            "Epoch [123/250], Step [20/263], Loss: 0.1089, Accuracy: 96.72%\n",
            "Epoch [123/250], Step [30/263], Loss: 0.1134, Accuracy: 97.34%\n",
            "Epoch [123/250], Step [40/263], Loss: 0.0656, Accuracy: 98.12%\n",
            "Epoch [123/250], Step [50/263], Loss: 0.0501, Accuracy: 97.97%\n",
            "Epoch [123/250], Step [60/263], Loss: 0.0654, Accuracy: 97.66%\n",
            "Epoch [123/250], Step [70/263], Loss: 0.0626, Accuracy: 97.97%\n",
            "Epoch [123/250], Step [80/263], Loss: 0.0435, Accuracy: 99.06%\n",
            "Epoch [123/250], Step [90/263], Loss: 0.0653, Accuracy: 97.66%\n",
            "Epoch [123/250], Step [100/263], Loss: 0.0399, Accuracy: 98.91%\n",
            "Epoch [123/250], Step [110/263], Loss: 0.0418, Accuracy: 98.28%\n",
            "Epoch [123/250], Step [120/263], Loss: 0.0420, Accuracy: 98.44%\n",
            "Epoch [123/250], Step [130/263], Loss: 0.0381, Accuracy: 98.59%\n",
            "Epoch [123/250], Step [140/263], Loss: 0.0193, Accuracy: 99.38%\n",
            "Epoch [123/250], Step [150/263], Loss: 0.0258, Accuracy: 99.22%\n",
            "Epoch [123/250], Step [160/263], Loss: 0.0233, Accuracy: 99.53%\n",
            "Epoch [123/250], Step [170/263], Loss: 0.0283, Accuracy: 98.75%\n",
            "Epoch [123/250], Step [180/263], Loss: 0.0351, Accuracy: 98.44%\n",
            "Epoch [123/250], Step [190/263], Loss: 0.0386, Accuracy: 99.22%\n",
            "Epoch [123/250], Step [200/263], Loss: 0.0228, Accuracy: 99.22%\n",
            "Epoch [123/250], Step [210/263], Loss: 0.0289, Accuracy: 99.06%\n",
            "Epoch [123/250], Step [220/263], Loss: 0.0385, Accuracy: 98.44%\n",
            "Epoch [123/250], Step [230/263], Loss: 0.0393, Accuracy: 98.75%\n",
            "Epoch [123/250], Step [240/263], Loss: 0.0265, Accuracy: 99.06%\n",
            "Epoch [123/250], Step [250/263], Loss: 0.0521, Accuracy: 98.44%\n",
            "Epoch [123/250], Step [260/263], Loss: 0.0258, Accuracy: 99.22%\n",
            "Epoch [124/250], Step [10/263], Loss: 0.0170, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [20/263], Loss: 0.0233, Accuracy: 99.06%\n",
            "Epoch [124/250], Step [30/263], Loss: 0.0261, Accuracy: 99.38%\n",
            "Epoch [124/250], Step [40/263], Loss: 0.0633, Accuracy: 99.22%\n",
            "Epoch [124/250], Step [50/263], Loss: 0.0223, Accuracy: 99.06%\n",
            "Epoch [124/250], Step [60/263], Loss: 0.0368, Accuracy: 99.53%\n",
            "Epoch [124/250], Step [70/263], Loss: 0.0260, Accuracy: 99.38%\n",
            "Epoch [124/250], Step [80/263], Loss: 0.0100, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [90/263], Loss: 0.0139, Accuracy: 99.38%\n",
            "Epoch [124/250], Step [100/263], Loss: 0.0070, Accuracy: 99.84%\n",
            "Epoch [124/250], Step [110/263], Loss: 0.0187, Accuracy: 99.22%\n",
            "Epoch [124/250], Step [120/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [124/250], Step [130/263], Loss: 0.0179, Accuracy: 99.38%\n",
            "Epoch [124/250], Step [140/263], Loss: 0.0071, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [150/263], Loss: 0.0113, Accuracy: 99.38%\n",
            "Epoch [124/250], Step [160/263], Loss: 0.0062, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [170/263], Loss: 0.0105, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [180/263], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [124/250], Step [190/263], Loss: 0.0072, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [200/263], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [124/250], Step [210/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [124/250], Step [220/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [124/250], Step [230/263], Loss: 0.0137, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [240/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [124/250], Step [250/263], Loss: 0.0211, Accuracy: 99.69%\n",
            "Epoch [124/250], Step [260/263], Loss: 0.0063, Accuracy: 99.69%\n",
            "Epoch [125/250], Step [10/263], Loss: 0.0065, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [20/263], Loss: 0.0161, Accuracy: 99.53%\n",
            "Epoch [125/250], Step [30/263], Loss: 0.0036, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [40/263], Loss: 0.0193, Accuracy: 99.38%\n",
            "Epoch [125/250], Step [50/263], Loss: 0.0115, Accuracy: 99.38%\n",
            "Epoch [125/250], Step [60/263], Loss: 0.0119, Accuracy: 99.53%\n",
            "Epoch [125/250], Step [70/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [80/263], Loss: 0.0057, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [90/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [100/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [110/263], Loss: 0.0097, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [120/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [130/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [140/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [150/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [160/263], Loss: 0.0054, Accuracy: 99.69%\n",
            "Epoch [125/250], Step [170/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [180/263], Loss: 0.0093, Accuracy: 99.69%\n",
            "Epoch [125/250], Step [190/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [200/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [210/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [125/250], Step [220/263], Loss: 0.0061, Accuracy: 99.69%\n",
            "Epoch [125/250], Step [230/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [240/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [250/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [125/250], Step [260/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [126/250], Step [10/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [20/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [30/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [126/250], Step [40/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [50/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [60/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [70/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [80/263], Loss: 0.0022, Accuracy: 99.84%\n",
            "Epoch [126/250], Step [90/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [100/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [110/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [120/263], Loss: 0.0018, Accuracy: 99.84%\n",
            "Epoch [126/250], Step [130/263], Loss: 0.0020, Accuracy: 99.84%\n",
            "Epoch [126/250], Step [140/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [150/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [160/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [170/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [180/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [200/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [210/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [230/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [240/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [126/250], Step [250/263], Loss: 0.0024, Accuracy: 99.84%\n",
            "Epoch [126/250], Step [260/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [10/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [20/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [40/263], Loss: 0.0018, Accuracy: 99.84%\n",
            "Epoch [127/250], Step [50/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [60/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [70/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [80/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [90/263], Loss: 0.0057, Accuracy: 99.84%\n",
            "Epoch [127/250], Step [100/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [127/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [120/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [130/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [140/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [150/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [160/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [170/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [180/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [210/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [220/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [250/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [127/250], Step [260/263], Loss: 0.0019, Accuracy: 99.84%\n",
            "Epoch [128/250], Step [10/263], Loss: 0.0016, Accuracy: 99.84%\n",
            "Epoch [128/250], Step [20/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [40/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [50/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [60/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [70/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [80/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [128/250], Step [90/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [100/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [110/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [140/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [128/250], Step [150/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [160/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [170/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [200/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [210/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [230/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [128/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [60/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [120/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [150/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [230/263], Loss: 0.0017, Accuracy: 99.84%\n",
            "Epoch [129/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [129/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [110/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [220/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [250/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [130/250], Step [260/263], Loss: 0.0023, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [10/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [131/250], Step [20/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [131/250], Step [30/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [40/263], Loss: 0.0200, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [50/263], Loss: 0.0076, Accuracy: 99.69%\n",
            "Epoch [131/250], Step [60/263], Loss: 0.0158, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [70/263], Loss: 0.0020, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [80/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [90/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [131/250], Step [100/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [131/250], Step [110/263], Loss: 0.0104, Accuracy: 99.69%\n",
            "Epoch [131/250], Step [120/263], Loss: 0.0421, Accuracy: 99.38%\n",
            "Epoch [131/250], Step [130/263], Loss: 0.0266, Accuracy: 99.69%\n",
            "Epoch [131/250], Step [140/263], Loss: 0.0224, Accuracy: 99.53%\n",
            "Epoch [131/250], Step [150/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [160/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [170/263], Loss: 0.0142, Accuracy: 99.69%\n",
            "Epoch [131/250], Step [180/263], Loss: 0.0062, Accuracy: 99.84%\n",
            "Epoch [131/250], Step [190/263], Loss: 0.0121, Accuracy: 99.38%\n",
            "Epoch [131/250], Step [200/263], Loss: 0.0191, Accuracy: 99.53%\n",
            "Epoch [131/250], Step [210/263], Loss: 0.0085, Accuracy: 99.69%\n",
            "Epoch [131/250], Step [220/263], Loss: 0.0232, Accuracy: 99.53%\n",
            "Epoch [131/250], Step [230/263], Loss: 0.0155, Accuracy: 99.53%\n",
            "Epoch [131/250], Step [240/263], Loss: 0.0142, Accuracy: 99.38%\n",
            "Epoch [131/250], Step [250/263], Loss: 0.0183, Accuracy: 99.22%\n",
            "Epoch [131/250], Step [260/263], Loss: 0.0286, Accuracy: 99.38%\n",
            "Epoch [132/250], Step [10/263], Loss: 0.0206, Accuracy: 99.22%\n",
            "Epoch [132/250], Step [20/263], Loss: 0.0341, Accuracy: 99.38%\n",
            "Epoch [132/250], Step [30/263], Loss: 0.0439, Accuracy: 98.91%\n",
            "Epoch [132/250], Step [40/263], Loss: 0.0559, Accuracy: 98.44%\n",
            "Epoch [132/250], Step [50/263], Loss: 0.0416, Accuracy: 98.44%\n",
            "Epoch [132/250], Step [60/263], Loss: 0.0395, Accuracy: 99.22%\n",
            "Epoch [132/250], Step [70/263], Loss: 0.0609, Accuracy: 97.50%\n",
            "Epoch [132/250], Step [80/263], Loss: 0.0682, Accuracy: 97.97%\n",
            "Epoch [132/250], Step [90/263], Loss: 0.0899, Accuracy: 97.19%\n",
            "Epoch [132/250], Step [100/263], Loss: 0.0592, Accuracy: 98.28%\n",
            "Epoch [132/250], Step [110/263], Loss: 0.1099, Accuracy: 97.19%\n",
            "Epoch [132/250], Step [120/263], Loss: 0.0315, Accuracy: 99.22%\n",
            "Epoch [132/250], Step [130/263], Loss: 0.0439, Accuracy: 98.91%\n",
            "Epoch [132/250], Step [140/263], Loss: 0.0285, Accuracy: 98.91%\n",
            "Epoch [132/250], Step [150/263], Loss: 0.0521, Accuracy: 98.59%\n",
            "Epoch [132/250], Step [160/263], Loss: 0.0588, Accuracy: 97.19%\n",
            "Epoch [132/250], Step [170/263], Loss: 0.0484, Accuracy: 98.12%\n",
            "Epoch [132/250], Step [180/263], Loss: 0.0396, Accuracy: 98.28%\n",
            "Epoch [132/250], Step [190/263], Loss: 0.0452, Accuracy: 97.97%\n",
            "Epoch [132/250], Step [200/263], Loss: 0.0321, Accuracy: 98.44%\n",
            "Epoch [132/250], Step [210/263], Loss: 0.0260, Accuracy: 99.06%\n",
            "Epoch [132/250], Step [220/263], Loss: 0.0507, Accuracy: 98.12%\n",
            "Epoch [132/250], Step [230/263], Loss: 0.0376, Accuracy: 98.75%\n",
            "Epoch [132/250], Step [240/263], Loss: 0.0433, Accuracy: 98.44%\n",
            "Epoch [132/250], Step [250/263], Loss: 0.0506, Accuracy: 98.44%\n",
            "Epoch [132/250], Step [260/263], Loss: 0.0497, Accuracy: 98.44%\n",
            "Epoch [133/250], Step [10/263], Loss: 0.0492, Accuracy: 98.28%\n",
            "Epoch [133/250], Step [20/263], Loss: 0.0141, Accuracy: 99.22%\n",
            "Epoch [133/250], Step [30/263], Loss: 0.0111, Accuracy: 99.69%\n",
            "Epoch [133/250], Step [40/263], Loss: 0.0254, Accuracy: 99.38%\n",
            "Epoch [133/250], Step [50/263], Loss: 0.0184, Accuracy: 99.22%\n",
            "Epoch [133/250], Step [60/263], Loss: 0.0175, Accuracy: 99.22%\n",
            "Epoch [133/250], Step [70/263], Loss: 0.0046, Accuracy: 99.84%\n",
            "Epoch [133/250], Step [80/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [133/250], Step [90/263], Loss: 0.0302, Accuracy: 99.38%\n",
            "Epoch [133/250], Step [100/263], Loss: 0.0105, Accuracy: 99.69%\n",
            "Epoch [133/250], Step [110/263], Loss: 0.0054, Accuracy: 99.69%\n",
            "Epoch [133/250], Step [120/263], Loss: 0.0300, Accuracy: 99.38%\n",
            "Epoch [133/250], Step [130/263], Loss: 0.0084, Accuracy: 99.84%\n",
            "Epoch [133/250], Step [140/263], Loss: 0.0197, Accuracy: 99.22%\n",
            "Epoch [133/250], Step [150/263], Loss: 0.0193, Accuracy: 99.38%\n",
            "Epoch [133/250], Step [160/263], Loss: 0.0164, Accuracy: 99.38%\n",
            "Epoch [133/250], Step [170/263], Loss: 0.0225, Accuracy: 99.22%\n",
            "Epoch [133/250], Step [180/263], Loss: 0.0070, Accuracy: 99.69%\n",
            "Epoch [133/250], Step [190/263], Loss: 0.0094, Accuracy: 99.53%\n",
            "Epoch [133/250], Step [200/263], Loss: 0.0139, Accuracy: 99.53%\n",
            "Epoch [133/250], Step [210/263], Loss: 0.0110, Accuracy: 99.69%\n",
            "Epoch [133/250], Step [220/263], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [133/250], Step [230/263], Loss: 0.0046, Accuracy: 99.84%\n",
            "Epoch [133/250], Step [240/263], Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch [133/250], Step [250/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [133/250], Step [260/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [10/263], Loss: 0.0033, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [20/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [30/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [40/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [50/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [60/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [70/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [134/250], Step [80/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [90/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [100/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [110/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [120/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [130/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [140/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [150/263], Loss: 0.0024, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [160/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [170/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [180/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [190/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [200/263], Loss: 0.0070, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [210/263], Loss: 0.0127, Accuracy: 99.69%\n",
            "Epoch [134/250], Step [220/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [230/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [134/250], Step [240/263], Loss: 0.0045, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [250/263], Loss: 0.0047, Accuracy: 99.84%\n",
            "Epoch [134/250], Step [260/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [10/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [20/263], Loss: 0.0020, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [30/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [40/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [50/263], Loss: 0.0029, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [60/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [70/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [80/263], Loss: 0.0029, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [90/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [100/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [110/263], Loss: 0.0172, Accuracy: 99.53%\n",
            "Epoch [135/250], Step [120/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [130/263], Loss: 0.0031, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [140/263], Loss: 0.0059, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [150/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [160/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [170/263], Loss: 0.0029, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [180/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [190/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [135/250], Step [200/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [210/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [220/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [230/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [240/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [250/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [135/250], Step [260/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [10/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [20/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [40/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [50/263], Loss: 0.0030, Accuracy: 99.84%\n",
            "Epoch [136/250], Step [60/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [70/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [80/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [90/263], Loss: 0.0140, Accuracy: 99.84%\n",
            "Epoch [136/250], Step [100/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [110/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [120/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [130/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [140/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [136/250], Step [150/263], Loss: 0.0085, Accuracy: 99.84%\n",
            "Epoch [136/250], Step [160/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [180/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [200/263], Loss: 0.0021, Accuracy: 99.84%\n",
            "Epoch [136/250], Step [210/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [220/263], Loss: 0.0043, Accuracy: 99.69%\n",
            "Epoch [136/250], Step [230/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [136/250], Step [240/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [250/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [136/250], Step [260/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [10/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [20/263], Loss: 0.0082, Accuracy: 99.84%\n",
            "Epoch [137/250], Step [30/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [137/250], Step [40/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [50/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [60/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [70/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [80/263], Loss: 0.0015, Accuracy: 99.84%\n",
            "Epoch [137/250], Step [90/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [100/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [110/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [120/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [150/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [170/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [180/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [190/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [200/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [220/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [137/250], Step [260/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [50/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [170/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [190/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [230/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [138/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [100/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/250], Step [260/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [20/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [90/263], Loss: 0.0018, Accuracy: 99.84%\n",
            "Epoch [140/250], Step [100/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [120/263], Loss: 0.0016, Accuracy: 99.84%\n",
            "Epoch [140/250], Step [130/263], Loss: 0.0043, Accuracy: 99.84%\n",
            "Epoch [140/250], Step [140/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [150/263], Loss: 0.0221, Accuracy: 99.69%\n",
            "Epoch [140/250], Step [160/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [140/250], Step [170/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [180/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [140/250], Step [190/263], Loss: 0.0147, Accuracy: 99.53%\n",
            "Epoch [140/250], Step [200/263], Loss: 0.0140, Accuracy: 99.53%\n",
            "Epoch [140/250], Step [210/263], Loss: 0.0080, Accuracy: 99.69%\n",
            "Epoch [140/250], Step [220/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [140/250], Step [230/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [240/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [140/250], Step [250/263], Loss: 0.0063, Accuracy: 99.84%\n",
            "Epoch [140/250], Step [260/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [141/250], Step [10/263], Loss: 0.0256, Accuracy: 99.22%\n",
            "Epoch [141/250], Step [20/263], Loss: 0.0364, Accuracy: 99.22%\n",
            "Epoch [141/250], Step [30/263], Loss: 0.0161, Accuracy: 99.22%\n",
            "Epoch [141/250], Step [40/263], Loss: 0.0832, Accuracy: 98.44%\n",
            "Epoch [141/250], Step [50/263], Loss: 0.0287, Accuracy: 99.06%\n",
            "Epoch [141/250], Step [60/263], Loss: 0.0687, Accuracy: 98.28%\n",
            "Epoch [141/250], Step [70/263], Loss: 0.0654, Accuracy: 97.81%\n",
            "Epoch [141/250], Step [80/263], Loss: 0.0906, Accuracy: 97.19%\n",
            "Epoch [141/250], Step [90/263], Loss: 0.0610, Accuracy: 98.12%\n",
            "Epoch [141/250], Step [100/263], Loss: 0.0961, Accuracy: 97.19%\n",
            "Epoch [141/250], Step [110/263], Loss: 0.0818, Accuracy: 97.34%\n",
            "Epoch [141/250], Step [120/263], Loss: 0.0776, Accuracy: 98.12%\n",
            "Epoch [141/250], Step [130/263], Loss: 0.0740, Accuracy: 97.97%\n",
            "Epoch [141/250], Step [140/263], Loss: 0.0789, Accuracy: 97.19%\n",
            "Epoch [141/250], Step [150/263], Loss: 0.0393, Accuracy: 98.28%\n",
            "Epoch [141/250], Step [160/263], Loss: 0.0427, Accuracy: 98.59%\n",
            "Epoch [141/250], Step [170/263], Loss: 0.0528, Accuracy: 98.44%\n",
            "Epoch [141/250], Step [180/263], Loss: 0.0768, Accuracy: 97.81%\n",
            "Epoch [141/250], Step [190/263], Loss: 0.0620, Accuracy: 99.06%\n",
            "Epoch [141/250], Step [200/263], Loss: 0.0331, Accuracy: 99.22%\n",
            "Epoch [141/250], Step [210/263], Loss: 0.0368, Accuracy: 99.06%\n",
            "Epoch [141/250], Step [220/263], Loss: 0.0341, Accuracy: 98.28%\n",
            "Epoch [141/250], Step [230/263], Loss: 0.0342, Accuracy: 99.06%\n",
            "Epoch [141/250], Step [240/263], Loss: 0.0369, Accuracy: 98.59%\n",
            "Epoch [141/250], Step [250/263], Loss: 0.0446, Accuracy: 98.59%\n",
            "Epoch [141/250], Step [260/263], Loss: 0.0298, Accuracy: 98.59%\n",
            "Epoch [142/250], Step [10/263], Loss: 0.0196, Accuracy: 98.91%\n",
            "Epoch [142/250], Step [20/263], Loss: 0.0375, Accuracy: 98.91%\n",
            "Epoch [142/250], Step [30/263], Loss: 0.0405, Accuracy: 99.22%\n",
            "Epoch [142/250], Step [40/263], Loss: 0.0095, Accuracy: 99.84%\n",
            "Epoch [142/250], Step [50/263], Loss: 0.0121, Accuracy: 99.84%\n",
            "Epoch [142/250], Step [60/263], Loss: 0.0133, Accuracy: 99.38%\n",
            "Epoch [142/250], Step [70/263], Loss: 0.0075, Accuracy: 99.84%\n",
            "Epoch [142/250], Step [80/263], Loss: 0.0202, Accuracy: 99.22%\n",
            "Epoch [142/250], Step [90/263], Loss: 0.0158, Accuracy: 99.38%\n",
            "Epoch [142/250], Step [100/263], Loss: 0.0182, Accuracy: 99.38%\n",
            "Epoch [142/250], Step [110/263], Loss: 0.0115, Accuracy: 99.53%\n",
            "Epoch [142/250], Step [120/263], Loss: 0.0213, Accuracy: 99.38%\n",
            "Epoch [142/250], Step [130/263], Loss: 0.0106, Accuracy: 99.53%\n",
            "Epoch [142/250], Step [140/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [142/250], Step [150/263], Loss: 0.0069, Accuracy: 99.53%\n",
            "Epoch [142/250], Step [160/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [142/250], Step [170/263], Loss: 0.0207, Accuracy: 99.69%\n",
            "Epoch [142/250], Step [180/263], Loss: 0.0056, Accuracy: 99.84%\n",
            "Epoch [142/250], Step [190/263], Loss: 0.0206, Accuracy: 99.53%\n",
            "Epoch [142/250], Step [200/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [142/250], Step [210/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [142/250], Step [220/263], Loss: 0.0190, Accuracy: 99.38%\n",
            "Epoch [142/250], Step [230/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [142/250], Step [240/263], Loss: 0.0117, Accuracy: 99.84%\n",
            "Epoch [142/250], Step [250/263], Loss: 0.0233, Accuracy: 99.06%\n",
            "Epoch [142/250], Step [260/263], Loss: 0.0105, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [10/263], Loss: 0.0115, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [20/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [30/263], Loss: 0.0124, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [40/263], Loss: 0.0141, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [50/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [60/263], Loss: 0.0056, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [70/263], Loss: 0.0041, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [80/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [90/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [100/263], Loss: 0.0042, Accuracy: 99.69%\n",
            "Epoch [143/250], Step [110/263], Loss: 0.0046, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [120/263], Loss: 0.0087, Accuracy: 99.38%\n",
            "Epoch [143/250], Step [130/263], Loss: 0.0111, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [140/263], Loss: 0.0064, Accuracy: 99.69%\n",
            "Epoch [143/250], Step [150/263], Loss: 0.0052, Accuracy: 99.69%\n",
            "Epoch [143/250], Step [160/263], Loss: 0.0068, Accuracy: 99.69%\n",
            "Epoch [143/250], Step [170/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [180/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [190/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [200/263], Loss: 0.0041, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [210/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [220/263], Loss: 0.0091, Accuracy: 99.84%\n",
            "Epoch [143/250], Step [230/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [240/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [250/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [143/250], Step [260/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [10/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [20/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [30/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [40/263], Loss: 0.0027, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [50/263], Loss: 0.0015, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [60/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [70/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [80/263], Loss: 0.0149, Accuracy: 99.69%\n",
            "Epoch [144/250], Step [90/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [100/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [110/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [120/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [130/263], Loss: 0.0027, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [140/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [150/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [160/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [170/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [180/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [200/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [210/263], Loss: 0.0085, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [220/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [230/263], Loss: 0.0021, Accuracy: 99.84%\n",
            "Epoch [144/250], Step [240/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [250/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [144/250], Step [260/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [10/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [20/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [40/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [50/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [60/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [70/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [90/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [100/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [120/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [130/263], Loss: 0.0045, Accuracy: 99.84%\n",
            "Epoch [145/250], Step [140/263], Loss: 0.0022, Accuracy: 99.84%\n",
            "Epoch [145/250], Step [150/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [145/250], Step [160/263], Loss: 0.0081, Accuracy: 99.69%\n",
            "Epoch [145/250], Step [170/263], Loss: 0.0063, Accuracy: 99.84%\n",
            "Epoch [145/250], Step [180/263], Loss: 0.0192, Accuracy: 99.53%\n",
            "Epoch [145/250], Step [190/263], Loss: 0.0093, Accuracy: 99.69%\n",
            "Epoch [145/250], Step [200/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [145/250], Step [210/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [220/263], Loss: 0.0064, Accuracy: 99.53%\n",
            "Epoch [145/250], Step [230/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [145/250], Step [240/263], Loss: 0.0197, Accuracy: 99.69%\n",
            "Epoch [145/250], Step [250/263], Loss: 0.0040, Accuracy: 100.00%\n",
            "Epoch [145/250], Step [260/263], Loss: 0.0071, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [10/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [20/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [146/250], Step [30/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [40/263], Loss: 0.0103, Accuracy: 99.69%\n",
            "Epoch [146/250], Step [50/263], Loss: 0.0127, Accuracy: 99.69%\n",
            "Epoch [146/250], Step [60/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [146/250], Step [70/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [146/250], Step [80/263], Loss: 0.0043, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [90/263], Loss: 0.0095, Accuracy: 99.69%\n",
            "Epoch [146/250], Step [100/263], Loss: 0.0129, Accuracy: 99.53%\n",
            "Epoch [146/250], Step [110/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [146/250], Step [120/263], Loss: 0.0097, Accuracy: 99.69%\n",
            "Epoch [146/250], Step [130/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [146/250], Step [140/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [150/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [146/250], Step [160/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [170/263], Loss: 0.0122, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [180/263], Loss: 0.0095, Accuracy: 99.53%\n",
            "Epoch [146/250], Step [190/263], Loss: 0.0036, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [200/263], Loss: 0.0159, Accuracy: 99.53%\n",
            "Epoch [146/250], Step [210/263], Loss: 0.0141, Accuracy: 99.69%\n",
            "Epoch [146/250], Step [220/263], Loss: 0.0078, Accuracy: 99.53%\n",
            "Epoch [146/250], Step [230/263], Loss: 0.0173, Accuracy: 99.53%\n",
            "Epoch [146/250], Step [240/263], Loss: 0.0099, Accuracy: 99.53%\n",
            "Epoch [146/250], Step [250/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [146/250], Step [260/263], Loss: 0.0266, Accuracy: 99.22%\n",
            "Epoch [147/250], Step [10/263], Loss: 0.0099, Accuracy: 99.69%\n",
            "Epoch [147/250], Step [20/263], Loss: 0.0542, Accuracy: 98.91%\n",
            "Epoch [147/250], Step [30/263], Loss: 0.0439, Accuracy: 99.22%\n",
            "Epoch [147/250], Step [40/263], Loss: 0.0081, Accuracy: 99.53%\n",
            "Epoch [147/250], Step [50/263], Loss: 0.0328, Accuracy: 99.38%\n",
            "Epoch [147/250], Step [60/263], Loss: 0.0049, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [70/263], Loss: 0.0262, Accuracy: 98.91%\n",
            "Epoch [147/250], Step [80/263], Loss: 0.0292, Accuracy: 99.06%\n",
            "Epoch [147/250], Step [90/263], Loss: 0.0174, Accuracy: 99.06%\n",
            "Epoch [147/250], Step [100/263], Loss: 0.0053, Accuracy: 99.69%\n",
            "Epoch [147/250], Step [110/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [147/250], Step [120/263], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [130/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [140/263], Loss: 0.0042, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [150/263], Loss: 0.0094, Accuracy: 99.69%\n",
            "Epoch [147/250], Step [160/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [170/263], Loss: 0.0047, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [180/263], Loss: 0.0030, Accuracy: 99.84%\n",
            "Epoch [147/250], Step [190/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [200/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [210/263], Loss: 0.0021, Accuracy: 99.84%\n",
            "Epoch [147/250], Step [220/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [147/250], Step [230/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [240/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [250/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [147/250], Step [260/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [10/263], Loss: 0.0143, Accuracy: 99.69%\n",
            "Epoch [148/250], Step [20/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [30/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [40/263], Loss: 0.0119, Accuracy: 99.53%\n",
            "Epoch [148/250], Step [50/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [60/263], Loss: 0.0119, Accuracy: 99.53%\n",
            "Epoch [148/250], Step [70/263], Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [80/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [148/250], Step [90/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [100/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [110/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [148/250], Step [120/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [130/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [140/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [148/250], Step [150/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [160/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [148/250], Step [170/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [180/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [190/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [200/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [210/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [220/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [230/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [240/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [250/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [148/250], Step [260/263], Loss: 0.0019, Accuracy: 99.84%\n",
            "Epoch [149/250], Step [10/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [20/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [30/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [50/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [60/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [70/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [90/263], Loss: 0.0104, Accuracy: 99.84%\n",
            "Epoch [149/250], Step [100/263], Loss: 0.0024, Accuracy: 99.84%\n",
            "Epoch [149/250], Step [110/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [120/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [130/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [140/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [150/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [160/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [170/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [180/263], Loss: 0.0021, Accuracy: 99.84%\n",
            "Epoch [149/250], Step [190/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [200/263], Loss: 0.0021, Accuracy: 99.84%\n",
            "Epoch [149/250], Step [210/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [230/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [240/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [149/250], Step [250/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [149/250], Step [260/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [150/250], Step [10/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [150/250], Step [20/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [150/250], Step [30/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [40/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [150/250], Step [50/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [60/263], Loss: 0.0069, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [70/263], Loss: 0.0110, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [80/263], Loss: 0.0059, Accuracy: 99.69%\n",
            "Epoch [150/250], Step [90/263], Loss: 0.0273, Accuracy: 99.38%\n",
            "Epoch [150/250], Step [100/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [110/263], Loss: 0.0091, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [120/263], Loss: 0.0164, Accuracy: 99.69%\n",
            "Epoch [150/250], Step [130/263], Loss: 0.0138, Accuracy: 99.53%\n",
            "Epoch [150/250], Step [140/263], Loss: 0.0091, Accuracy: 99.69%\n",
            "Epoch [150/250], Step [150/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [160/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [170/263], Loss: 0.0274, Accuracy: 99.53%\n",
            "Epoch [150/250], Step [180/263], Loss: 0.0113, Accuracy: 99.53%\n",
            "Epoch [150/250], Step [190/263], Loss: 0.0046, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [200/263], Loss: 0.0072, Accuracy: 99.69%\n",
            "Epoch [150/250], Step [210/263], Loss: 0.0165, Accuracy: 99.53%\n",
            "Epoch [150/250], Step [220/263], Loss: 0.0041, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [230/263], Loss: 0.0194, Accuracy: 99.38%\n",
            "Epoch [150/250], Step [240/263], Loss: 0.0149, Accuracy: 99.69%\n",
            "Epoch [150/250], Step [250/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [150/250], Step [260/263], Loss: 0.0127, Accuracy: 99.53%\n",
            "Epoch [151/250], Step [10/263], Loss: 0.0170, Accuracy: 99.53%\n",
            "Epoch [151/250], Step [20/263], Loss: 0.0192, Accuracy: 99.53%\n",
            "Epoch [151/250], Step [30/263], Loss: 0.0080, Accuracy: 99.69%\n",
            "Epoch [151/250], Step [40/263], Loss: 0.0065, Accuracy: 99.84%\n",
            "Epoch [151/250], Step [50/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [151/250], Step [60/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [151/250], Step [70/263], Loss: 0.0070, Accuracy: 99.84%\n",
            "Epoch [151/250], Step [80/263], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [151/250], Step [90/263], Loss: 0.0089, Accuracy: 99.69%\n",
            "Epoch [151/250], Step [100/263], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [151/250], Step [110/263], Loss: 0.0133, Accuracy: 99.38%\n",
            "Epoch [151/250], Step [120/263], Loss: 0.0131, Accuracy: 99.69%\n",
            "Epoch [151/250], Step [130/263], Loss: 0.0216, Accuracy: 99.06%\n",
            "Epoch [151/250], Step [140/263], Loss: 0.0404, Accuracy: 99.38%\n",
            "Epoch [151/250], Step [150/263], Loss: 0.0132, Accuracy: 99.53%\n",
            "Epoch [151/250], Step [160/263], Loss: 0.0178, Accuracy: 99.53%\n",
            "Epoch [151/250], Step [170/263], Loss: 0.0157, Accuracy: 99.69%\n",
            "Epoch [151/250], Step [180/263], Loss: 0.0333, Accuracy: 99.38%\n",
            "Epoch [151/250], Step [190/263], Loss: 0.0339, Accuracy: 99.06%\n",
            "Epoch [151/250], Step [200/263], Loss: 0.0191, Accuracy: 99.53%\n",
            "Epoch [151/250], Step [210/263], Loss: 0.0325, Accuracy: 99.22%\n",
            "Epoch [151/250], Step [220/263], Loss: 0.0121, Accuracy: 99.53%\n",
            "Epoch [151/250], Step [230/263], Loss: 0.0218, Accuracy: 99.22%\n",
            "Epoch [151/250], Step [240/263], Loss: 0.0286, Accuracy: 99.69%\n",
            "Epoch [151/250], Step [250/263], Loss: 0.0113, Accuracy: 99.69%\n",
            "Epoch [151/250], Step [260/263], Loss: 0.0259, Accuracy: 99.06%\n",
            "Epoch [152/250], Step [10/263], Loss: 0.0211, Accuracy: 98.59%\n",
            "Epoch [152/250], Step [20/263], Loss: 0.0110, Accuracy: 99.53%\n",
            "Epoch [152/250], Step [30/263], Loss: 0.0182, Accuracy: 99.69%\n",
            "Epoch [152/250], Step [40/263], Loss: 0.0062, Accuracy: 99.84%\n",
            "Epoch [152/250], Step [50/263], Loss: 0.0156, Accuracy: 99.53%\n",
            "Epoch [152/250], Step [60/263], Loss: 0.0204, Accuracy: 99.53%\n",
            "Epoch [152/250], Step [70/263], Loss: 0.0071, Accuracy: 99.84%\n",
            "Epoch [152/250], Step [80/263], Loss: 0.0132, Accuracy: 99.69%\n",
            "Epoch [152/250], Step [90/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [152/250], Step [100/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [152/250], Step [110/263], Loss: 0.0167, Accuracy: 99.22%\n",
            "Epoch [152/250], Step [120/263], Loss: 0.0094, Accuracy: 99.53%\n",
            "Epoch [152/250], Step [130/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [152/250], Step [140/263], Loss: 0.0055, Accuracy: 99.69%\n",
            "Epoch [152/250], Step [150/263], Loss: 0.0054, Accuracy: 99.69%\n",
            "Epoch [152/250], Step [160/263], Loss: 0.0173, Accuracy: 99.38%\n",
            "Epoch [152/250], Step [170/263], Loss: 0.0117, Accuracy: 99.38%\n",
            "Epoch [152/250], Step [180/263], Loss: 0.0206, Accuracy: 99.53%\n",
            "Epoch [152/250], Step [190/263], Loss: 0.0066, Accuracy: 99.69%\n",
            "Epoch [152/250], Step [200/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [152/250], Step [210/263], Loss: 0.0056, Accuracy: 99.84%\n",
            "Epoch [152/250], Step [220/263], Loss: 0.0095, Accuracy: 99.53%\n",
            "Epoch [152/250], Step [230/263], Loss: 0.0113, Accuracy: 99.69%\n",
            "Epoch [152/250], Step [240/263], Loss: 0.0159, Accuracy: 99.38%\n",
            "Epoch [152/250], Step [250/263], Loss: 0.0206, Accuracy: 99.22%\n",
            "Epoch [152/250], Step [260/263], Loss: 0.0209, Accuracy: 99.22%\n",
            "Epoch [153/250], Step [10/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [153/250], Step [20/263], Loss: 0.0067, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [30/263], Loss: 0.0048, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [40/263], Loss: 0.0043, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [50/263], Loss: 0.0028, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [60/263], Loss: 0.0057, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [70/263], Loss: 0.0069, Accuracy: 99.69%\n",
            "Epoch [153/250], Step [80/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [90/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [100/263], Loss: 0.0047, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [110/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [120/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [130/263], Loss: 0.0115, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [140/263], Loss: 0.0073, Accuracy: 99.69%\n",
            "Epoch [153/250], Step [150/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [160/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [170/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [153/250], Step [180/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [153/250], Step [190/263], Loss: 0.0056, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [200/263], Loss: 0.0027, Accuracy: 99.84%\n",
            "Epoch [153/250], Step [210/263], Loss: 0.0139, Accuracy: 99.53%\n",
            "Epoch [153/250], Step [220/263], Loss: 0.0236, Accuracy: 99.53%\n",
            "Epoch [153/250], Step [230/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [153/250], Step [240/263], Loss: 0.0216, Accuracy: 99.69%\n",
            "Epoch [153/250], Step [250/263], Loss: 0.0154, Accuracy: 99.69%\n",
            "Epoch [153/250], Step [260/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [10/263], Loss: 0.0114, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [20/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [154/250], Step [30/263], Loss: 0.0057, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [40/263], Loss: 0.0061, Accuracy: 99.69%\n",
            "Epoch [154/250], Step [50/263], Loss: 0.0018, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [60/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [154/250], Step [70/263], Loss: 0.0069, Accuracy: 99.69%\n",
            "Epoch [154/250], Step [80/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [90/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [154/250], Step [100/263], Loss: 0.0102, Accuracy: 99.53%\n",
            "Epoch [154/250], Step [110/263], Loss: 0.0136, Accuracy: 99.69%\n",
            "Epoch [154/250], Step [120/263], Loss: 0.0030, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [130/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [140/263], Loss: 0.0052, Accuracy: 99.69%\n",
            "Epoch [154/250], Step [150/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [160/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [170/263], Loss: 0.0120, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [180/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [154/250], Step [190/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [154/250], Step [200/263], Loss: 0.0055, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [210/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [154/250], Step [220/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [154/250], Step [230/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [154/250], Step [240/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [154/250], Step [250/263], Loss: 0.0091, Accuracy: 99.69%\n",
            "Epoch [154/250], Step [260/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [10/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [20/263], Loss: 0.0168, Accuracy: 99.84%\n",
            "Epoch [155/250], Step [30/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [40/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [50/263], Loss: 0.0171, Accuracy: 99.53%\n",
            "Epoch [155/250], Step [60/263], Loss: 0.0178, Accuracy: 99.69%\n",
            "Epoch [155/250], Step [70/263], Loss: 0.0129, Accuracy: 99.53%\n",
            "Epoch [155/250], Step [80/263], Loss: 0.0071, Accuracy: 99.84%\n",
            "Epoch [155/250], Step [90/263], Loss: 0.0064, Accuracy: 99.69%\n",
            "Epoch [155/250], Step [100/263], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [110/263], Loss: 0.0029, Accuracy: 99.84%\n",
            "Epoch [155/250], Step [120/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [130/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [140/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [150/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [160/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [155/250], Step [170/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [180/263], Loss: 0.0170, Accuracy: 99.53%\n",
            "Epoch [155/250], Step [190/263], Loss: 0.0121, Accuracy: 99.69%\n",
            "Epoch [155/250], Step [200/263], Loss: 0.0073, Accuracy: 99.69%\n",
            "Epoch [155/250], Step [210/263], Loss: 0.0096, Accuracy: 99.69%\n",
            "Epoch [155/250], Step [220/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [230/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [240/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [155/250], Step [250/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [155/250], Step [260/263], Loss: 0.0070, Accuracy: 99.69%\n",
            "Epoch [156/250], Step [10/263], Loss: 0.0112, Accuracy: 99.69%\n",
            "Epoch [156/250], Step [20/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [30/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [40/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [50/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [60/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [70/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [80/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [90/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [100/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [110/263], Loss: 0.0211, Accuracy: 99.69%\n",
            "Epoch [156/250], Step [120/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [130/263], Loss: 0.0028, Accuracy: 99.84%\n",
            "Epoch [156/250], Step [140/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [150/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [160/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [170/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [180/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [190/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [200/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [210/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [220/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [230/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [240/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [156/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [20/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [70/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [80/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [90/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [170/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [157/250], Step [250/263], Loss: 0.0016, Accuracy: 99.84%\n",
            "Epoch [157/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [10/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [158/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [159/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [160/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [161/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [162/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [163/250], Step [10/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [163/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [163/250], Step [30/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [163/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [163/250], Step [50/263], Loss: 0.0086, Accuracy: 99.84%\n",
            "Epoch [163/250], Step [60/263], Loss: 0.0076, Accuracy: 99.84%\n",
            "Epoch [163/250], Step [70/263], Loss: 0.0119, Accuracy: 99.84%\n",
            "Epoch [163/250], Step [80/263], Loss: 0.0257, Accuracy: 99.06%\n",
            "Epoch [163/250], Step [90/263], Loss: 0.0886, Accuracy: 97.50%\n",
            "Epoch [163/250], Step [100/263], Loss: 0.0591, Accuracy: 98.44%\n",
            "Epoch [163/250], Step [110/263], Loss: 0.0903, Accuracy: 97.19%\n",
            "Epoch [163/250], Step [120/263], Loss: 0.0797, Accuracy: 97.50%\n",
            "Epoch [163/250], Step [130/263], Loss: 0.1143, Accuracy: 95.47%\n",
            "Epoch [163/250], Step [140/263], Loss: 0.0888, Accuracy: 97.81%\n",
            "Epoch [163/250], Step [150/263], Loss: 0.1078, Accuracy: 96.41%\n",
            "Epoch [163/250], Step [160/263], Loss: 0.1057, Accuracy: 96.56%\n",
            "Epoch [163/250], Step [170/263], Loss: 0.0629, Accuracy: 98.28%\n",
            "Epoch [163/250], Step [180/263], Loss: 0.0879, Accuracy: 97.19%\n",
            "Epoch [163/250], Step [190/263], Loss: 0.0935, Accuracy: 96.88%\n",
            "Epoch [163/250], Step [200/263], Loss: 0.0417, Accuracy: 98.28%\n",
            "Epoch [163/250], Step [210/263], Loss: 0.0417, Accuracy: 98.44%\n",
            "Epoch [163/250], Step [220/263], Loss: 0.0315, Accuracy: 99.06%\n",
            "Epoch [163/250], Step [230/263], Loss: 0.0332, Accuracy: 99.06%\n",
            "Epoch [163/250], Step [240/263], Loss: 0.0488, Accuracy: 98.12%\n",
            "Epoch [163/250], Step [250/263], Loss: 0.0712, Accuracy: 97.97%\n",
            "Epoch [163/250], Step [260/263], Loss: 0.0282, Accuracy: 98.75%\n",
            "Epoch [164/250], Step [10/263], Loss: 0.0373, Accuracy: 98.28%\n",
            "Epoch [164/250], Step [20/263], Loss: 0.0332, Accuracy: 99.38%\n",
            "Epoch [164/250], Step [30/263], Loss: 0.0492, Accuracy: 98.91%\n",
            "Epoch [164/250], Step [40/263], Loss: 0.0241, Accuracy: 99.38%\n",
            "Epoch [164/250], Step [50/263], Loss: 0.0142, Accuracy: 99.53%\n",
            "Epoch [164/250], Step [60/263], Loss: 0.0136, Accuracy: 99.38%\n",
            "Epoch [164/250], Step [70/263], Loss: 0.0071, Accuracy: 99.69%\n",
            "Epoch [164/250], Step [80/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [164/250], Step [90/263], Loss: 0.0089, Accuracy: 99.53%\n",
            "Epoch [164/250], Step [100/263], Loss: 0.0180, Accuracy: 99.69%\n",
            "Epoch [164/250], Step [110/263], Loss: 0.0134, Accuracy: 99.38%\n",
            "Epoch [164/250], Step [120/263], Loss: 0.0275, Accuracy: 99.38%\n",
            "Epoch [164/250], Step [130/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [164/250], Step [140/263], Loss: 0.0066, Accuracy: 99.84%\n",
            "Epoch [164/250], Step [150/263], Loss: 0.0164, Accuracy: 99.69%\n",
            "Epoch [164/250], Step [160/263], Loss: 0.0050, Accuracy: 100.00%\n",
            "Epoch [164/250], Step [170/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [164/250], Step [180/263], Loss: 0.0124, Accuracy: 99.53%\n",
            "Epoch [164/250], Step [190/263], Loss: 0.0137, Accuracy: 99.06%\n",
            "Epoch [164/250], Step [200/263], Loss: 0.0246, Accuracy: 99.69%\n",
            "Epoch [164/250], Step [210/263], Loss: 0.0079, Accuracy: 99.69%\n",
            "Epoch [164/250], Step [220/263], Loss: 0.0111, Accuracy: 99.38%\n",
            "Epoch [164/250], Step [230/263], Loss: 0.0199, Accuracy: 99.38%\n",
            "Epoch [164/250], Step [240/263], Loss: 0.0052, Accuracy: 100.00%\n",
            "Epoch [164/250], Step [250/263], Loss: 0.0093, Accuracy: 99.69%\n",
            "Epoch [164/250], Step [260/263], Loss: 0.0282, Accuracy: 99.53%\n",
            "Epoch [165/250], Step [10/263], Loss: 0.0059, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [20/263], Loss: 0.0158, Accuracy: 99.69%\n",
            "Epoch [165/250], Step [30/263], Loss: 0.0121, Accuracy: 99.53%\n",
            "Epoch [165/250], Step [40/263], Loss: 0.0188, Accuracy: 99.22%\n",
            "Epoch [165/250], Step [50/263], Loss: 0.0073, Accuracy: 99.69%\n",
            "Epoch [165/250], Step [60/263], Loss: 0.0036, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [70/263], Loss: 0.0073, Accuracy: 99.69%\n",
            "Epoch [165/250], Step [80/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [90/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [165/250], Step [100/263], Loss: 0.0088, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [110/263], Loss: 0.0092, Accuracy: 99.69%\n",
            "Epoch [165/250], Step [120/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [130/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [140/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [165/250], Step [150/263], Loss: 0.0027, Accuracy: 100.00%\n",
            "Epoch [165/250], Step [160/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [165/250], Step [170/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [165/250], Step [180/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [165/250], Step [190/263], Loss: 0.0019, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [200/263], Loss: 0.0071, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [210/263], Loss: 0.0063, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [220/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [230/263], Loss: 0.0122, Accuracy: 99.69%\n",
            "Epoch [165/250], Step [240/263], Loss: 0.0041, Accuracy: 99.84%\n",
            "Epoch [165/250], Step [250/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [165/250], Step [260/263], Loss: 0.0069, Accuracy: 99.84%\n",
            "Epoch [166/250], Step [10/263], Loss: 0.0168, Accuracy: 99.84%\n",
            "Epoch [166/250], Step [20/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [30/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [40/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [50/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [60/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [70/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [80/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [90/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [100/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [110/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [120/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [130/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [140/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [150/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [160/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [180/263], Loss: 0.0044, Accuracy: 99.69%\n",
            "Epoch [166/250], Step [190/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [200/263], Loss: 0.0041, Accuracy: 99.84%\n",
            "Epoch [166/250], Step [210/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [166/250], Step [220/263], Loss: 0.0161, Accuracy: 99.69%\n",
            "Epoch [166/250], Step [230/263], Loss: 0.0045, Accuracy: 99.84%\n",
            "Epoch [166/250], Step [240/263], Loss: 0.0085, Accuracy: 99.53%\n",
            "Epoch [166/250], Step [250/263], Loss: 0.0230, Accuracy: 99.69%\n",
            "Epoch [166/250], Step [260/263], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [10/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [20/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [30/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [167/250], Step [40/263], Loss: 0.0102, Accuracy: 99.84%\n",
            "Epoch [167/250], Step [50/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [60/263], Loss: 0.0054, Accuracy: 99.84%\n",
            "Epoch [167/250], Step [70/263], Loss: 0.0038, Accuracy: 99.84%\n",
            "Epoch [167/250], Step [80/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [90/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [100/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [110/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [120/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [130/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [140/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [150/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [160/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [180/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [190/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [200/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [230/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [250/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [167/250], Step [260/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [10/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [70/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [130/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [150/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [168/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [169/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [170/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [171/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [200/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [172/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [140/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [173/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [110/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [120/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [140/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [150/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [160/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [190/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [220/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [230/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [240/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [174/250], Step [260/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [175/250], Step [10/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [175/250], Step [20/263], Loss: 0.0169, Accuracy: 99.84%\n",
            "Epoch [175/250], Step [30/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [175/250], Step [40/263], Loss: 0.0392, Accuracy: 99.69%\n",
            "Epoch [175/250], Step [50/263], Loss: 0.0112, Accuracy: 99.69%\n",
            "Epoch [175/250], Step [60/263], Loss: 0.0176, Accuracy: 99.69%\n",
            "Epoch [175/250], Step [70/263], Loss: 0.0081, Accuracy: 99.69%\n",
            "Epoch [175/250], Step [80/263], Loss: 0.0152, Accuracy: 99.53%\n",
            "Epoch [175/250], Step [90/263], Loss: 0.0305, Accuracy: 98.91%\n",
            "Epoch [175/250], Step [100/263], Loss: 0.0374, Accuracy: 98.59%\n",
            "Epoch [175/250], Step [110/263], Loss: 0.0292, Accuracy: 99.38%\n",
            "Epoch [175/250], Step [120/263], Loss: 0.0573, Accuracy: 98.91%\n",
            "Epoch [175/250], Step [130/263], Loss: 0.0493, Accuracy: 98.12%\n",
            "Epoch [175/250], Step [140/263], Loss: 0.0743, Accuracy: 98.44%\n",
            "Epoch [175/250], Step [150/263], Loss: 0.0824, Accuracy: 97.03%\n",
            "Epoch [175/250], Step [160/263], Loss: 0.0839, Accuracy: 98.28%\n",
            "Epoch [175/250], Step [170/263], Loss: 0.0358, Accuracy: 98.59%\n",
            "Epoch [175/250], Step [180/263], Loss: 0.0963, Accuracy: 96.56%\n",
            "Epoch [175/250], Step [190/263], Loss: 0.0430, Accuracy: 98.44%\n",
            "Epoch [175/250], Step [200/263], Loss: 0.0740, Accuracy: 97.34%\n",
            "Epoch [175/250], Step [210/263], Loss: 0.0636, Accuracy: 97.34%\n",
            "Epoch [175/250], Step [220/263], Loss: 0.0709, Accuracy: 98.12%\n",
            "Epoch [175/250], Step [230/263], Loss: 0.0416, Accuracy: 98.59%\n",
            "Epoch [175/250], Step [240/263], Loss: 0.0293, Accuracy: 98.75%\n",
            "Epoch [175/250], Step [250/263], Loss: 0.0587, Accuracy: 98.44%\n",
            "Epoch [175/250], Step [260/263], Loss: 0.0959, Accuracy: 97.50%\n",
            "Epoch [176/250], Step [10/263], Loss: 0.0557, Accuracy: 98.12%\n",
            "Epoch [176/250], Step [20/263], Loss: 0.0237, Accuracy: 99.22%\n",
            "Epoch [176/250], Step [30/263], Loss: 0.0521, Accuracy: 98.44%\n",
            "Epoch [176/250], Step [40/263], Loss: 0.0239, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [50/263], Loss: 0.0119, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [60/263], Loss: 0.0281, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [70/263], Loss: 0.0211, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [80/263], Loss: 0.0302, Accuracy: 98.91%\n",
            "Epoch [176/250], Step [90/263], Loss: 0.0479, Accuracy: 98.12%\n",
            "Epoch [176/250], Step [100/263], Loss: 0.0095, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [110/263], Loss: 0.0067, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [120/263], Loss: 0.0233, Accuracy: 99.38%\n",
            "Epoch [176/250], Step [130/263], Loss: 0.0226, Accuracy: 99.22%\n",
            "Epoch [176/250], Step [140/263], Loss: 0.0343, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [150/263], Loss: 0.0204, Accuracy: 99.22%\n",
            "Epoch [176/250], Step [160/263], Loss: 0.0224, Accuracy: 99.06%\n",
            "Epoch [176/250], Step [170/263], Loss: 0.0292, Accuracy: 98.91%\n",
            "Epoch [176/250], Step [180/263], Loss: 0.0122, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [190/263], Loss: 0.0216, Accuracy: 99.38%\n",
            "Epoch [176/250], Step [200/263], Loss: 0.0074, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [210/263], Loss: 0.0142, Accuracy: 99.38%\n",
            "Epoch [176/250], Step [220/263], Loss: 0.0085, Accuracy: 99.69%\n",
            "Epoch [176/250], Step [230/263], Loss: 0.0081, Accuracy: 99.84%\n",
            "Epoch [176/250], Step [240/263], Loss: 0.0192, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [250/263], Loss: 0.0173, Accuracy: 99.53%\n",
            "Epoch [176/250], Step [260/263], Loss: 0.0061, Accuracy: 99.84%\n",
            "Epoch [177/250], Step [10/263], Loss: 0.0106, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [20/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [30/263], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [40/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [177/250], Step [50/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [60/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [70/263], Loss: 0.0053, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [80/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [90/263], Loss: 0.0046, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [100/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [110/263], Loss: 0.0060, Accuracy: 99.84%\n",
            "Epoch [177/250], Step [120/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [130/263], Loss: 0.0044, Accuracy: 99.84%\n",
            "Epoch [177/250], Step [140/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [150/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [177/250], Step [160/263], Loss: 0.0043, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [170/263], Loss: 0.0111, Accuracy: 99.84%\n",
            "Epoch [177/250], Step [180/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [190/263], Loss: 0.0045, Accuracy: 99.84%\n",
            "Epoch [177/250], Step [200/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [210/263], Loss: 0.0044, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [220/263], Loss: 0.0051, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [230/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [240/263], Loss: 0.0063, Accuracy: 99.69%\n",
            "Epoch [177/250], Step [250/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [177/250], Step [260/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [10/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [20/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [40/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [50/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [60/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [70/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [80/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [90/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [100/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [110/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [120/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [130/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [140/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [150/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [160/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [190/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [200/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [220/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [178/250], Step [260/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [10/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [120/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [150/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [250/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [179/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [100/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [180/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [181/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [182/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [183/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [184/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [185/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [186/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [187/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [188/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [250/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [189/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [190/250], Step [10/263], Loss: 0.0277, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [20/263], Loss: 0.0459, Accuracy: 98.44%\n",
            "Epoch [190/250], Step [30/263], Loss: 0.1093, Accuracy: 97.97%\n",
            "Epoch [190/250], Step [40/263], Loss: 0.1137, Accuracy: 96.72%\n",
            "Epoch [190/250], Step [50/263], Loss: 0.1688, Accuracy: 94.53%\n",
            "Epoch [190/250], Step [60/263], Loss: 0.1580, Accuracy: 96.56%\n",
            "Epoch [190/250], Step [70/263], Loss: 0.2211, Accuracy: 94.53%\n",
            "Epoch [190/250], Step [80/263], Loss: 0.0810, Accuracy: 97.34%\n",
            "Epoch [190/250], Step [90/263], Loss: 0.0777, Accuracy: 97.81%\n",
            "Epoch [190/250], Step [100/263], Loss: 0.0625, Accuracy: 98.12%\n",
            "Epoch [190/250], Step [110/263], Loss: 0.0466, Accuracy: 98.44%\n",
            "Epoch [190/250], Step [120/263], Loss: 0.0867, Accuracy: 97.66%\n",
            "Epoch [190/250], Step [130/263], Loss: 0.1109, Accuracy: 96.88%\n",
            "Epoch [190/250], Step [140/263], Loss: 0.0799, Accuracy: 97.34%\n",
            "Epoch [190/250], Step [150/263], Loss: 0.0550, Accuracy: 97.81%\n",
            "Epoch [190/250], Step [160/263], Loss: 0.0558, Accuracy: 98.44%\n",
            "Epoch [190/250], Step [170/263], Loss: 0.0446, Accuracy: 98.44%\n",
            "Epoch [190/250], Step [180/263], Loss: 0.0616, Accuracy: 97.81%\n",
            "Epoch [190/250], Step [190/263], Loss: 0.0494, Accuracy: 98.59%\n",
            "Epoch [190/250], Step [200/263], Loss: 0.0252, Accuracy: 99.06%\n",
            "Epoch [190/250], Step [210/263], Loss: 0.0290, Accuracy: 99.06%\n",
            "Epoch [190/250], Step [220/263], Loss: 0.0170, Accuracy: 99.38%\n",
            "Epoch [190/250], Step [230/263], Loss: 0.0467, Accuracy: 98.75%\n",
            "Epoch [190/250], Step [240/263], Loss: 0.0150, Accuracy: 99.69%\n",
            "Epoch [190/250], Step [250/263], Loss: 0.0103, Accuracy: 99.53%\n",
            "Epoch [190/250], Step [260/263], Loss: 0.0381, Accuracy: 99.06%\n",
            "Epoch [191/250], Step [10/263], Loss: 0.0198, Accuracy: 99.22%\n",
            "Epoch [191/250], Step [20/263], Loss: 0.0384, Accuracy: 99.38%\n",
            "Epoch [191/250], Step [30/263], Loss: 0.0167, Accuracy: 99.53%\n",
            "Epoch [191/250], Step [40/263], Loss: 0.0190, Accuracy: 99.38%\n",
            "Epoch [191/250], Step [50/263], Loss: 0.0130, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [60/263], Loss: 0.0162, Accuracy: 99.38%\n",
            "Epoch [191/250], Step [70/263], Loss: 0.0031, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [80/263], Loss: 0.0081, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [90/263], Loss: 0.0086, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [100/263], Loss: 0.0391, Accuracy: 99.38%\n",
            "Epoch [191/250], Step [110/263], Loss: 0.0144, Accuracy: 99.38%\n",
            "Epoch [191/250], Step [120/263], Loss: 0.0111, Accuracy: 99.53%\n",
            "Epoch [191/250], Step [130/263], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [140/263], Loss: 0.0146, Accuracy: 99.53%\n",
            "Epoch [191/250], Step [150/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [160/263], Loss: 0.0074, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [170/263], Loss: 0.0046, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [180/263], Loss: 0.0098, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [190/263], Loss: 0.0123, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [200/263], Loss: 0.0095, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [210/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [220/263], Loss: 0.0119, Accuracy: 99.53%\n",
            "Epoch [191/250], Step [230/263], Loss: 0.0069, Accuracy: 99.69%\n",
            "Epoch [191/250], Step [240/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [191/250], Step [250/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [191/250], Step [260/263], Loss: 0.0061, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [10/263], Loss: 0.0022, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [20/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [30/263], Loss: 0.0050, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [40/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [50/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [60/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [70/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [80/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [90/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [100/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [110/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [120/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [130/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [140/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [150/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [160/263], Loss: 0.0020, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [170/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [180/263], Loss: 0.0030, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [190/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [200/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [210/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [220/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [230/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [192/250], Step [240/263], Loss: 0.0051, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [250/263], Loss: 0.0018, Accuracy: 99.84%\n",
            "Epoch [192/250], Step [260/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [10/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [30/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [50/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [60/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [80/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [100/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [110/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [140/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [150/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [160/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [180/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [190/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [200/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [220/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [250/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [193/250], Step [260/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [40/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [70/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [194/250], Step [260/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [20/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [195/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [196/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [197/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [198/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [199/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [200/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [201/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [202/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [203/250], Step [10/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [203/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [203/250], Step [30/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [203/250], Step [40/263], Loss: 0.1007, Accuracy: 98.59%\n",
            "Epoch [203/250], Step [50/263], Loss: 0.0281, Accuracy: 99.06%\n",
            "Epoch [203/250], Step [60/263], Loss: 0.0429, Accuracy: 98.91%\n",
            "Epoch [203/250], Step [70/263], Loss: 0.0643, Accuracy: 97.97%\n",
            "Epoch [203/250], Step [80/263], Loss: 0.0721, Accuracy: 97.50%\n",
            "Epoch [203/250], Step [90/263], Loss: 0.0889, Accuracy: 97.34%\n",
            "Epoch [203/250], Step [100/263], Loss: 0.0608, Accuracy: 97.97%\n",
            "Epoch [203/250], Step [110/263], Loss: 0.0462, Accuracy: 98.59%\n",
            "Epoch [203/250], Step [120/263], Loss: 0.0529, Accuracy: 97.66%\n",
            "Epoch [203/250], Step [130/263], Loss: 0.0758, Accuracy: 97.97%\n",
            "Epoch [203/250], Step [140/263], Loss: 0.0503, Accuracy: 98.59%\n",
            "Epoch [203/250], Step [150/263], Loss: 0.0841, Accuracy: 97.66%\n",
            "Epoch [203/250], Step [160/263], Loss: 0.0755, Accuracy: 97.03%\n",
            "Epoch [203/250], Step [170/263], Loss: 0.0500, Accuracy: 98.44%\n",
            "Epoch [203/250], Step [180/263], Loss: 0.0435, Accuracy: 98.12%\n",
            "Epoch [203/250], Step [190/263], Loss: 0.0403, Accuracy: 98.59%\n",
            "Epoch [203/250], Step [200/263], Loss: 0.0298, Accuracy: 99.22%\n",
            "Epoch [203/250], Step [210/263], Loss: 0.0504, Accuracy: 97.97%\n",
            "Epoch [203/250], Step [220/263], Loss: 0.0258, Accuracy: 99.22%\n",
            "Epoch [203/250], Step [230/263], Loss: 0.0488, Accuracy: 98.91%\n",
            "Epoch [203/250], Step [240/263], Loss: 0.0346, Accuracy: 99.22%\n",
            "Epoch [203/250], Step [250/263], Loss: 0.0287, Accuracy: 99.06%\n",
            "Epoch [203/250], Step [260/263], Loss: 0.0436, Accuracy: 98.28%\n",
            "Epoch [204/250], Step [10/263], Loss: 0.0210, Accuracy: 99.06%\n",
            "Epoch [204/250], Step [20/263], Loss: 0.0237, Accuracy: 99.06%\n",
            "Epoch [204/250], Step [30/263], Loss: 0.0205, Accuracy: 99.06%\n",
            "Epoch [204/250], Step [40/263], Loss: 0.0276, Accuracy: 99.22%\n",
            "Epoch [204/250], Step [50/263], Loss: 0.0286, Accuracy: 98.91%\n",
            "Epoch [204/250], Step [60/263], Loss: 0.0149, Accuracy: 99.53%\n",
            "Epoch [204/250], Step [70/263], Loss: 0.0211, Accuracy: 98.91%\n",
            "Epoch [204/250], Step [80/263], Loss: 0.0214, Accuracy: 99.22%\n",
            "Epoch [204/250], Step [90/263], Loss: 0.0132, Accuracy: 99.22%\n",
            "Epoch [204/250], Step [100/263], Loss: 0.0242, Accuracy: 99.22%\n",
            "Epoch [204/250], Step [110/263], Loss: 0.0073, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [120/263], Loss: 0.0237, Accuracy: 99.06%\n",
            "Epoch [204/250], Step [130/263], Loss: 0.0072, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [140/263], Loss: 0.0230, Accuracy: 99.22%\n",
            "Epoch [204/250], Step [150/263], Loss: 0.0097, Accuracy: 99.53%\n",
            "Epoch [204/250], Step [160/263], Loss: 0.0045, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [170/263], Loss: 0.0074, Accuracy: 99.69%\n",
            "Epoch [204/250], Step [180/263], Loss: 0.0057, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [190/263], Loss: 0.0102, Accuracy: 99.22%\n",
            "Epoch [204/250], Step [200/263], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [204/250], Step [210/263], Loss: 0.0080, Accuracy: 99.69%\n",
            "Epoch [204/250], Step [220/263], Loss: 0.0080, Accuracy: 99.69%\n",
            "Epoch [204/250], Step [230/263], Loss: 0.0160, Accuracy: 99.22%\n",
            "Epoch [204/250], Step [240/263], Loss: 0.0084, Accuracy: 99.38%\n",
            "Epoch [204/250], Step [250/263], Loss: 0.0067, Accuracy: 99.84%\n",
            "Epoch [204/250], Step [260/263], Loss: 0.0212, Accuracy: 99.38%\n",
            "Epoch [205/250], Step [10/263], Loss: 0.0083, Accuracy: 99.53%\n",
            "Epoch [205/250], Step [20/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [30/263], Loss: 0.0053, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [40/263], Loss: 0.0043, Accuracy: 99.69%\n",
            "Epoch [205/250], Step [50/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [60/263], Loss: 0.0025, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [70/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [80/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [90/263], Loss: 0.0029, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [100/263], Loss: 0.0028, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [110/263], Loss: 0.0040, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [120/263], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [130/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [140/263], Loss: 0.0058, Accuracy: 99.53%\n",
            "Epoch [205/250], Step [150/263], Loss: 0.0094, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [160/263], Loss: 0.0067, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [170/263], Loss: 0.0089, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [180/263], Loss: 0.0039, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [190/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [200/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [210/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [220/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [230/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [205/250], Step [240/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [250/263], Loss: 0.0025, Accuracy: 99.84%\n",
            "Epoch [205/250], Step [260/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [10/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [20/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [30/263], Loss: 0.0090, Accuracy: 99.69%\n",
            "Epoch [206/250], Step [40/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [50/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [60/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [70/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [80/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [90/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [100/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [110/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [120/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [140/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [150/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [160/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [170/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [190/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [200/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [230/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [240/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [250/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [206/250], Step [260/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [20/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [60/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [70/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [150/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [210/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [207/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [60/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [90/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [208/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [30/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [209/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [210/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [211/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [212/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [213/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [214/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [215/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [216/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [217/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [218/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [219/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [220/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [221/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [222/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [223/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [224/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [140/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [150/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [180/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [190/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [225/250], Step [200/263], Loss: 0.0232, Accuracy: 99.38%\n",
            "Epoch [225/250], Step [210/263], Loss: 0.0921, Accuracy: 98.44%\n",
            "Epoch [225/250], Step [220/263], Loss: 0.1063, Accuracy: 97.81%\n",
            "Epoch [225/250], Step [230/263], Loss: 0.1794, Accuracy: 96.25%\n",
            "Epoch [225/250], Step [240/263], Loss: 0.1462, Accuracy: 96.41%\n",
            "Epoch [225/250], Step [250/263], Loss: 0.1354, Accuracy: 95.16%\n",
            "Epoch [225/250], Step [260/263], Loss: 0.1064, Accuracy: 97.03%\n",
            "Epoch [226/250], Step [10/263], Loss: 0.0995, Accuracy: 97.97%\n",
            "Epoch [226/250], Step [20/263], Loss: 0.0755, Accuracy: 97.97%\n",
            "Epoch [226/250], Step [30/263], Loss: 0.1096, Accuracy: 96.41%\n",
            "Epoch [226/250], Step [40/263], Loss: 0.0729, Accuracy: 97.50%\n",
            "Epoch [226/250], Step [50/263], Loss: 0.0436, Accuracy: 98.59%\n",
            "Epoch [226/250], Step [60/263], Loss: 0.0244, Accuracy: 99.38%\n",
            "Epoch [226/250], Step [70/263], Loss: 0.0291, Accuracy: 98.91%\n",
            "Epoch [226/250], Step [80/263], Loss: 0.0530, Accuracy: 98.12%\n",
            "Epoch [226/250], Step [90/263], Loss: 0.0385, Accuracy: 98.91%\n",
            "Epoch [226/250], Step [100/263], Loss: 0.0499, Accuracy: 98.12%\n",
            "Epoch [226/250], Step [110/263], Loss: 0.0346, Accuracy: 98.91%\n",
            "Epoch [226/250], Step [120/263], Loss: 0.0386, Accuracy: 98.59%\n",
            "Epoch [226/250], Step [130/263], Loss: 0.0319, Accuracy: 98.75%\n",
            "Epoch [226/250], Step [140/263], Loss: 0.0491, Accuracy: 98.59%\n",
            "Epoch [226/250], Step [150/263], Loss: 0.0387, Accuracy: 98.75%\n",
            "Epoch [226/250], Step [160/263], Loss: 0.0403, Accuracy: 98.59%\n",
            "Epoch [226/250], Step [170/263], Loss: 0.0369, Accuracy: 98.75%\n",
            "Epoch [226/250], Step [180/263], Loss: 0.0466, Accuracy: 98.12%\n",
            "Epoch [226/250], Step [190/263], Loss: 0.0434, Accuracy: 98.59%\n",
            "Epoch [226/250], Step [200/263], Loss: 0.0339, Accuracy: 99.22%\n",
            "Epoch [226/250], Step [210/263], Loss: 0.0276, Accuracy: 99.53%\n",
            "Epoch [226/250], Step [220/263], Loss: 0.0353, Accuracy: 98.44%\n",
            "Epoch [226/250], Step [230/263], Loss: 0.0221, Accuracy: 99.38%\n",
            "Epoch [226/250], Step [240/263], Loss: 0.0189, Accuracy: 99.22%\n",
            "Epoch [226/250], Step [250/263], Loss: 0.0228, Accuracy: 99.06%\n",
            "Epoch [226/250], Step [260/263], Loss: 0.0184, Accuracy: 99.22%\n",
            "Epoch [227/250], Step [10/263], Loss: 0.0141, Accuracy: 99.53%\n",
            "Epoch [227/250], Step [20/263], Loss: 0.0077, Accuracy: 99.69%\n",
            "Epoch [227/250], Step [30/263], Loss: 0.0103, Accuracy: 99.22%\n",
            "Epoch [227/250], Step [40/263], Loss: 0.0060, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [50/263], Loss: 0.0071, Accuracy: 99.69%\n",
            "Epoch [227/250], Step [60/263], Loss: 0.0070, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [70/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [80/263], Loss: 0.0087, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [90/263], Loss: 0.0125, Accuracy: 99.38%\n",
            "Epoch [227/250], Step [100/263], Loss: 0.0091, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [110/263], Loss: 0.0164, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [120/263], Loss: 0.0064, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [130/263], Loss: 0.0049, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [140/263], Loss: 0.0067, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [150/263], Loss: 0.0022, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [160/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [170/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [180/263], Loss: 0.0162, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [190/263], Loss: 0.0073, Accuracy: 99.69%\n",
            "Epoch [227/250], Step [200/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [210/263], Loss: 0.0056, Accuracy: 99.69%\n",
            "Epoch [227/250], Step [220/263], Loss: 0.0055, Accuracy: 99.69%\n",
            "Epoch [227/250], Step [230/263], Loss: 0.0065, Accuracy: 99.69%\n",
            "Epoch [227/250], Step [240/263], Loss: 0.0042, Accuracy: 99.84%\n",
            "Epoch [227/250], Step [250/263], Loss: 0.0081, Accuracy: 99.53%\n",
            "Epoch [227/250], Step [260/263], Loss: 0.0125, Accuracy: 99.84%\n",
            "Epoch [228/250], Step [10/263], Loss: 0.0062, Accuracy: 99.84%\n",
            "Epoch [228/250], Step [20/263], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [30/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [40/263], Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [50/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [60/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [70/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [80/263], Loss: 0.0029, Accuracy: 99.84%\n",
            "Epoch [228/250], Step [90/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [100/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [110/263], Loss: 0.0066, Accuracy: 99.69%\n",
            "Epoch [228/250], Step [120/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [130/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [140/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [150/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [160/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [170/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [180/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [190/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [200/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [210/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [220/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [230/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [240/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [250/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [228/250], Step [260/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [10/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [20/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [30/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [40/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [50/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [60/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [70/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [80/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [110/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [120/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [130/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [140/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [150/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [160/263], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [170/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [240/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [229/250], Step [260/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [10/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [20/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [80/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [90/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [130/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [150/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [180/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [190/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [210/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [230/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [50/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [100/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [140/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [160/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [180/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [220/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [240/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [231/250], Step [260/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [120/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [170/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [200/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [230/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [250/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [232/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [110/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [233/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [190/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [234/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [235/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [70/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [236/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [90/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [130/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [150/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [200/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [237/250], Step [260/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [10/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [20/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [100/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [140/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [180/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [210/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [250/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [238/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [30/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [40/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [60/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [240/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [239/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [50/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [240/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [120/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [160/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [170/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [241/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [80/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [190/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [230/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [242/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [110/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [220/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [243/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [40/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [60/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [70/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [80/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [90/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [100/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [110/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [120/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [130/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [140/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [150/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [160/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [170/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [180/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [190/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [200/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [210/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [220/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [230/263], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [240/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [250/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [244/250], Step [260/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [10/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [20/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [30/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [40/263], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [50/263], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [245/250], Step [60/263], Loss: 0.0069, Accuracy: 99.69%\n",
            "Epoch [245/250], Step [70/263], Loss: 0.1319, Accuracy: 98.12%\n",
            "Epoch [245/250], Step [80/263], Loss: 0.1270, Accuracy: 96.88%\n",
            "Epoch [245/250], Step [90/263], Loss: 0.1646, Accuracy: 96.41%\n",
            "Epoch [245/250], Step [100/263], Loss: 0.1227, Accuracy: 96.41%\n",
            "Epoch [245/250], Step [110/263], Loss: 0.1390, Accuracy: 96.41%\n",
            "Epoch [245/250], Step [120/263], Loss: 0.1067, Accuracy: 96.25%\n",
            "Epoch [245/250], Step [130/263], Loss: 0.1504, Accuracy: 95.78%\n",
            "Epoch [245/250], Step [140/263], Loss: 0.1071, Accuracy: 96.56%\n",
            "Epoch [245/250], Step [150/263], Loss: 0.1270, Accuracy: 97.03%\n",
            "Epoch [245/250], Step [160/263], Loss: 0.0359, Accuracy: 98.59%\n",
            "Epoch [245/250], Step [170/263], Loss: 0.0453, Accuracy: 98.44%\n",
            "Epoch [245/250], Step [180/263], Loss: 0.0427, Accuracy: 97.81%\n",
            "Epoch [245/250], Step [190/263], Loss: 0.0443, Accuracy: 98.59%\n",
            "Epoch [245/250], Step [200/263], Loss: 0.0327, Accuracy: 98.59%\n",
            "Epoch [245/250], Step [210/263], Loss: 0.0381, Accuracy: 98.91%\n",
            "Epoch [245/250], Step [220/263], Loss: 0.0501, Accuracy: 98.75%\n",
            "Epoch [245/250], Step [230/263], Loss: 0.0418, Accuracy: 98.91%\n",
            "Epoch [245/250], Step [240/263], Loss: 0.0434, Accuracy: 98.91%\n",
            "Epoch [245/250], Step [250/263], Loss: 0.0322, Accuracy: 98.91%\n",
            "Epoch [245/250], Step [260/263], Loss: 0.0269, Accuracy: 98.91%\n",
            "Epoch [246/250], Step [10/263], Loss: 0.0275, Accuracy: 99.06%\n",
            "Epoch [246/250], Step [20/263], Loss: 0.0259, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [30/263], Loss: 0.0122, Accuracy: 99.84%\n",
            "Epoch [246/250], Step [40/263], Loss: 0.0128, Accuracy: 99.69%\n",
            "Epoch [246/250], Step [50/263], Loss: 0.0186, Accuracy: 99.06%\n",
            "Epoch [246/250], Step [60/263], Loss: 0.0161, Accuracy: 99.69%\n",
            "Epoch [246/250], Step [70/263], Loss: 0.0250, Accuracy: 99.38%\n",
            "Epoch [246/250], Step [80/263], Loss: 0.0216, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [90/263], Loss: 0.0244, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [100/263], Loss: 0.0229, Accuracy: 99.22%\n",
            "Epoch [246/250], Step [110/263], Loss: 0.0066, Accuracy: 99.69%\n",
            "Epoch [246/250], Step [120/263], Loss: 0.0130, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [130/263], Loss: 0.0156, Accuracy: 99.22%\n",
            "Epoch [246/250], Step [140/263], Loss: 0.0253, Accuracy: 99.22%\n",
            "Epoch [246/250], Step [150/263], Loss: 0.0122, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [160/263], Loss: 0.0130, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [170/263], Loss: 0.0114, Accuracy: 99.38%\n",
            "Epoch [246/250], Step [180/263], Loss: 0.0159, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [190/263], Loss: 0.0077, Accuracy: 99.69%\n",
            "Epoch [246/250], Step [200/263], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [246/250], Step [210/263], Loss: 0.0097, Accuracy: 99.69%\n",
            "Epoch [246/250], Step [220/263], Loss: 0.0153, Accuracy: 99.53%\n",
            "Epoch [246/250], Step [230/263], Loss: 0.0153, Accuracy: 99.38%\n",
            "Epoch [246/250], Step [240/263], Loss: 0.0048, Accuracy: 99.84%\n",
            "Epoch [246/250], Step [250/263], Loss: 0.0048, Accuracy: 99.84%\n",
            "Epoch [246/250], Step [260/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [10/263], Loss: 0.0050, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [20/263], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [30/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [40/263], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [50/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [60/263], Loss: 0.0067, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [70/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [80/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [90/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [100/263], Loss: 0.0037, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [110/263], Loss: 0.0064, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [120/263], Loss: 0.0043, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [130/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [140/263], Loss: 0.0026, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [150/263], Loss: 0.0042, Accuracy: 99.69%\n",
            "Epoch [247/250], Step [160/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [170/263], Loss: 0.0021, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [180/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [190/263], Loss: 0.0069, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [200/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [210/263], Loss: 0.0245, Accuracy: 99.69%\n",
            "Epoch [247/250], Step [220/263], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [230/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [247/250], Step [240/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [247/250], Step [250/263], Loss: 0.0073, Accuracy: 99.69%\n",
            "Epoch [247/250], Step [260/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [10/263], Loss: 0.0069, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [20/263], Loss: 0.0078, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [30/263], Loss: 0.0068, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [40/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [50/263], Loss: 0.0028, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [60/263], Loss: 0.0051, Accuracy: 99.69%\n",
            "Epoch [248/250], Step [70/263], Loss: 0.0088, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [80/263], Loss: 0.0147, Accuracy: 99.53%\n",
            "Epoch [248/250], Step [90/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [100/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [110/263], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [120/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [130/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [140/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [150/263], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [160/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [170/263], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [180/263], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [190/263], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [200/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [210/263], Loss: 0.0015, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [220/263], Loss: 0.0029, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [230/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [248/250], Step [240/263], Loss: 0.0046, Accuracy: 99.69%\n",
            "Epoch [248/250], Step [250/263], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [248/250], Step [260/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [10/263], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [20/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [30/263], Loss: 0.0058, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [40/263], Loss: 0.0016, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [50/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [60/263], Loss: 0.0131, Accuracy: 99.53%\n",
            "Epoch [249/250], Step [70/263], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [80/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [90/263], Loss: 0.0043, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [100/263], Loss: 0.0167, Accuracy: 99.53%\n",
            "Epoch [249/250], Step [110/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [120/263], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [130/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [140/263], Loss: 0.0121, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [150/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [160/263], Loss: 0.0033, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [170/263], Loss: 0.0026, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [180/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [190/263], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [200/263], Loss: 0.0112, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [210/263], Loss: 0.0035, Accuracy: 99.84%\n",
            "Epoch [249/250], Step [220/263], Loss: 0.0050, Accuracy: 99.69%\n",
            "Epoch [249/250], Step [230/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [240/263], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [250/263], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [249/250], Step [260/263], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [10/263], Loss: 0.0079, Accuracy: 99.53%\n",
            "Epoch [250/250], Step [20/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [30/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [40/263], Loss: 0.0114, Accuracy: 99.69%\n",
            "Epoch [250/250], Step [50/263], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [60/263], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [70/263], Loss: 0.0027, Accuracy: 99.84%\n",
            "Epoch [250/250], Step [80/263], Loss: 0.0030, Accuracy: 99.84%\n",
            "Epoch [250/250], Step [90/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [100/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [250/250], Step [110/263], Loss: 0.0071, Accuracy: 99.84%\n",
            "Epoch [250/250], Step [120/263], Loss: 0.0034, Accuracy: 99.84%\n",
            "Epoch [250/250], Step [130/263], Loss: 0.0078, Accuracy: 99.69%\n",
            "Epoch [250/250], Step [140/263], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [150/263], Loss: 0.0084, Accuracy: 99.69%\n",
            "Epoch [250/250], Step [160/263], Loss: 0.0340, Accuracy: 99.06%\n",
            "Epoch [250/250], Step [170/263], Loss: 0.0127, Accuracy: 99.53%\n",
            "Epoch [250/250], Step [180/263], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [190/263], Loss: 0.0081, Accuracy: 99.69%\n",
            "Epoch [250/250], Step [200/263], Loss: 0.0060, Accuracy: 99.84%\n",
            "Epoch [250/250], Step [210/263], Loss: 0.0267, Accuracy: 99.53%\n",
            "Epoch [250/250], Step [220/263], Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch [250/250], Step [230/263], Loss: 0.0075, Accuracy: 99.69%\n",
            "Epoch [250/250], Step [240/263], Loss: 0.0049, Accuracy: 99.69%\n",
            "Epoch [250/250], Step [250/263], Loss: 0.0052, Accuracy: 99.84%\n",
            "Epoch [250/250], Step [260/263], Loss: 0.0037, Accuracy: 100.00%\n",
            "Training completed for model:  ParallelCNNLSTMModel\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(models: List, train_loader: DataLoader, epochs: int):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss_history = {model.__class__.__name__: [] for model in models}\n",
        "    accuracy_history = {model.__class__.__name__: [] for model in models}\n",
        "\n",
        "    for model in models:\n",
        "        print(\"Training model: \", model.__class__.__name__)\n",
        "        model.train()\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            for i, (x, y) in enumerate(train_loader):\n",
        "                x = x.to(device)\n",
        "                y = y.squeeze().to(device)  # Remove the extra dimension from the target tensor\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x)\n",
        "                loss = criterion(y_pred, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Compute accuracy\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                correct_predictions += (predicted_labels == y).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    avg_loss = total_loss / 10\n",
        "                    accuracy = correct_predictions / total_samples * 100\n",
        "                    print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], '\n",
        "                          f'Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "                    total_loss = 0.0\n",
        "                    correct_predictions = 0\n",
        "                    total_samples = 0\n",
        "\n",
        "            # Store loss and accuracy values for plotting\n",
        "            loss_history[model.__class__.__name__].append(avg_loss)\n",
        "            accuracy_history[model.__class__.__name__].append(accuracy)\n",
        "\n",
        "        print(\"Training completed for model: \", model.__class__.__name__)\n",
        "\n",
        "    # Plot loss and accuracy for each model\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for model in models:\n",
        "        plt.subplot(1, 2, 1)  # Loss plot\n",
        "        plt.plot(range(1, epochs + 1), loss_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)  # Accuracy plot\n",
        "        plt.plot(range(1, epochs + 1), accuracy_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# train\n",
        "models = [cnn_lstm_parallel]\n",
        "num_epochs = 250\n",
        "train(models, train_loader, epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDe5OdQJ1UDM"
      },
      "outputs": [],
      "source": [
        "models = [cnn_lstm, cnn_lstm_parallel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG7XqjJzDHRa"
      },
      "outputs": [],
      "source": [
        "def test(models: List, test_loader: DataLoader):\n",
        "    accuracy_history = {model.__class__.__name__: [] for model in models}\n",
        "\n",
        "    for model in models:\n",
        "        print(\"Testing model: \", model.__class__.__name__)\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation during inference\n",
        "            for x, y in test_loader:\n",
        "                x = x.to(device)\n",
        "                y = y.squeeze().to(device)  # Remove the extra dimension from the target tensor\n",
        "                y_pred = model(x)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                correct_predictions += (predicted_labels == y).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "\n",
        "        accuracy = correct_predictions / total_samples * 100\n",
        "        print(\"Test Accuracy: {:.2f}%\".format(accuracy))\n",
        "        accuracy_history[model.__class__.__name__].append(accuracy)\n",
        "\n",
        "    # Plot accuracy for each model\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    models_names = [model.__class__.__name__ for model in models]\n",
        "    x_points = np.arange(len(models_names))\n",
        "    plt.bar(x_points, [accuracy_history[model_name][0] for model_name in models_names])\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.xticks(x_points, models_names)\n",
        "    plt.show()\n",
        "\n",
        "# Test the models\n",
        "test(models, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX0EgzeeDHO_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "model_filename = 'cnn_lstm.pkl'\n",
        "with open(model_filename, 'wb') as file:\n",
        "    pickle.dump(cnn_lstm, file)\n",
        "\n",
        "import pickle\n",
        "model_filename = 'cnn_lstm_parallel.pkl'\n",
        "with open(model_filename, 'wb') as file:\n",
        "    pickle.dump(cnn_lstm_parallel, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahv929iZrhFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdQYq1R-rhCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "coN2a_RUrg_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfAoEp8Rrg78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdLsJuYPDHMh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN-LSTM on scattering coefficient"
      ],
      "metadata": {
        "id": "AMMzI0jDrNc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "# Replace 'path_to_directory' with the path of the directory containing the files\n",
        "directory = '/content/drive/MyDrive/Day10/Day10_csv'\n",
        "\n",
        "# List all files in the directory\n",
        "file_list = os.listdir(directory)\n",
        "\n",
        "\n",
        "# Iterate over the files and do something with each file\n",
        "for count_, file_name in enumerate(file_list):\n",
        "    #print(count_)\n",
        "    # If you want to work with the full file path, use os.path.join()\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Perform some action with the file\n",
        "    print(f\"Processing file: {file_name}\")\n",
        "    csvfile =directory + \"/\" + file_name\n",
        "    class_ = pd.read_csv(csvfile)\n",
        "    array_= str(class_.order2.values).replace(\"\\\\n      \",\"\").split(\"]), array([\")\n",
        "    order2 = []\n",
        "\n",
        "    class_name = str(file_name).split(\"_\")[1]\n",
        "    for i in array_:\n",
        "      order_ = i.replace(\"['[array([\",\"\").replace(\"])]']\",\"\")\n",
        "      order2_ = str(order_).split(\", \")\n",
        "      for coef in order2_:\n",
        "        coef = coef.replace(\" \",\"\")\n",
        "        order2.append(float(coef))\n",
        "\n",
        "    order2.append(class_name)\n",
        "    numpy_array = np.array(order2)\n",
        "    if count_ == 0:\n",
        "      day1_df = pd.DataFrame([numpy_array])\n",
        "    else:\n",
        "      day1_df = day1_df.append(pd.DataFrame([numpy_array]), ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsLWKRpkrNRv",
        "outputId": "f332e2e2-87ad-415b-98e4-014992307e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: class_1_1400.csv\n",
            "Processing file: class_0_1401.csv\n",
            "Processing file: class_9_1402.csv\n",
            "Processing file: class_9_1403.csv\n",
            "Processing file: class_8_1404.csv\n",
            "Processing file: class_1_1405.csv\n",
            "Processing file: class_3_1406.csv\n",
            "Processing file: class_3_1407.csv\n",
            "Processing file: class_6_1408.csv\n",
            "Processing file: class_1_1409.csv\n",
            "Processing file: class_9_1410.csv\n",
            "Processing file: class_1_1411.csv\n",
            "Processing file: class_6_1412.csv\n",
            "Processing file: class_0_1413.csv\n",
            "Processing file: class_3_1414.csv\n",
            "Processing file: class_1_1415.csv\n",
            "Processing file: class_7_1416.csv\n",
            "Processing file: class_1_1417.csv\n",
            "Processing file: class_8_1418.csv\n",
            "Processing file: class_9_1419.csv\n",
            "Processing file: class_6_1420.csv\n",
            "Processing file: class_5_1421.csv\n",
            "Processing file: class_9_1422.csv\n",
            "Processing file: class_4_1423.csv\n",
            "Processing file: class_7_1424.csv\n",
            "Processing file: class_8_1425.csv\n",
            "Processing file: class_9_1426.csv\n",
            "Processing file: class_11_1427.csv\n",
            "Processing file: class_0_1428.csv\n",
            "Processing file: class_8_1429.csv\n",
            "Processing file: class_10_1430.csv\n",
            "Processing file: class_1_1431.csv\n",
            "Processing file: class_2_1432.csv\n",
            "Processing file: class_11_1433.csv\n",
            "Processing file: class_11_1434.csv\n",
            "Processing file: class_0_1435.csv\n",
            "Processing file: class_0_1436.csv\n",
            "Processing file: class_2_1437.csv\n",
            "Processing file: class_1_1438.csv\n",
            "Processing file: class_7_1439.csv\n",
            "Processing file: class_5_1440.csv\n",
            "Processing file: class_1_1441.csv\n",
            "Processing file: class_5_1442.csv\n",
            "Processing file: class_4_1443.csv\n",
            "Processing file: class_6_1444.csv\n",
            "Processing file: class_6_1445.csv\n",
            "Processing file: class_8_1446.csv\n",
            "Processing file: class_2_1447.csv\n",
            "Processing file: class_8_1448.csv\n",
            "Processing file: class_9_1449.csv\n",
            "Processing file: class_9_1450.csv\n",
            "Processing file: class_3_1451.csv\n",
            "Processing file: class_8_1452.csv\n",
            "Processing file: class_6_1453.csv\n",
            "Processing file: class_3_1454.csv\n",
            "Processing file: class_10_1455.csv\n",
            "Processing file: class_5_1456.csv\n",
            "Processing file: class_4_1457.csv\n",
            "Processing file: class_4_1458.csv\n",
            "Processing file: class_10_1459.csv\n",
            "Processing file: class_1_1460.csv\n",
            "Processing file: class_6_1461.csv\n",
            "Processing file: class_6_1462.csv\n",
            "Processing file: class_7_1463.csv\n",
            "Processing file: class_8_1464.csv\n",
            "Processing file: class_4_1465.csv\n",
            "Processing file: class_8_1466.csv\n",
            "Processing file: class_8_1467.csv\n",
            "Processing file: class_10_1468.csv\n",
            "Processing file: class_3_1469.csv\n",
            "Processing file: class_9_1470.csv\n",
            "Processing file: class_4_1471.csv\n",
            "Processing file: class_7_1472.csv\n",
            "Processing file: class_7_1473.csv\n",
            "Processing file: class_5_1474.csv\n",
            "Processing file: class_9_1475.csv\n",
            "Processing file: class_6_1476.csv\n",
            "Processing file: class_9_1477.csv\n",
            "Processing file: class_8_1478.csv\n",
            "Processing file: class_6_1479.csv\n",
            "Processing file: class_5_1480.csv\n",
            "Processing file: class_9_1481.csv\n",
            "Processing file: class_8_1482.csv\n",
            "Processing file: class_11_1483.csv\n",
            "Processing file: class_9_1484.csv\n",
            "Processing file: class_0_1485.csv\n",
            "Processing file: class_2_1486.csv\n",
            "Processing file: class_0_1487.csv\n",
            "Processing file: class_3_1488.csv\n",
            "Processing file: class_8_1489.csv\n",
            "Processing file: class_6_1490.csv\n",
            "Processing file: class_3_1491.csv\n",
            "Processing file: class_2_1492.csv\n",
            "Processing file: class_6_1493.csv\n",
            "Processing file: class_4_1494.csv\n",
            "Processing file: class_5_1495.csv\n",
            "Processing file: class_11_1496.csv\n",
            "Processing file: class_5_1497.csv\n",
            "Processing file: class_2_1498.csv\n",
            "Processing file: class_3_1499.csv\n",
            "Processing file: class_4_1500.csv\n",
            "Processing file: class_4_1501.csv\n",
            "Processing file: class_5_1502.csv\n",
            "Processing file: class_9_1503.csv\n",
            "Processing file: class_3_1504.csv\n",
            "Processing file: class_7_1505.csv\n",
            "Processing file: class_4_1506.csv\n",
            "Processing file: class_11_1507.csv\n",
            "Processing file: class_11_1508.csv\n",
            "Processing file: class_0_1509.csv\n",
            "Processing file: class_11_1510.csv\n",
            "Processing file: class_2_1511.csv\n",
            "Processing file: class_1_1512.csv\n",
            "Processing file: class_2_1513.csv\n",
            "Processing file: class_1_1514.csv\n",
            "Processing file: class_5_1515.csv\n",
            "Processing file: class_9_1516.csv\n",
            "Processing file: class_7_1517.csv\n",
            "Processing file: class_5_1518.csv\n",
            "Processing file: class_2_1519.csv\n",
            "Processing file: class_11_1520.csv\n",
            "Processing file: class_1_1521.csv\n",
            "Processing file: class_11_1522.csv\n",
            "Processing file: class_5_1523.csv\n",
            "Processing file: class_1_1524.csv\n",
            "Processing file: class_10_1525.csv\n",
            "Processing file: class_2_1526.csv\n",
            "Processing file: class_4_1527.csv\n",
            "Processing file: class_8_1528.csv\n",
            "Processing file: class_2_1529.csv\n",
            "Processing file: class_10_1530.csv\n",
            "Processing file: class_4_1531.csv\n",
            "Processing file: class_1_1532.csv\n",
            "Processing file: class_2_1533.csv\n",
            "Processing file: class_3_1534.csv\n",
            "Processing file: class_8_1535.csv\n",
            "Processing file: class_7_1536.csv\n",
            "Processing file: class_5_1537.csv\n",
            "Processing file: class_8_1538.csv\n",
            "Processing file: class_1_1539.csv\n",
            "Processing file: class_4_1540.csv\n",
            "Processing file: class_5_1541.csv\n",
            "Processing file: class_3_1542.csv\n",
            "Processing file: class_6_1543.csv\n",
            "Processing file: class_7_1544.csv\n",
            "Processing file: class_8_1545.csv\n",
            "Processing file: class_7_1546.csv\n",
            "Processing file: class_11_1547.csv\n",
            "Processing file: class_10_1548.csv\n",
            "Processing file: class_4_1549.csv\n",
            "Processing file: class_11_1550.csv\n",
            "Processing file: class_8_1551.csv\n",
            "Processing file: class_7_1552.csv\n",
            "Processing file: class_8_1553.csv\n",
            "Processing file: class_0_1554.csv\n",
            "Processing file: class_11_1555.csv\n",
            "Processing file: class_8_1556.csv\n",
            "Processing file: class_0_1557.csv\n",
            "Processing file: class_0_1558.csv\n",
            "Processing file: class_6_1559.csv\n",
            "Processing file: class_0_1560.csv\n",
            "Processing file: class_5_1561.csv\n",
            "Processing file: class_9_1562.csv\n",
            "Processing file: class_9_1563.csv\n",
            "Processing file: class_4_1564.csv\n",
            "Processing file: class_10_1565.csv\n",
            "Processing file: class_7_1566.csv\n",
            "Processing file: class_0_1567.csv\n",
            "Processing file: class_8_1568.csv\n",
            "Processing file: class_4_1569.csv\n",
            "Processing file: class_10_1570.csv\n",
            "Processing file: class_4_1571.csv\n",
            "Processing file: class_0_1572.csv\n",
            "Processing file: class_7_1573.csv\n",
            "Processing file: class_9_1574.csv\n",
            "Processing file: class_8_1575.csv\n",
            "Processing file: class_1_1576.csv\n",
            "Processing file: class_0_1577.csv\n",
            "Processing file: class_1_1578.csv\n",
            "Processing file: class_6_1579.csv\n",
            "Processing file: class_9_1580.csv\n",
            "Processing file: class_10_1581.csv\n",
            "Processing file: class_4_1582.csv\n",
            "Processing file: class_4_1583.csv\n",
            "Processing file: class_0_1584.csv\n",
            "Processing file: class_8_1585.csv\n",
            "Processing file: class_6_1586.csv\n",
            "Processing file: class_2_1587.csv\n",
            "Processing file: class_9_1588.csv\n",
            "Processing file: class_3_1589.csv\n",
            "Processing file: class_9_1590.csv\n",
            "Processing file: class_10_1591.csv\n",
            "Processing file: class_10_1592.csv\n",
            "Processing file: class_5_1593.csv\n",
            "Processing file: class_7_1594.csv\n",
            "Processing file: class_9_1595.csv\n",
            "Processing file: class_6_1596.csv\n",
            "Processing file: class_7_1597.csv\n",
            "Processing file: class_2_1598.csv\n",
            "Processing file: class_8_1599.csv\n",
            "Processing file: class_7_1600.csv\n",
            "Processing file: class_10_1601.csv\n",
            "Processing file: class_1_1602.csv\n",
            "Processing file: class_4_1603.csv\n",
            "Processing file: class_5_1604.csv\n",
            "Processing file: class_5_1605.csv\n",
            "Processing file: class_2_1606.csv\n",
            "Processing file: class_4_1607.csv\n",
            "Processing file: class_5_1608.csv\n",
            "Processing file: class_3_1609.csv\n",
            "Processing file: class_11_1610.csv\n",
            "Processing file: class_4_1611.csv\n",
            "Processing file: class_11_1612.csv\n",
            "Processing file: class_1_1613.csv\n",
            "Processing file: class_2_1614.csv\n",
            "Processing file: class_5_1615.csv\n",
            "Processing file: class_8_1616.csv\n",
            "Processing file: class_8_1617.csv\n",
            "Processing file: class_5_1618.csv\n",
            "Processing file: class_5_1619.csv\n",
            "Processing file: class_4_1620.csv\n",
            "Processing file: class_9_1621.csv\n",
            "Processing file: class_6_1622.csv\n",
            "Processing file: class_5_1623.csv\n",
            "Processing file: class_0_1624.csv\n",
            "Processing file: class_9_1625.csv\n",
            "Processing file: class_5_1626.csv\n",
            "Processing file: class_7_1627.csv\n",
            "Processing file: class_7_1628.csv\n",
            "Processing file: class_4_1629.csv\n",
            "Processing file: class_9_1630.csv\n",
            "Processing file: class_6_1631.csv\n",
            "Processing file: class_8_1632.csv\n",
            "Processing file: class_5_1633.csv\n",
            "Processing file: class_11_1634.csv\n",
            "Processing file: class_0_1635.csv\n",
            "Processing file: class_7_1636.csv\n",
            "Processing file: class_8_1637.csv\n",
            "Processing file: class_7_1638.csv\n",
            "Processing file: class_0_1639.csv\n",
            "Processing file: class_1_1640.csv\n",
            "Processing file: class_0_1641.csv\n",
            "Processing file: class_0_1642.csv\n",
            "Processing file: class_2_1643.csv\n",
            "Processing file: class_5_1644.csv\n",
            "Processing file: class_5_1645.csv\n",
            "Processing file: class_7_1646.csv\n",
            "Processing file: class_4_1647.csv\n",
            "Processing file: class_11_1648.csv\n",
            "Processing file: class_7_1649.csv\n",
            "Processing file: class_7_1650.csv\n",
            "Processing file: class_5_1651.csv\n",
            "Processing file: class_7_1652.csv\n",
            "Processing file: class_3_1653.csv\n",
            "Processing file: class_1_1654.csv\n",
            "Processing file: class_8_1655.csv\n",
            "Processing file: class_2_1656.csv\n",
            "Processing file: class_3_1657.csv\n",
            "Processing file: class_11_1658.csv\n",
            "Processing file: class_7_1659.csv\n",
            "Processing file: class_5_1660.csv\n",
            "Processing file: class_8_1661.csv\n",
            "Processing file: class_9_1662.csv\n",
            "Processing file: class_10_1663.csv\n",
            "Processing file: class_1_1664.csv\n",
            "Processing file: class_3_1665.csv\n",
            "Processing file: class_6_1666.csv\n",
            "Processing file: class_7_1667.csv\n",
            "Processing file: class_8_1668.csv\n",
            "Processing file: class_1_1669.csv\n",
            "Processing file: class_0_1670.csv\n",
            "Processing file: class_4_1671.csv\n",
            "Processing file: class_8_1672.csv\n",
            "Processing file: class_3_1673.csv\n",
            "Processing file: class_1_1674.csv\n",
            "Processing file: class_6_1675.csv\n",
            "Processing file: class_11_1676.csv\n",
            "Processing file: class_5_1677.csv\n",
            "Processing file: class_2_1678.csv\n",
            "Processing file: class_6_1679.csv\n",
            "Processing file: class_2_1680.csv\n",
            "Processing file: class_3_1681.csv\n",
            "Processing file: class_11_1682.csv\n",
            "Processing file: class_2_1683.csv\n",
            "Processing file: class_9_1684.csv\n",
            "Processing file: class_1_1685.csv\n",
            "Processing file: class_0_1686.csv\n",
            "Processing file: class_4_1687.csv\n",
            "Processing file: class_8_1688.csv\n",
            "Processing file: class_1_1689.csv\n",
            "Processing file: class_2_1690.csv\n",
            "Processing file: class_10_1691.csv\n",
            "Processing file: class_6_1692.csv\n",
            "Processing file: class_10_1693.csv\n",
            "Processing file: class_2_1694.csv\n",
            "Processing file: class_4_1695.csv\n",
            "Processing file: class_4_1696.csv\n",
            "Processing file: class_9_1697.csv\n",
            "Processing file: class_6_1698.csv\n",
            "Processing file: class_0_1699.csv\n",
            "Processing file: class_0_1700.csv\n",
            "Processing file: class_4_1701.csv\n",
            "Processing file: class_5_1702.csv\n",
            "Processing file: class_0_1703.csv\n",
            "Processing file: class_7_1704.csv\n",
            "Processing file: class_4_1705.csv\n",
            "Processing file: class_11_1706.csv\n",
            "Processing file: class_5_1707.csv\n",
            "Processing file: class_2_1708.csv\n",
            "Processing file: class_4_1709.csv\n",
            "Processing file: class_0_1710.csv\n",
            "Processing file: class_6_1711.csv\n",
            "Processing file: class_11_1712.csv\n",
            "Processing file: class_5_1713.csv\n",
            "Processing file: class_9_1714.csv\n",
            "Processing file: class_11_1715.csv\n",
            "Processing file: class_4_1716.csv\n",
            "Processing file: class_6_1717.csv\n",
            "Processing file: class_4_1718.csv\n",
            "Processing file: class_5_1719.csv\n",
            "Processing file: class_7_1720.csv\n",
            "Processing file: class_10_1721.csv\n",
            "Processing file: class_3_1722.csv\n",
            "Processing file: class_4_1723.csv\n",
            "Processing file: class_8_1724.csv\n",
            "Processing file: class_6_1725.csv\n",
            "Processing file: class_11_1726.csv\n",
            "Processing file: class_0_1727.csv\n",
            "Processing file: class_4_1728.csv\n",
            "Processing file: class_3_1729.csv\n",
            "Processing file: class_9_1730.csv\n",
            "Processing file: class_7_1731.csv\n",
            "Processing file: class_11_1732.csv\n",
            "Processing file: class_3_1733.csv\n",
            "Processing file: class_2_1734.csv\n",
            "Processing file: class_11_1735.csv\n",
            "Processing file: class_10_1736.csv\n",
            "Processing file: class_8_1737.csv\n",
            "Processing file: class_3_1738.csv\n",
            "Processing file: class_4_1739.csv\n",
            "Processing file: class_1_1740.csv\n",
            "Processing file: class_8_1741.csv\n",
            "Processing file: class_11_1742.csv\n",
            "Processing file: class_2_1743.csv\n",
            "Processing file: class_7_1744.csv\n",
            "Processing file: class_1_1745.csv\n",
            "Processing file: class_4_1746.csv\n",
            "Processing file: class_6_1747.csv\n",
            "Processing file: class_4_1748.csv\n",
            "Processing file: class_1_1749.csv\n",
            "Processing file: class_9_1750.csv\n",
            "Processing file: class_9_1751.csv\n",
            "Processing file: class_1_1752.csv\n",
            "Processing file: class_11_1753.csv\n",
            "Processing file: class_5_1754.csv\n",
            "Processing file: class_7_1755.csv\n",
            "Processing file: class_6_1756.csv\n",
            "Processing file: class_3_1757.csv\n",
            "Processing file: class_4_1758.csv\n",
            "Processing file: class_4_1759.csv\n",
            "Processing file: class_1_1760.csv\n",
            "Processing file: class_4_1761.csv\n",
            "Processing file: class_5_1762.csv\n",
            "Processing file: class_4_1763.csv\n",
            "Processing file: class_6_1764.csv\n",
            "Processing file: class_3_1765.csv\n",
            "Processing file: class_1_1766.csv\n",
            "Processing file: class_2_1767.csv\n",
            "Processing file: class_8_1768.csv\n",
            "Processing file: class_9_1769.csv\n",
            "Processing file: class_10_1770.csv\n",
            "Processing file: class_10_1771.csv\n",
            "Processing file: class_4_1772.csv\n",
            "Processing file: class_8_1773.csv\n",
            "Processing file: class_9_1774.csv\n",
            "Processing file: class_0_1775.csv\n",
            "Processing file: class_1_1776.csv\n",
            "Processing file: class_10_1777.csv\n",
            "Processing file: class_8_1778.csv\n",
            "Processing file: class_5_1779.csv\n",
            "Processing file: class_6_1780.csv\n",
            "Processing file: class_5_1781.csv\n",
            "Processing file: class_7_1782.csv\n",
            "Processing file: class_4_1783.csv\n",
            "Processing file: class_3_1784.csv\n",
            "Processing file: class_5_1785.csv\n",
            "Processing file: class_4_1786.csv\n",
            "Processing file: class_2_1787.csv\n",
            "Processing file: class_8_1788.csv\n",
            "Processing file: class_6_1789.csv\n",
            "Processing file: class_7_1790.csv\n",
            "Processing file: class_1_1791.csv\n",
            "Processing file: class_5_1792.csv\n",
            "Processing file: class_9_1793.csv\n",
            "Processing file: class_4_1794.csv\n",
            "Processing file: class_6_1795.csv\n",
            "Processing file: class_9_1796.csv\n",
            "Processing file: class_10_1797.csv\n",
            "Processing file: class_0_1798.csv\n",
            "Processing file: class_0_1799.csv\n",
            "Processing file: class_8_1800.csv\n",
            "Processing file: class_5_1801.csv\n",
            "Processing file: class_11_1802.csv\n",
            "Processing file: class_1_1803.csv\n",
            "Processing file: class_4_1804.csv\n",
            "Processing file: class_11_1805.csv\n",
            "Processing file: class_1_1806.csv\n",
            "Processing file: class_10_1807.csv\n",
            "Processing file: class_11_1808.csv\n",
            "Processing file: class_2_1809.csv\n",
            "Processing file: class_3_1810.csv\n",
            "Processing file: class_2_1811.csv\n",
            "Processing file: class_3_1812.csv\n",
            "Processing file: class_1_1813.csv\n",
            "Processing file: class_5_1814.csv\n",
            "Processing file: class_2_1815.csv\n",
            "Processing file: class_2_1816.csv\n",
            "Processing file: class_11_1817.csv\n",
            "Processing file: class_2_1818.csv\n",
            "Processing file: class_10_1819.csv\n",
            "Processing file: class_10_1820.csv\n",
            "Processing file: class_0_1821.csv\n",
            "Processing file: class_11_1822.csv\n",
            "Processing file: class_3_1823.csv\n",
            "Processing file: class_0_1824.csv\n",
            "Processing file: class_5_1825.csv\n",
            "Processing file: class_1_1826.csv\n",
            "Processing file: class_4_1827.csv\n",
            "Processing file: class_9_1828.csv\n",
            "Processing file: class_0_1829.csv\n",
            "Processing file: class_3_1830.csv\n",
            "Processing file: class_11_1831.csv\n",
            "Processing file: class_8_1832.csv\n",
            "Processing file: class_7_1833.csv\n",
            "Processing file: class_8_1834.csv\n",
            "Processing file: class_7_1835.csv\n",
            "Processing file: class_4_1836.csv\n",
            "Processing file: class_7_1837.csv\n",
            "Processing file: class_11_1838.csv\n",
            "Processing file: class_9_1839.csv\n",
            "Processing file: class_9_1840.csv\n",
            "Processing file: class_8_1841.csv\n",
            "Processing file: class_2_1842.csv\n",
            "Processing file: class_10_1843.csv\n",
            "Processing file: class_5_1844.csv\n",
            "Processing file: class_4_1845.csv\n",
            "Processing file: class_3_1846.csv\n",
            "Processing file: class_6_1847.csv\n",
            "Processing file: class_7_1848.csv\n",
            "Processing file: class_7_1849.csv\n",
            "Processing file: class_5_1850.csv\n",
            "Processing file: class_3_1851.csv\n",
            "Processing file: class_8_1852.csv\n",
            "Processing file: class_7_1853.csv\n",
            "Processing file: class_5_1854.csv\n",
            "Processing file: class_9_1855.csv\n",
            "Processing file: class_10_1856.csv\n",
            "Processing file: class_1_1857.csv\n",
            "Processing file: class_6_1858.csv\n",
            "Processing file: class_2_1859.csv\n",
            "Processing file: class_5_1860.csv\n",
            "Processing file: class_1_1861.csv\n",
            "Processing file: class_4_1862.csv\n",
            "Processing file: class_9_1863.csv\n",
            "Processing file: class_7_1864.csv\n",
            "Processing file: class_4_1865.csv\n",
            "Processing file: class_2_1866.csv\n",
            "Processing file: class_0_1867.csv\n",
            "Processing file: class_4_1868.csv\n",
            "Processing file: class_1_1869.csv\n",
            "Processing file: class_6_1870.csv\n",
            "Processing file: class_2_1871.csv\n",
            "Processing file: class_2_1872.csv\n",
            "Processing file: class_10_1873.csv\n",
            "Processing file: class_2_1874.csv\n",
            "Processing file: class_0_1875.csv\n",
            "Processing file: class_8_1876.csv\n",
            "Processing file: class_2_1877.csv\n",
            "Processing file: class_4_1878.csv\n",
            "Processing file: class_0_1879.csv\n",
            "Processing file: class_5_1880.csv\n",
            "Processing file: class_3_1881.csv\n",
            "Processing file: class_8_1882.csv\n",
            "Processing file: class_10_1883.csv\n",
            "Processing file: class_5_1884.csv\n",
            "Processing file: class_1_1885.csv\n",
            "Processing file: class_6_1886.csv\n",
            "Processing file: class_8_1887.csv\n",
            "Processing file: class_8_1888.csv\n",
            "Processing file: class_2_1889.csv\n",
            "Processing file: class_2_1890.csv\n",
            "Processing file: class_0_1891.csv\n",
            "Processing file: class_6_1892.csv\n",
            "Processing file: class_7_1893.csv\n",
            "Processing file: class_9_1894.csv\n",
            "Processing file: class_11_1895.csv\n",
            "Processing file: class_2_1896.csv\n",
            "Processing file: class_8_1897.csv\n",
            "Processing file: class_4_1898.csv\n",
            "Processing file: class_7_1899.csv\n",
            "Processing file: class_7_1900.csv\n",
            "Processing file: class_6_1901.csv\n",
            "Processing file: class_7_1902.csv\n",
            "Processing file: class_7_1903.csv\n",
            "Processing file: class_4_1904.csv\n",
            "Processing file: class_8_1905.csv\n",
            "Processing file: class_6_1906.csv\n",
            "Processing file: class_1_1907.csv\n",
            "Processing file: class_10_1908.csv\n",
            "Processing file: class_0_1909.csv\n",
            "Processing file: class_3_1910.csv\n",
            "Processing file: class_3_1911.csv\n",
            "Processing file: class_3_1912.csv\n",
            "Processing file: class_10_1913.csv\n",
            "Processing file: class_9_1914.csv\n",
            "Processing file: class_1_1915.csv\n",
            "Processing file: class_9_1916.csv\n",
            "Processing file: class_7_1917.csv\n",
            "Processing file: class_4_1918.csv\n",
            "Processing file: class_1_1919.csv\n",
            "Processing file: class_8_1920.csv\n",
            "Processing file: class_1_1921.csv\n",
            "Processing file: class_11_1922.csv\n",
            "Processing file: class_3_1923.csv\n",
            "Processing file: class_2_1924.csv\n",
            "Processing file: class_2_1925.csv\n",
            "Processing file: class_3_1926.csv\n",
            "Processing file: class_7_1927.csv\n",
            "Processing file: class_1_1928.csv\n",
            "Processing file: class_10_1929.csv\n",
            "Processing file: class_6_1930.csv\n",
            "Processing file: class_9_1931.csv\n",
            "Processing file: class_7_1932.csv\n",
            "Processing file: class_4_1933.csv\n",
            "Processing file: class_7_1934.csv\n",
            "Processing file: class_1_1935.csv\n",
            "Processing file: class_8_1936.csv\n",
            "Processing file: class_2_1937.csv\n",
            "Processing file: class_0_1938.csv\n",
            "Processing file: class_3_1939.csv\n",
            "Processing file: class_7_1940.csv\n",
            "Processing file: class_10_1941.csv\n",
            "Processing file: class_9_1942.csv\n",
            "Processing file: class_0_1943.csv\n",
            "Processing file: class_8_1944.csv\n",
            "Processing file: class_0_1945.csv\n",
            "Processing file: class_1_1946.csv\n",
            "Processing file: class_6_1947.csv\n",
            "Processing file: class_8_1948.csv\n",
            "Processing file: class_2_1949.csv\n",
            "Processing file: class_0_1950.csv\n",
            "Processing file: class_9_1951.csv\n",
            "Processing file: class_7_1952.csv\n",
            "Processing file: class_11_1953.csv\n",
            "Processing file: class_2_1954.csv\n",
            "Processing file: class_6_1955.csv\n",
            "Processing file: class_0_1956.csv\n",
            "Processing file: class_1_1957.csv\n",
            "Processing file: class_0_1958.csv\n",
            "Processing file: class_8_1959.csv\n",
            "Processing file: class_11_1960.csv\n",
            "Processing file: class_0_1961.csv\n",
            "Processing file: class_10_1962.csv\n",
            "Processing file: class_10_1963.csv\n",
            "Processing file: class_4_1964.csv\n",
            "Processing file: class_1_1965.csv\n",
            "Processing file: class_2_1966.csv\n",
            "Processing file: class_1_1967.csv\n",
            "Processing file: class_8_1968.csv\n",
            "Processing file: class_9_1969.csv\n",
            "Processing file: class_11_1970.csv\n",
            "Processing file: class_4_1971.csv\n",
            "Processing file: class_3_1972.csv\n",
            "Processing file: class_3_1973.csv\n",
            "Processing file: class_11_1974.csv\n",
            "Processing file: class_10_1975.csv\n",
            "Processing file: class_8_1976.csv\n",
            "Processing file: class_3_1977.csv\n",
            "Processing file: class_1_1978.csv\n",
            "Processing file: class_6_1979.csv\n",
            "Processing file: class_5_1980.csv\n",
            "Processing file: class_11_1981.csv\n",
            "Processing file: class_4_1982.csv\n",
            "Processing file: class_0_1983.csv\n",
            "Processing file: class_11_1984.csv\n",
            "Processing file: class_7_1985.csv\n",
            "Processing file: class_3_1986.csv\n",
            "Processing file: class_9_1987.csv\n",
            "Processing file: class_10_1988.csv\n",
            "Processing file: class_2_1989.csv\n",
            "Processing file: class_1_1990.csv\n",
            "Processing file: class_9_1991.csv\n",
            "Processing file: class_7_1992.csv\n",
            "Processing file: class_8_1993.csv\n",
            "Processing file: class_8_1994.csv\n",
            "Processing file: class_1_1995.csv\n",
            "Processing file: class_1_1996.csv\n",
            "Processing file: class_1_1997.csv\n",
            "Processing file: class_6_1998.csv\n",
            "Processing file: class_4_1999.csv\n",
            "Processing file: class_0_2000.csv\n",
            "Processing file: class_3_2001.csv\n",
            "Processing file: class_4_2002.csv\n",
            "Processing file: class_2_2003.csv\n",
            "Processing file: class_0_2004.csv\n",
            "Processing file: class_6_2005.csv\n",
            "Processing file: class_5_2006.csv\n",
            "Processing file: class_2_2007.csv\n",
            "Processing file: class_5_2008.csv\n",
            "Processing file: class_2_2009.csv\n",
            "Processing file: class_0_2010.csv\n",
            "Processing file: class_6_2011.csv\n",
            "Processing file: class_4_2012.csv\n",
            "Processing file: class_8_2013.csv\n",
            "Processing file: class_2_2014.csv\n",
            "Processing file: class_9_2015.csv\n",
            "Processing file: class_4_2016.csv\n",
            "Processing file: class_6_2017.csv\n",
            "Processing file: class_8_2018.csv\n",
            "Processing file: class_9_2019.csv\n",
            "Processing file: class_7_2020.csv\n",
            "Processing file: class_0_2021.csv\n",
            "Processing file: class_8_2022.csv\n",
            "Processing file: class_9_2023.csv\n",
            "Processing file: class_6_2024.csv\n",
            "Processing file: class_1_2025.csv\n",
            "Processing file: class_10_2026.csv\n",
            "Processing file: class_7_2027.csv\n",
            "Processing file: class_3_2028.csv\n",
            "Processing file: class_0_2029.csv\n",
            "Processing file: class_3_2030.csv\n",
            "Processing file: class_5_2031.csv\n",
            "Processing file: class_9_2032.csv\n",
            "Processing file: class_8_2033.csv\n",
            "Processing file: class_8_2034.csv\n",
            "Processing file: class_2_2035.csv\n",
            "Processing file: class_6_2036.csv\n",
            "Processing file: class_5_2037.csv\n",
            "Processing file: class_2_2038.csv\n",
            "Processing file: class_3_2039.csv\n",
            "Processing file: class_10_2040.csv\n",
            "Processing file: class_1_2041.csv\n",
            "Processing file: class_3_2042.csv\n",
            "Processing file: class_0_2043.csv\n",
            "Processing file: class_6_2044.csv\n",
            "Processing file: class_9_2045.csv\n",
            "Processing file: class_3_2046.csv\n",
            "Processing file: class_1_2047.csv\n",
            "Processing file: class_3_2048.csv\n",
            "Processing file: class_0_2049.csv\n",
            "Processing file: class_9_2050.csv\n",
            "Processing file: class_7_2051.csv\n",
            "Processing file: class_8_2052.csv\n",
            "Processing file: class_0_2053.csv\n",
            "Processing file: class_2_2054.csv\n",
            "Processing file: class_3_2055.csv\n",
            "Processing file: class_4_2056.csv\n",
            "Processing file: class_10_2057.csv\n",
            "Processing file: class_9_2058.csv\n",
            "Processing file: class_6_2059.csv\n",
            "Processing file: class_10_2060.csv\n",
            "Processing file: class_6_2061.csv\n",
            "Processing file: class_10_2062.csv\n",
            "Processing file: class_9_2063.csv\n",
            "Processing file: class_11_2064.csv\n",
            "Processing file: class_4_2065.csv\n",
            "Processing file: class_6_2066.csv\n",
            "Processing file: class_3_2067.csv\n",
            "Processing file: class_11_2068.csv\n",
            "Processing file: class_11_2069.csv\n",
            "Processing file: class_3_2070.csv\n",
            "Processing file: class_7_2071.csv\n",
            "Processing file: class_11_2072.csv\n",
            "Processing file: class_4_2073.csv\n",
            "Processing file: class_9_2074.csv\n",
            "Processing file: class_10_2075.csv\n",
            "Processing file: class_6_2076.csv\n",
            "Processing file: class_8_2077.csv\n",
            "Processing file: class_1_2078.csv\n",
            "Processing file: class_9_2079.csv\n",
            "Processing file: class_6_2080.csv\n",
            "Processing file: class_2_2081.csv\n",
            "Processing file: class_1_2082.csv\n",
            "Processing file: class_3_2083.csv\n",
            "Processing file: class_6_2084.csv\n",
            "Processing file: class_5_2085.csv\n",
            "Processing file: class_7_2086.csv\n",
            "Processing file: class_7_2087.csv\n",
            "Processing file: class_9_2088.csv\n",
            "Processing file: class_8_2089.csv\n",
            "Processing file: class_4_2090.csv\n",
            "Processing file: class_1_2091.csv\n",
            "Processing file: class_8_2092.csv\n",
            "Processing file: class_10_2093.csv\n",
            "Processing file: class_6_2094.csv\n",
            "Processing file: class_9_2095.csv\n",
            "Processing file: class_7_2096.csv\n",
            "Processing file: class_10_2097.csv\n",
            "Processing file: class_10_2098.csv\n",
            "Processing file: class_11_2099.csv\n",
            "Processing file: class_2_2100.csv\n",
            "Processing file: class_0_2101.csv\n",
            "Processing file: class_5_2102.csv\n",
            "Processing file: class_11_2103.csv\n",
            "Processing file: class_7_2104.csv\n",
            "Processing file: class_6_2105.csv\n",
            "Processing file: class_11_2106.csv\n",
            "Processing file: class_1_2107.csv\n",
            "Processing file: class_9_2108.csv\n",
            "Processing file: class_0_2109.csv\n",
            "Processing file: class_6_2110.csv\n",
            "Processing file: class_1_2111.csv\n",
            "Processing file: class_6_2112.csv\n",
            "Processing file: class_0_2113.csv\n",
            "Processing file: class_6_2114.csv\n",
            "Processing file: class_5_2115.csv\n",
            "Processing file: class_11_2116.csv\n",
            "Processing file: class_10_2117.csv\n",
            "Processing file: class_6_2118.csv\n",
            "Processing file: class_10_2119.csv\n",
            "Processing file: class_6_2120.csv\n",
            "Processing file: class_9_2121.csv\n",
            "Processing file: class_8_2122.csv\n",
            "Processing file: class_1_2123.csv\n",
            "Processing file: class_4_2124.csv\n",
            "Processing file: class_10_2125.csv\n",
            "Processing file: class_0_2126.csv\n",
            "Processing file: class_8_2127.csv\n",
            "Processing file: class_3_2128.csv\n",
            "Processing file: class_4_2129.csv\n",
            "Processing file: class_5_2130.csv\n",
            "Processing file: class_5_2131.csv\n",
            "Processing file: class_1_2132.csv\n",
            "Processing file: class_11_2133.csv\n",
            "Processing file: class_2_2134.csv\n",
            "Processing file: class_5_2135.csv\n",
            "Processing file: class_0_2136.csv\n",
            "Processing file: class_3_2137.csv\n",
            "Processing file: class_6_2138.csv\n",
            "Processing file: class_10_2139.csv\n",
            "Processing file: class_6_2140.csv\n",
            "Processing file: class_9_2141.csv\n",
            "Processing file: class_6_2142.csv\n",
            "Processing file: class_3_2143.csv\n",
            "Processing file: class_0_2144.csv\n",
            "Processing file: class_11_2145.csv\n",
            "Processing file: class_7_2146.csv\n",
            "Processing file: class_2_2147.csv\n",
            "Processing file: class_7_2148.csv\n",
            "Processing file: class_6_2149.csv\n",
            "Processing file: class_5_2150.csv\n",
            "Processing file: class_11_2151.csv\n",
            "Processing file: class_5_2152.csv\n",
            "Processing file: class_8_2153.csv\n",
            "Processing file: class_7_2154.csv\n",
            "Processing file: class_11_2155.csv\n",
            "Processing file: class_10_2156.csv\n",
            "Processing file: class_7_2157.csv\n",
            "Processing file: class_11_2158.csv\n",
            "Processing file: class_5_2159.csv\n",
            "Processing file: class_3_2160.csv\n",
            "Processing file: class_1_2161.csv\n",
            "Processing file: class_10_2162.csv\n",
            "Processing file: class_10_2163.csv\n",
            "Processing file: class_5_2164.csv\n",
            "Processing file: class_0_2165.csv\n",
            "Processing file: class_1_2166.csv\n",
            "Processing file: class_4_2167.csv\n",
            "Processing file: class_10_2168.csv\n",
            "Processing file: class_1_2169.csv\n",
            "Processing file: class_10_2170.csv\n",
            "Processing file: class_10_2171.csv\n",
            "Processing file: class_3_2172.csv\n",
            "Processing file: class_5_2173.csv\n",
            "Processing file: class_4_2174.csv\n",
            "Processing file: class_11_2175.csv\n",
            "Processing file: class_9_2176.csv\n",
            "Processing file: class_8_2177.csv\n",
            "Processing file: class_3_2178.csv\n",
            "Processing file: class_2_2179.csv\n",
            "Processing file: class_5_2180.csv\n",
            "Processing file: class_9_2181.csv\n",
            "Processing file: class_8_2182.csv\n",
            "Processing file: class_4_2183.csv\n",
            "Processing file: class_4_2184.csv\n",
            "Processing file: class_3_2185.csv\n",
            "Processing file: class_10_2186.csv\n",
            "Processing file: class_10_2187.csv\n",
            "Processing file: class_8_2188.csv\n",
            "Processing file: class_7_2189.csv\n",
            "Processing file: class_0_2190.csv\n",
            "Processing file: class_0_2191.csv\n",
            "Processing file: class_6_2192.csv\n",
            "Processing file: class_11_2193.csv\n",
            "Processing file: class_8_2194.csv\n",
            "Processing file: class_10_2195.csv\n",
            "Processing file: class_2_2196.csv\n",
            "Processing file: class_9_2197.csv\n",
            "Processing file: class_8_2198.csv\n",
            "Processing file: class_4_2199.csv\n",
            "Processing file: class_6_2200.csv\n",
            "Processing file: class_6_2201.csv\n",
            "Processing file: class_0_2202.csv\n",
            "Processing file: class_7_2203.csv\n",
            "Processing file: class_7_2204.csv\n",
            "Processing file: class_8_2205.csv\n",
            "Processing file: class_3_2206.csv\n",
            "Processing file: class_2_2207.csv\n",
            "Processing file: class_5_2208.csv\n",
            "Processing file: class_11_2209.csv\n",
            "Processing file: class_2_2210.csv\n",
            "Processing file: class_0_2211.csv\n",
            "Processing file: class_2_2212.csv\n",
            "Processing file: class_8_2213.csv\n",
            "Processing file: class_9_2214.csv\n",
            "Processing file: class_2_2215.csv\n",
            "Processing file: class_5_2216.csv\n",
            "Processing file: class_6_2217.csv\n",
            "Processing file: class_10_2218.csv\n",
            "Processing file: class_5_2219.csv\n",
            "Processing file: class_0_2220.csv\n",
            "Processing file: class_9_2221.csv\n",
            "Processing file: class_7_2222.csv\n",
            "Processing file: class_0_2223.csv\n",
            "Processing file: class_10_2224.csv\n",
            "Processing file: class_4_2225.csv\n",
            "Processing file: class_9_2226.csv\n",
            "Processing file: class_11_2227.csv\n",
            "Processing file: class_4_2228.csv\n",
            "Processing file: class_5_2229.csv\n",
            "Processing file: class_4_2230.csv\n",
            "Processing file: class_1_2231.csv\n",
            "Processing file: class_2_2232.csv\n",
            "Processing file: class_2_2233.csv\n",
            "Processing file: class_4_2234.csv\n",
            "Processing file: class_8_2235.csv\n",
            "Processing file: class_9_2236.csv\n",
            "Processing file: class_6_2237.csv\n",
            "Processing file: class_7_2238.csv\n",
            "Processing file: class_7_2239.csv\n",
            "Processing file: class_6_2240.csv\n",
            "Processing file: class_1_2241.csv\n",
            "Processing file: class_0_2242.csv\n",
            "Processing file: class_2_2243.csv\n",
            "Processing file: class_10_2244.csv\n",
            "Processing file: class_2_2245.csv\n",
            "Processing file: class_0_2246.csv\n",
            "Processing file: class_10_2247.csv\n",
            "Processing file: class_4_2248.csv\n",
            "Processing file: class_9_2249.csv\n",
            "Processing file: class_1_2250.csv\n",
            "Processing file: class_1_2251.csv\n",
            "Processing file: class_8_2252.csv\n",
            "Processing file: class_2_2253.csv\n",
            "Processing file: class_4_2254.csv\n",
            "Processing file: class_1_2255.csv\n",
            "Processing file: class_8_2256.csv\n",
            "Processing file: class_11_2257.csv\n",
            "Processing file: class_4_2258.csv\n",
            "Processing file: class_0_2259.csv\n",
            "Processing file: class_4_2260.csv\n",
            "Processing file: class_8_2261.csv\n",
            "Processing file: class_4_2262.csv\n",
            "Processing file: class_3_2263.csv\n",
            "Processing file: class_10_2264.csv\n",
            "Processing file: class_8_2265.csv\n",
            "Processing file: class_4_2266.csv\n",
            "Processing file: class_8_2267.csv\n",
            "Processing file: class_10_2268.csv\n",
            "Processing file: class_7_2269.csv\n",
            "Processing file: class_7_2270.csv\n",
            "Processing file: class_5_2271.csv\n",
            "Processing file: class_5_2272.csv\n",
            "Processing file: class_4_2273.csv\n",
            "Processing file: class_9_2274.csv\n",
            "Processing file: class_6_2275.csv\n",
            "Processing file: class_5_2276.csv\n",
            "Processing file: class_11_2277.csv\n",
            "Processing file: class_2_2278.csv\n",
            "Processing file: class_10_2279.csv\n",
            "Processing file: class_3_2280.csv\n",
            "Processing file: class_2_2281.csv\n",
            "Processing file: class_7_2282.csv\n",
            "Processing file: class_2_2283.csv\n",
            "Processing file: class_6_2284.csv\n",
            "Processing file: class_7_2285.csv\n",
            "Processing file: class_5_2286.csv\n",
            "Processing file: class_7_2287.csv\n",
            "Processing file: class_9_2288.csv\n",
            "Processing file: class_9_2289.csv\n",
            "Processing file: class_3_2290.csv\n",
            "Processing file: class_3_2291.csv\n",
            "Processing file: class_3_2292.csv\n",
            "Processing file: class_5_2293.csv\n",
            "Processing file: class_7_2294.csv\n",
            "Processing file: class_8_2295.csv\n",
            "Processing file: class_3_2296.csv\n",
            "Processing file: class_9_2297.csv\n",
            "Processing file: class_4_2298.csv\n",
            "Processing file: class_7_2299.csv\n",
            "Processing file: class_10_2300.csv\n",
            "Processing file: class_9_2301.csv\n",
            "Processing file: class_6_2302.csv\n",
            "Processing file: class_2_2303.csv\n",
            "Processing file: class_0_2304.csv\n",
            "Processing file: class_0_2305.csv\n",
            "Processing file: class_7_2306.csv\n",
            "Processing file: class_6_2307.csv\n",
            "Processing file: class_0_2308.csv\n",
            "Processing file: class_4_2309.csv\n",
            "Processing file: class_7_2310.csv\n",
            "Processing file: class_11_2311.csv\n",
            "Processing file: class_2_2312.csv\n",
            "Processing file: class_11_2313.csv\n",
            "Processing file: class_4_2314.csv\n",
            "Processing file: class_6_2315.csv\n",
            "Processing file: class_0_2316.csv\n",
            "Processing file: class_8_2317.csv\n",
            "Processing file: class_6_2318.csv\n",
            "Processing file: class_9_2319.csv\n",
            "Processing file: class_4_2320.csv\n",
            "Processing file: class_3_2321.csv\n",
            "Processing file: class_10_2322.csv\n",
            "Processing file: class_4_2323.csv\n",
            "Processing file: class_8_2324.csv\n",
            "Processing file: class_2_2325.csv\n",
            "Processing file: class_4_2326.csv\n",
            "Processing file: class_4_2327.csv\n",
            "Processing file: class_9_2328.csv\n",
            "Processing file: class_0_2329.csv\n",
            "Processing file: class_2_2330.csv\n",
            "Processing file: class_3_2331.csv\n",
            "Processing file: class_3_2332.csv\n",
            "Processing file: class_6_2333.csv\n",
            "Processing file: class_9_2334.csv\n",
            "Processing file: class_6_2335.csv\n",
            "Processing file: class_7_2336.csv\n",
            "Processing file: class_8_2337.csv\n",
            "Processing file: class_5_2338.csv\n",
            "Processing file: class_4_2339.csv\n",
            "Processing file: class_8_2340.csv\n",
            "Processing file: class_3_2341.csv\n",
            "Processing file: class_11_2342.csv\n",
            "Processing file: class_9_2343.csv\n",
            "Processing file: class_11_2344.csv\n",
            "Processing file: class_2_2345.csv\n",
            "Processing file: class_2_2346.csv\n",
            "Processing file: class_8_2347.csv\n",
            "Processing file: class_11_2348.csv\n",
            "Processing file: class_8_2349.csv\n",
            "Processing file: class_10_2350.csv\n",
            "Processing file: class_8_2351.csv\n",
            "Processing file: class_11_2352.csv\n",
            "Processing file: class_10_2353.csv\n",
            "Processing file: class_4_2354.csv\n",
            "Processing file: class_3_2355.csv\n",
            "Processing file: class_5_2356.csv\n",
            "Processing file: class_2_2357.csv\n",
            "Processing file: class_11_2358.csv\n",
            "Processing file: class_1_2359.csv\n",
            "Processing file: class_1_2360.csv\n",
            "Processing file: class_11_2361.csv\n",
            "Processing file: class_6_2362.csv\n",
            "Processing file: class_2_2363.csv\n",
            "Processing file: class_6_2364.csv\n",
            "Processing file: class_11_2365.csv\n",
            "Processing file: class_11_2366.csv\n",
            "Processing file: class_7_2367.csv\n",
            "Processing file: class_9_2368.csv\n",
            "Processing file: class_10_2369.csv\n",
            "Processing file: class_2_2370.csv\n",
            "Processing file: class_1_2371.csv\n",
            "Processing file: class_5_2372.csv\n",
            "Processing file: class_9_2373.csv\n",
            "Processing file: class_3_2374.csv\n",
            "Processing file: class_0_2375.csv\n",
            "Processing file: class_11_2376.csv\n",
            "Processing file: class_1_2377.csv\n",
            "Processing file: class_10_2378.csv\n",
            "Processing file: class_6_2379.csv\n",
            "Processing file: class_10_2380.csv\n",
            "Processing file: class_7_2381.csv\n",
            "Processing file: class_11_2382.csv\n",
            "Processing file: class_10_2383.csv\n",
            "Processing file: class_10_2384.csv\n",
            "Processing file: class_5_2385.csv\n",
            "Processing file: class_2_2386.csv\n",
            "Processing file: class_9_2387.csv\n",
            "Processing file: class_8_2388.csv\n",
            "Processing file: class_2_2389.csv\n",
            "Processing file: class_11_2390.csv\n",
            "Processing file: class_7_2391.csv\n",
            "Processing file: class_1_2392.csv\n",
            "Processing file: class_9_2393.csv\n",
            "Processing file: class_10_2394.csv\n",
            "Processing file: class_1_2395.csv\n",
            "Processing file: class_1_2396.csv\n",
            "Processing file: class_3_2397.csv\n",
            "Processing file: class_1_2398.csv\n",
            "Processing file: class_6_2399.csv\n",
            "Processing file: class_9_400.csv\n",
            "Processing file: class_9_401.csv\n",
            "Processing file: class_9_402.csv\n",
            "Processing file: class_2_403.csv\n",
            "Processing file: class_11_404.csv\n",
            "Processing file: class_4_405.csv\n",
            "Processing file: class_0_406.csv\n",
            "Processing file: class_4_407.csv\n",
            "Processing file: class_10_408.csv\n",
            "Processing file: class_8_409.csv\n",
            "Processing file: class_5_410.csv\n",
            "Processing file: class_7_411.csv\n",
            "Processing file: class_4_412.csv\n",
            "Processing file: class_5_413.csv\n",
            "Processing file: class_5_414.csv\n",
            "Processing file: class_10_415.csv\n",
            "Processing file: class_0_416.csv\n",
            "Processing file: class_11_417.csv\n",
            "Processing file: class_0_418.csv\n",
            "Processing file: class_2_419.csv\n",
            "Processing file: class_2_420.csv\n",
            "Processing file: class_5_421.csv\n",
            "Processing file: class_2_422.csv\n",
            "Processing file: class_4_423.csv\n",
            "Processing file: class_6_424.csv\n",
            "Processing file: class_1_425.csv\n",
            "Processing file: class_3_426.csv\n",
            "Processing file: class_10_427.csv\n",
            "Processing file: class_0_428.csv\n",
            "Processing file: class_8_429.csv\n",
            "Processing file: class_9_430.csv\n",
            "Processing file: class_6_431.csv\n",
            "Processing file: class_6_432.csv\n",
            "Processing file: class_0_433.csv\n",
            "Processing file: class_4_434.csv\n",
            "Processing file: class_0_435.csv\n",
            "Processing file: class_10_436.csv\n",
            "Processing file: class_2_437.csv\n",
            "Processing file: class_4_438.csv\n",
            "Processing file: class_9_439.csv\n",
            "Processing file: class_1_440.csv\n",
            "Processing file: class_6_441.csv\n",
            "Processing file: class_7_442.csv\n",
            "Processing file: class_10_443.csv\n",
            "Processing file: class_3_444.csv\n",
            "Processing file: class_2_445.csv\n",
            "Processing file: class_0_446.csv\n",
            "Processing file: class_8_447.csv\n",
            "Processing file: class_4_448.csv\n",
            "Processing file: class_3_449.csv\n",
            "Processing file: class_11_450.csv\n",
            "Processing file: class_0_451.csv\n",
            "Processing file: class_11_452.csv\n",
            "Processing file: class_0_453.csv\n",
            "Processing file: class_6_454.csv\n",
            "Processing file: class_6_455.csv\n",
            "Processing file: class_5_456.csv\n",
            "Processing file: class_11_457.csv\n",
            "Processing file: class_5_458.csv\n",
            "Processing file: class_2_459.csv\n",
            "Processing file: class_1_460.csv\n",
            "Processing file: class_11_461.csv\n",
            "Processing file: class_8_462.csv\n",
            "Processing file: class_11_463.csv\n",
            "Processing file: class_7_464.csv\n",
            "Processing file: class_11_465.csv\n",
            "Processing file: class_9_466.csv\n",
            "Processing file: class_6_467.csv\n",
            "Processing file: class_0_468.csv\n",
            "Processing file: class_3_469.csv\n",
            "Processing file: class_10_470.csv\n",
            "Processing file: class_11_471.csv\n",
            "Processing file: class_0_472.csv\n",
            "Processing file: class_5_473.csv\n",
            "Processing file: class_2_474.csv\n",
            "Processing file: class_11_475.csv\n",
            "Processing file: class_1_476.csv\n",
            "Processing file: class_2_477.csv\n",
            "Processing file: class_5_478.csv\n",
            "Processing file: class_4_479.csv\n",
            "Processing file: class_6_480.csv\n",
            "Processing file: class_3_481.csv\n",
            "Processing file: class_1_482.csv\n",
            "Processing file: class_2_483.csv\n",
            "Processing file: class_6_484.csv\n",
            "Processing file: class_11_485.csv\n",
            "Processing file: class_2_486.csv\n",
            "Processing file: class_11_487.csv\n",
            "Processing file: class_0_488.csv\n",
            "Processing file: class_2_489.csv\n",
            "Processing file: class_8_490.csv\n",
            "Processing file: class_10_491.csv\n",
            "Processing file: class_2_492.csv\n",
            "Processing file: class_8_493.csv\n",
            "Processing file: class_2_494.csv\n",
            "Processing file: class_9_495.csv\n",
            "Processing file: class_10_496.csv\n",
            "Processing file: class_10_497.csv\n",
            "Processing file: class_1_498.csv\n",
            "Processing file: class_7_499.csv\n",
            "Processing file: class_1_500.csv\n",
            "Processing file: class_0_501.csv\n",
            "Processing file: class_0_502.csv\n",
            "Processing file: class_11_503.csv\n",
            "Processing file: class_7_504.csv\n",
            "Processing file: class_3_505.csv\n",
            "Processing file: class_1_506.csv\n",
            "Processing file: class_9_507.csv\n",
            "Processing file: class_4_508.csv\n",
            "Processing file: class_4_509.csv\n",
            "Processing file: class_11_510.csv\n",
            "Processing file: class_3_511.csv\n",
            "Processing file: class_10_512.csv\n",
            "Processing file: class_9_513.csv\n",
            "Processing file: class_3_514.csv\n",
            "Processing file: class_4_515.csv\n",
            "Processing file: class_0_516.csv\n",
            "Processing file: class_3_517.csv\n",
            "Processing file: class_7_518.csv\n",
            "Processing file: class_3_519.csv\n",
            "Processing file: class_2_520.csv\n",
            "Processing file: class_7_521.csv\n",
            "Processing file: class_10_522.csv\n",
            "Processing file: class_9_523.csv\n",
            "Processing file: class_1_524.csv\n",
            "Processing file: class_2_525.csv\n",
            "Processing file: class_0_526.csv\n",
            "Processing file: class_5_527.csv\n",
            "Processing file: class_8_528.csv\n",
            "Processing file: class_6_529.csv\n",
            "Processing file: class_3_530.csv\n",
            "Processing file: class_6_531.csv\n",
            "Processing file: class_0_532.csv\n",
            "Processing file: class_3_533.csv\n",
            "Processing file: class_3_534.csv\n",
            "Processing file: class_7_535.csv\n",
            "Processing file: class_8_536.csv\n",
            "Processing file: class_1_537.csv\n",
            "Processing file: class_2_538.csv\n",
            "Processing file: class_9_539.csv\n",
            "Processing file: class_1_540.csv\n",
            "Processing file: class_5_541.csv\n",
            "Processing file: class_3_542.csv\n",
            "Processing file: class_1_543.csv\n",
            "Processing file: class_1_544.csv\n",
            "Processing file: class_1_545.csv\n",
            "Processing file: class_11_546.csv\n",
            "Processing file: class_9_547.csv\n",
            "Processing file: class_3_548.csv\n",
            "Processing file: class_9_549.csv\n",
            "Processing file: class_5_550.csv\n",
            "Processing file: class_8_551.csv\n",
            "Processing file: class_1_552.csv\n",
            "Processing file: class_9_553.csv\n",
            "Processing file: class_9_554.csv\n",
            "Processing file: class_8_555.csv\n",
            "Processing file: class_10_556.csv\n",
            "Processing file: class_9_557.csv\n",
            "Processing file: class_8_558.csv\n",
            "Processing file: class_7_559.csv\n",
            "Processing file: class_4_560.csv\n",
            "Processing file: class_4_561.csv\n",
            "Processing file: class_0_562.csv\n",
            "Processing file: class_11_563.csv\n",
            "Processing file: class_2_564.csv\n",
            "Processing file: class_0_565.csv\n",
            "Processing file: class_7_566.csv\n",
            "Processing file: class_6_567.csv\n",
            "Processing file: class_7_568.csv\n",
            "Processing file: class_0_569.csv\n",
            "Processing file: class_6_570.csv\n",
            "Processing file: class_4_571.csv\n",
            "Processing file: class_10_572.csv\n",
            "Processing file: class_4_573.csv\n",
            "Processing file: class_7_574.csv\n",
            "Processing file: class_0_575.csv\n",
            "Processing file: class_1_576.csv\n",
            "Processing file: class_5_577.csv\n",
            "Processing file: class_2_578.csv\n",
            "Processing file: class_5_579.csv\n",
            "Processing file: class_2_580.csv\n",
            "Processing file: class_0_581.csv\n",
            "Processing file: class_9_582.csv\n",
            "Processing file: class_11_583.csv\n",
            "Processing file: class_5_584.csv\n",
            "Processing file: class_4_585.csv\n",
            "Processing file: class_9_586.csv\n",
            "Processing file: class_7_587.csv\n",
            "Processing file: class_10_588.csv\n",
            "Processing file: class_6_589.csv\n",
            "Processing file: class_0_590.csv\n",
            "Processing file: class_4_591.csv\n",
            "Processing file: class_3_592.csv\n",
            "Processing file: class_9_593.csv\n",
            "Processing file: class_2_594.csv\n",
            "Processing file: class_6_595.csv\n",
            "Processing file: class_3_596.csv\n",
            "Processing file: class_4_597.csv\n",
            "Processing file: class_0_598.csv\n",
            "Processing file: class_1_599.csv\n",
            "Processing file: class_7_600.csv\n",
            "Processing file: class_0_601.csv\n",
            "Processing file: class_5_602.csv\n",
            "Processing file: class_2_603.csv\n",
            "Processing file: class_1_604.csv\n",
            "Processing file: class_1_605.csv\n",
            "Processing file: class_2_606.csv\n",
            "Processing file: class_7_607.csv\n",
            "Processing file: class_11_608.csv\n",
            "Processing file: class_4_609.csv\n",
            "Processing file: class_0_610.csv\n",
            "Processing file: class_4_611.csv\n",
            "Processing file: class_6_612.csv\n",
            "Processing file: class_10_613.csv\n",
            "Processing file: class_4_614.csv\n",
            "Processing file: class_4_615.csv\n",
            "Processing file: class_11_616.csv\n",
            "Processing file: class_9_617.csv\n",
            "Processing file: class_0_618.csv\n",
            "Processing file: class_4_619.csv\n",
            "Processing file: class_5_620.csv\n",
            "Processing file: class_10_621.csv\n",
            "Processing file: class_5_622.csv\n",
            "Processing file: class_6_623.csv\n",
            "Processing file: class_6_624.csv\n",
            "Processing file: class_3_625.csv\n",
            "Processing file: class_11_626.csv\n",
            "Processing file: class_9_627.csv\n",
            "Processing file: class_6_628.csv\n",
            "Processing file: class_2_629.csv\n",
            "Processing file: class_10_630.csv\n",
            "Processing file: class_7_631.csv\n",
            "Processing file: class_6_632.csv\n",
            "Processing file: class_3_633.csv\n",
            "Processing file: class_7_634.csv\n",
            "Processing file: class_10_635.csv\n",
            "Processing file: class_9_636.csv\n",
            "Processing file: class_4_637.csv\n",
            "Processing file: class_6_638.csv\n",
            "Processing file: class_5_639.csv\n",
            "Processing file: class_10_640.csv\n",
            "Processing file: class_0_641.csv\n",
            "Processing file: class_9_642.csv\n",
            "Processing file: class_11_643.csv\n",
            "Processing file: class_11_644.csv\n",
            "Processing file: class_4_645.csv\n",
            "Processing file: class_9_646.csv\n",
            "Processing file: class_8_647.csv\n",
            "Processing file: class_1_648.csv\n",
            "Processing file: class_9_649.csv\n",
            "Processing file: class_5_650.csv\n",
            "Processing file: class_9_651.csv\n",
            "Processing file: class_5_652.csv\n",
            "Processing file: class_11_653.csv\n",
            "Processing file: class_3_654.csv\n",
            "Processing file: class_2_655.csv\n",
            "Processing file: class_9_656.csv\n",
            "Processing file: class_9_657.csv\n",
            "Processing file: class_2_658.csv\n",
            "Processing file: class_9_659.csv\n",
            "Processing file: class_4_660.csv\n",
            "Processing file: class_4_661.csv\n",
            "Processing file: class_1_662.csv\n",
            "Processing file: class_9_663.csv\n",
            "Processing file: class_6_664.csv\n",
            "Processing file: class_9_665.csv\n",
            "Processing file: class_3_666.csv\n",
            "Processing file: class_10_667.csv\n",
            "Processing file: class_6_668.csv\n",
            "Processing file: class_8_669.csv\n",
            "Processing file: class_6_670.csv\n",
            "Processing file: class_4_671.csv\n",
            "Processing file: class_1_672.csv\n",
            "Processing file: class_2_673.csv\n",
            "Processing file: class_5_674.csv\n",
            "Processing file: class_7_675.csv\n",
            "Processing file: class_10_676.csv\n",
            "Processing file: class_0_677.csv\n",
            "Processing file: class_8_678.csv\n",
            "Processing file: class_3_679.csv\n",
            "Processing file: class_7_680.csv\n",
            "Processing file: class_4_681.csv\n",
            "Processing file: class_11_682.csv\n",
            "Processing file: class_8_683.csv\n",
            "Processing file: class_9_684.csv\n",
            "Processing file: class_2_685.csv\n",
            "Processing file: class_6_686.csv\n",
            "Processing file: class_0_687.csv\n",
            "Processing file: class_11_688.csv\n",
            "Processing file: class_0_689.csv\n",
            "Processing file: class_1_690.csv\n",
            "Processing file: class_8_691.csv\n",
            "Processing file: class_5_692.csv\n",
            "Processing file: class_6_693.csv\n",
            "Processing file: class_7_694.csv\n",
            "Processing file: class_1_695.csv\n",
            "Processing file: class_1_696.csv\n",
            "Processing file: class_0_697.csv\n",
            "Processing file: class_0_698.csv\n",
            "Processing file: class_10_699.csv\n",
            "Processing file: class_10_700.csv\n",
            "Processing file: class_9_701.csv\n",
            "Processing file: class_6_702.csv\n",
            "Processing file: class_11_703.csv\n",
            "Processing file: class_11_704.csv\n",
            "Processing file: class_5_705.csv\n",
            "Processing file: class_4_706.csv\n",
            "Processing file: class_7_707.csv\n",
            "Processing file: class_2_708.csv\n",
            "Processing file: class_7_709.csv\n",
            "Processing file: class_3_710.csv\n",
            "Processing file: class_1_711.csv\n",
            "Processing file: class_1_712.csv\n",
            "Processing file: class_1_713.csv\n",
            "Processing file: class_10_714.csv\n",
            "Processing file: class_2_715.csv\n",
            "Processing file: class_1_716.csv\n",
            "Processing file: class_2_717.csv\n",
            "Processing file: class_5_718.csv\n",
            "Processing file: class_10_719.csv\n",
            "Processing file: class_10_720.csv\n",
            "Processing file: class_10_721.csv\n",
            "Processing file: class_3_722.csv\n",
            "Processing file: class_3_723.csv\n",
            "Processing file: class_1_724.csv\n",
            "Processing file: class_3_725.csv\n",
            "Processing file: class_5_726.csv\n",
            "Processing file: class_1_727.csv\n",
            "Processing file: class_4_728.csv\n",
            "Processing file: class_5_729.csv\n",
            "Processing file: class_10_730.csv\n",
            "Processing file: class_3_731.csv\n",
            "Processing file: class_8_732.csv\n",
            "Processing file: class_4_733.csv\n",
            "Processing file: class_9_734.csv\n",
            "Processing file: class_0_735.csv\n",
            "Processing file: class_7_736.csv\n",
            "Processing file: class_6_737.csv\n",
            "Processing file: class_3_738.csv\n",
            "Processing file: class_11_739.csv\n",
            "Processing file: class_1_740.csv\n",
            "Processing file: class_4_741.csv\n",
            "Processing file: class_0_742.csv\n",
            "Processing file: class_6_743.csv\n",
            "Processing file: class_2_744.csv\n",
            "Processing file: class_4_745.csv\n",
            "Processing file: class_11_746.csv\n",
            "Processing file: class_11_747.csv\n",
            "Processing file: class_1_748.csv\n",
            "Processing file: class_3_749.csv\n",
            "Processing file: class_0_750.csv\n",
            "Processing file: class_0_751.csv\n",
            "Processing file: class_9_752.csv\n",
            "Processing file: class_11_753.csv\n",
            "Processing file: class_10_754.csv\n",
            "Processing file: class_9_755.csv\n",
            "Processing file: class_7_756.csv\n",
            "Processing file: class_5_757.csv\n",
            "Processing file: class_4_758.csv\n",
            "Processing file: class_0_759.csv\n",
            "Processing file: class_10_760.csv\n",
            "Processing file: class_11_761.csv\n",
            "Processing file: class_8_762.csv\n",
            "Processing file: class_1_763.csv\n",
            "Processing file: class_9_764.csv\n",
            "Processing file: class_11_765.csv\n",
            "Processing file: class_6_766.csv\n",
            "Processing file: class_5_767.csv\n",
            "Processing file: class_1_768.csv\n",
            "Processing file: class_2_769.csv\n",
            "Processing file: class_9_770.csv\n",
            "Processing file: class_7_771.csv\n",
            "Processing file: class_1_772.csv\n",
            "Processing file: class_11_773.csv\n",
            "Processing file: class_2_774.csv\n",
            "Processing file: class_3_775.csv\n",
            "Processing file: class_3_776.csv\n",
            "Processing file: class_1_777.csv\n",
            "Processing file: class_7_778.csv\n",
            "Processing file: class_7_779.csv\n",
            "Processing file: class_7_780.csv\n",
            "Processing file: class_10_781.csv\n",
            "Processing file: class_5_782.csv\n",
            "Processing file: class_0_783.csv\n",
            "Processing file: class_9_784.csv\n",
            "Processing file: class_6_785.csv\n",
            "Processing file: class_0_786.csv\n",
            "Processing file: class_3_787.csv\n",
            "Processing file: class_9_788.csv\n",
            "Processing file: class_9_789.csv\n",
            "Processing file: class_5_790.csv\n",
            "Processing file: class_11_791.csv\n",
            "Processing file: class_5_792.csv\n",
            "Processing file: class_3_793.csv\n",
            "Processing file: class_3_794.csv\n",
            "Processing file: class_3_795.csv\n",
            "Processing file: class_6_796.csv\n",
            "Processing file: class_7_797.csv\n",
            "Processing file: class_8_798.csv\n",
            "Processing file: class_11_799.csv\n",
            "Processing file: class_11_800.csv\n",
            "Processing file: class_8_801.csv\n",
            "Processing file: class_10_802.csv\n",
            "Processing file: class_8_803.csv\n",
            "Processing file: class_6_804.csv\n",
            "Processing file: class_8_805.csv\n",
            "Processing file: class_3_806.csv\n",
            "Processing file: class_3_807.csv\n",
            "Processing file: class_6_808.csv\n",
            "Processing file: class_4_809.csv\n",
            "Processing file: class_8_810.csv\n",
            "Processing file: class_9_811.csv\n",
            "Processing file: class_7_812.csv\n",
            "Processing file: class_0_813.csv\n",
            "Processing file: class_6_814.csv\n",
            "Processing file: class_4_815.csv\n",
            "Processing file: class_3_816.csv\n",
            "Processing file: class_5_817.csv\n",
            "Processing file: class_6_818.csv\n",
            "Processing file: class_0_819.csv\n",
            "Processing file: class_1_820.csv\n",
            "Processing file: class_9_821.csv\n",
            "Processing file: class_10_822.csv\n",
            "Processing file: class_5_823.csv\n",
            "Processing file: class_9_824.csv\n",
            "Processing file: class_8_825.csv\n",
            "Processing file: class_6_826.csv\n",
            "Processing file: class_11_827.csv\n",
            "Processing file: class_10_828.csv\n",
            "Processing file: class_8_829.csv\n",
            "Processing file: class_4_830.csv\n",
            "Processing file: class_6_831.csv\n",
            "Processing file: class_4_832.csv\n",
            "Processing file: class_9_833.csv\n",
            "Processing file: class_7_834.csv\n",
            "Processing file: class_7_835.csv\n",
            "Processing file: class_0_836.csv\n",
            "Processing file: class_3_837.csv\n",
            "Processing file: class_9_838.csv\n",
            "Processing file: class_2_839.csv\n",
            "Processing file: class_3_840.csv\n",
            "Processing file: class_8_841.csv\n",
            "Processing file: class_0_842.csv\n",
            "Processing file: class_0_843.csv\n",
            "Processing file: class_1_844.csv\n",
            "Processing file: class_10_845.csv\n",
            "Processing file: class_8_846.csv\n",
            "Processing file: class_8_847.csv\n",
            "Processing file: class_10_848.csv\n",
            "Processing file: class_6_849.csv\n",
            "Processing file: class_2_850.csv\n",
            "Processing file: class_11_851.csv\n",
            "Processing file: class_4_852.csv\n",
            "Processing file: class_2_853.csv\n",
            "Processing file: class_8_854.csv\n",
            "Processing file: class_7_855.csv\n",
            "Processing file: class_1_856.csv\n",
            "Processing file: class_7_857.csv\n",
            "Processing file: class_2_858.csv\n",
            "Processing file: class_2_859.csv\n",
            "Processing file: class_5_860.csv\n",
            "Processing file: class_1_861.csv\n",
            "Processing file: class_6_862.csv\n",
            "Processing file: class_1_863.csv\n",
            "Processing file: class_4_864.csv\n",
            "Processing file: class_5_865.csv\n",
            "Processing file: class_10_866.csv\n",
            "Processing file: class_5_867.csv\n",
            "Processing file: class_0_868.csv\n",
            "Processing file: class_3_869.csv\n",
            "Processing file: class_11_870.csv\n",
            "Processing file: class_2_871.csv\n",
            "Processing file: class_2_872.csv\n",
            "Processing file: class_6_873.csv\n",
            "Processing file: class_7_874.csv\n",
            "Processing file: class_7_875.csv\n",
            "Processing file: class_7_876.csv\n",
            "Processing file: class_4_877.csv\n",
            "Processing file: class_3_878.csv\n",
            "Processing file: class_11_879.csv\n",
            "Processing file: class_3_880.csv\n",
            "Processing file: class_10_881.csv\n",
            "Processing file: class_5_882.csv\n",
            "Processing file: class_3_883.csv\n",
            "Processing file: class_5_884.csv\n",
            "Processing file: class_8_885.csv\n",
            "Processing file: class_1_886.csv\n",
            "Processing file: class_11_887.csv\n",
            "Processing file: class_3_888.csv\n",
            "Processing file: class_2_889.csv\n",
            "Processing file: class_6_890.csv\n",
            "Processing file: class_10_891.csv\n",
            "Processing file: class_1_892.csv\n",
            "Processing file: class_6_893.csv\n",
            "Processing file: class_7_894.csv\n",
            "Processing file: class_5_895.csv\n",
            "Processing file: class_11_896.csv\n",
            "Processing file: class_1_897.csv\n",
            "Processing file: class_1_898.csv\n",
            "Processing file: class_9_899.csv\n",
            "Processing file: class_1_900.csv\n",
            "Processing file: class_11_901.csv\n",
            "Processing file: class_11_902.csv\n",
            "Processing file: class_1_903.csv\n",
            "Processing file: class_1_904.csv\n",
            "Processing file: class_8_905.csv\n",
            "Processing file: class_6_906.csv\n",
            "Processing file: class_6_907.csv\n",
            "Processing file: class_5_908.csv\n",
            "Processing file: class_3_909.csv\n",
            "Processing file: class_7_910.csv\n",
            "Processing file: class_0_911.csv\n",
            "Processing file: class_0_912.csv\n",
            "Processing file: class_10_913.csv\n",
            "Processing file: class_1_914.csv\n",
            "Processing file: class_0_915.csv\n",
            "Processing file: class_7_916.csv\n",
            "Processing file: class_9_917.csv\n",
            "Processing file: class_5_918.csv\n",
            "Processing file: class_11_919.csv\n",
            "Processing file: class_2_920.csv\n",
            "Processing file: class_6_921.csv\n",
            "Processing file: class_8_922.csv\n",
            "Processing file: class_2_923.csv\n",
            "Processing file: class_10_924.csv\n",
            "Processing file: class_10_925.csv\n",
            "Processing file: class_1_926.csv\n",
            "Processing file: class_10_927.csv\n",
            "Processing file: class_3_928.csv\n",
            "Processing file: class_4_929.csv\n",
            "Processing file: class_8_930.csv\n",
            "Processing file: class_6_931.csv\n",
            "Processing file: class_5_932.csv\n",
            "Processing file: class_2_933.csv\n",
            "Processing file: class_0_934.csv\n",
            "Processing file: class_7_935.csv\n",
            "Processing file: class_9_936.csv\n",
            "Processing file: class_9_937.csv\n",
            "Processing file: class_5_938.csv\n",
            "Processing file: class_5_939.csv\n",
            "Processing file: class_6_940.csv\n",
            "Processing file: class_3_941.csv\n",
            "Processing file: class_8_942.csv\n",
            "Processing file: class_7_943.csv\n",
            "Processing file: class_7_944.csv\n",
            "Processing file: class_4_945.csv\n",
            "Processing file: class_1_946.csv\n",
            "Processing file: class_10_947.csv\n",
            "Processing file: class_10_948.csv\n",
            "Processing file: class_6_949.csv\n",
            "Processing file: class_0_950.csv\n",
            "Processing file: class_10_951.csv\n",
            "Processing file: class_1_952.csv\n",
            "Processing file: class_10_953.csv\n",
            "Processing file: class_10_954.csv\n",
            "Processing file: class_0_955.csv\n",
            "Processing file: class_7_956.csv\n",
            "Processing file: class_5_957.csv\n",
            "Processing file: class_3_958.csv\n",
            "Processing file: class_9_959.csv\n",
            "Processing file: class_0_960.csv\n",
            "Processing file: class_5_961.csv\n",
            "Processing file: class_8_962.csv\n",
            "Processing file: class_11_963.csv\n",
            "Processing file: class_9_964.csv\n",
            "Processing file: class_7_965.csv\n",
            "Processing file: class_9_966.csv\n",
            "Processing file: class_3_967.csv\n",
            "Processing file: class_8_968.csv\n",
            "Processing file: class_4_969.csv\n",
            "Processing file: class_10_970.csv\n",
            "Processing file: class_7_971.csv\n",
            "Processing file: class_4_972.csv\n",
            "Processing file: class_6_973.csv\n",
            "Processing file: class_3_974.csv\n",
            "Processing file: class_7_975.csv\n",
            "Processing file: class_4_976.csv\n",
            "Processing file: class_1_977.csv\n",
            "Processing file: class_8_978.csv\n",
            "Processing file: class_2_979.csv\n",
            "Processing file: class_6_980.csv\n",
            "Processing file: class_3_981.csv\n",
            "Processing file: class_6_982.csv\n",
            "Processing file: class_10_983.csv\n",
            "Processing file: class_2_984.csv\n",
            "Processing file: class_11_985.csv\n",
            "Processing file: class_1_986.csv\n",
            "Processing file: class_7_987.csv\n",
            "Processing file: class_6_988.csv\n",
            "Processing file: class_7_989.csv\n",
            "Processing file: class_10_990.csv\n",
            "Processing file: class_7_991.csv\n",
            "Processing file: class_1_992.csv\n",
            "Processing file: class_3_993.csv\n",
            "Processing file: class_3_994.csv\n",
            "Processing file: class_8_995.csv\n",
            "Processing file: class_6_996.csv\n",
            "Processing file: class_6_997.csv\n",
            "Processing file: class_5_998.csv\n",
            "Processing file: class_5_999.csv\n",
            "Processing file: class_9_1000.csv\n",
            "Processing file: class_3_1001.csv\n",
            "Processing file: class_8_1002.csv\n",
            "Processing file: class_3_1003.csv\n",
            "Processing file: class_4_1004.csv\n",
            "Processing file: class_11_1005.csv\n",
            "Processing file: class_4_1006.csv\n",
            "Processing file: class_2_1007.csv\n",
            "Processing file: class_1_1008.csv\n",
            "Processing file: class_8_1009.csv\n",
            "Processing file: class_9_1010.csv\n",
            "Processing file: class_9_1011.csv\n",
            "Processing file: class_1_1012.csv\n",
            "Processing file: class_9_1013.csv\n",
            "Processing file: class_10_1014.csv\n",
            "Processing file: class_6_1015.csv\n",
            "Processing file: class_7_1016.csv\n",
            "Processing file: class_0_1017.csv\n",
            "Processing file: class_5_1018.csv\n",
            "Processing file: class_10_1019.csv\n",
            "Processing file: class_8_1020.csv\n",
            "Processing file: class_0_1021.csv\n",
            "Processing file: class_2_1022.csv\n",
            "Processing file: class_2_1023.csv\n",
            "Processing file: class_5_1024.csv\n",
            "Processing file: class_4_1025.csv\n",
            "Processing file: class_10_1026.csv\n",
            "Processing file: class_3_1027.csv\n",
            "Processing file: class_7_1028.csv\n",
            "Processing file: class_9_1029.csv\n",
            "Processing file: class_0_1030.csv\n",
            "Processing file: class_7_1031.csv\n",
            "Processing file: class_3_1032.csv\n",
            "Processing file: class_11_1033.csv\n",
            "Processing file: class_7_1034.csv\n",
            "Processing file: class_0_1035.csv\n",
            "Processing file: class_1_1036.csv\n",
            "Processing file: class_11_1037.csv\n",
            "Processing file: class_0_1038.csv\n",
            "Processing file: class_5_1039.csv\n",
            "Processing file: class_6_1040.csv\n",
            "Processing file: class_4_1041.csv\n",
            "Processing file: class_1_1042.csv\n",
            "Processing file: class_4_1043.csv\n",
            "Processing file: class_0_1044.csv\n",
            "Processing file: class_10_1045.csv\n",
            "Processing file: class_7_1046.csv\n",
            "Processing file: class_1_1047.csv\n",
            "Processing file: class_8_1048.csv\n",
            "Processing file: class_8_1049.csv\n",
            "Processing file: class_9_1050.csv\n",
            "Processing file: class_0_1051.csv\n",
            "Processing file: class_10_1052.csv\n",
            "Processing file: class_10_1053.csv\n",
            "Processing file: class_5_1054.csv\n",
            "Processing file: class_0_1055.csv\n",
            "Processing file: class_7_1056.csv\n",
            "Processing file: class_8_1057.csv\n",
            "Processing file: class_9_1058.csv\n",
            "Processing file: class_10_1059.csv\n",
            "Processing file: class_9_1060.csv\n",
            "Processing file: class_5_1061.csv\n",
            "Processing file: class_0_1062.csv\n",
            "Processing file: class_1_1063.csv\n",
            "Processing file: class_1_1064.csv\n",
            "Processing file: class_5_1065.csv\n",
            "Processing file: class_6_1066.csv\n",
            "Processing file: class_0_1067.csv\n",
            "Processing file: class_4_1068.csv\n",
            "Processing file: class_7_1069.csv\n",
            "Processing file: class_8_1070.csv\n",
            "Processing file: class_10_1071.csv\n",
            "Processing file: class_1_1072.csv\n",
            "Processing file: class_8_1073.csv\n",
            "Processing file: class_3_1074.csv\n",
            "Processing file: class_3_1075.csv\n",
            "Processing file: class_7_1076.csv\n",
            "Processing file: class_7_1077.csv\n",
            "Processing file: class_10_1078.csv\n",
            "Processing file: class_3_1079.csv\n",
            "Processing file: class_8_1080.csv\n",
            "Processing file: class_9_1081.csv\n",
            "Processing file: class_9_1082.csv\n",
            "Processing file: class_7_1083.csv\n",
            "Processing file: class_4_1084.csv\n",
            "Processing file: class_4_1085.csv\n",
            "Processing file: class_10_1086.csv\n",
            "Processing file: class_9_1087.csv\n",
            "Processing file: class_2_1088.csv\n",
            "Processing file: class_6_1089.csv\n",
            "Processing file: class_5_1090.csv\n",
            "Processing file: class_6_1091.csv\n",
            "Processing file: class_1_1092.csv\n",
            "Processing file: class_11_1093.csv\n",
            "Processing file: class_4_1094.csv\n",
            "Processing file: class_11_1095.csv\n",
            "Processing file: class_3_1096.csv\n",
            "Processing file: class_1_1097.csv\n",
            "Processing file: class_7_1098.csv\n",
            "Processing file: class_7_1099.csv\n",
            "Processing file: class_2_1100.csv\n",
            "Processing file: class_4_1101.csv\n",
            "Processing file: class_0_1102.csv\n",
            "Processing file: class_4_1103.csv\n",
            "Processing file: class_3_1104.csv\n",
            "Processing file: class_9_1105.csv\n",
            "Processing file: class_3_1106.csv\n",
            "Processing file: class_1_1107.csv\n",
            "Processing file: class_11_1108.csv\n",
            "Processing file: class_11_1109.csv\n",
            "Processing file: class_4_1110.csv\n",
            "Processing file: class_4_1111.csv\n",
            "Processing file: class_10_1112.csv\n",
            "Processing file: class_5_1113.csv\n",
            "Processing file: class_3_1114.csv\n",
            "Processing file: class_2_1115.csv\n",
            "Processing file: class_3_1116.csv\n",
            "Processing file: class_9_1117.csv\n",
            "Processing file: class_2_1118.csv\n",
            "Processing file: class_0_1119.csv\n",
            "Processing file: class_6_1120.csv\n",
            "Processing file: class_6_1121.csv\n",
            "Processing file: class_3_1122.csv\n",
            "Processing file: class_4_1123.csv\n",
            "Processing file: class_9_1124.csv\n",
            "Processing file: class_4_1125.csv\n",
            "Processing file: class_8_1126.csv\n",
            "Processing file: class_11_1127.csv\n",
            "Processing file: class_8_1128.csv\n",
            "Processing file: class_2_1129.csv\n",
            "Processing file: class_6_1130.csv\n",
            "Processing file: class_4_1131.csv\n",
            "Processing file: class_0_1132.csv\n",
            "Processing file: class_5_1133.csv\n",
            "Processing file: class_0_1134.csv\n",
            "Processing file: class_1_1135.csv\n",
            "Processing file: class_8_1136.csv\n",
            "Processing file: class_3_1137.csv\n",
            "Processing file: class_3_1138.csv\n",
            "Processing file: class_9_1139.csv\n",
            "Processing file: class_11_1140.csv\n",
            "Processing file: class_8_1141.csv\n",
            "Processing file: class_7_1142.csv\n",
            "Processing file: class_4_1143.csv\n",
            "Processing file: class_9_1144.csv\n",
            "Processing file: class_0_1145.csv\n",
            "Processing file: class_8_1146.csv\n",
            "Processing file: class_6_1147.csv\n",
            "Processing file: class_11_1148.csv\n",
            "Processing file: class_10_1149.csv\n",
            "Processing file: class_5_1150.csv\n",
            "Processing file: class_1_1151.csv\n",
            "Processing file: class_5_1152.csv\n",
            "Processing file: class_11_1153.csv\n",
            "Processing file: class_7_1154.csv\n",
            "Processing file: class_8_1155.csv\n",
            "Processing file: class_3_1156.csv\n",
            "Processing file: class_11_1157.csv\n",
            "Processing file: class_7_1158.csv\n",
            "Processing file: class_5_1159.csv\n",
            "Processing file: class_1_1160.csv\n",
            "Processing file: class_0_1161.csv\n",
            "Processing file: class_1_1162.csv\n",
            "Processing file: class_5_1163.csv\n",
            "Processing file: class_3_1164.csv\n",
            "Processing file: class_7_1165.csv\n",
            "Processing file: class_5_1166.csv\n",
            "Processing file: class_3_1167.csv\n",
            "Processing file: class_1_1168.csv\n",
            "Processing file: class_6_1169.csv\n",
            "Processing file: class_1_1170.csv\n",
            "Processing file: class_8_1171.csv\n",
            "Processing file: class_4_1172.csv\n",
            "Processing file: class_1_1173.csv\n",
            "Processing file: class_10_1174.csv\n",
            "Processing file: class_0_1175.csv\n",
            "Processing file: class_0_1176.csv\n",
            "Processing file: class_6_1177.csv\n",
            "Processing file: class_9_1178.csv\n",
            "Processing file: class_11_1179.csv\n",
            "Processing file: class_3_1180.csv\n",
            "Processing file: class_11_1181.csv\n",
            "Processing file: class_0_1182.csv\n",
            "Processing file: class_6_1183.csv\n",
            "Processing file: class_4_1184.csv\n",
            "Processing file: class_0_1185.csv\n",
            "Processing file: class_0_1186.csv\n",
            "Processing file: class_6_1187.csv\n",
            "Processing file: class_5_1188.csv\n",
            "Processing file: class_2_1189.csv\n",
            "Processing file: class_9_1190.csv\n",
            "Processing file: class_8_1191.csv\n",
            "Processing file: class_0_1192.csv\n",
            "Processing file: class_7_1193.csv\n",
            "Processing file: class_7_1194.csv\n",
            "Processing file: class_11_1195.csv\n",
            "Processing file: class_2_1196.csv\n",
            "Processing file: class_11_1197.csv\n",
            "Processing file: class_8_1198.csv\n",
            "Processing file: class_7_1199.csv\n",
            "Processing file: class_8_1200.csv\n",
            "Processing file: class_10_1201.csv\n",
            "Processing file: class_0_1202.csv\n",
            "Processing file: class_7_1203.csv\n",
            "Processing file: class_9_1204.csv\n",
            "Processing file: class_2_1205.csv\n",
            "Processing file: class_8_1206.csv\n",
            "Processing file: class_10_1207.csv\n",
            "Processing file: class_6_1208.csv\n",
            "Processing file: class_11_1209.csv\n",
            "Processing file: class_7_1210.csv\n",
            "Processing file: class_0_1211.csv\n",
            "Processing file: class_7_1212.csv\n",
            "Processing file: class_8_1213.csv\n",
            "Processing file: class_2_1214.csv\n",
            "Processing file: class_3_1215.csv\n",
            "Processing file: class_10_1216.csv\n",
            "Processing file: class_11_1217.csv\n",
            "Processing file: class_10_1218.csv\n",
            "Processing file: class_10_1219.csv\n",
            "Processing file: class_3_1220.csv\n",
            "Processing file: class_5_1221.csv\n",
            "Processing file: class_11_1222.csv\n",
            "Processing file: class_11_1223.csv\n",
            "Processing file: class_11_1224.csv\n",
            "Processing file: class_7_1225.csv\n",
            "Processing file: class_9_1226.csv\n",
            "Processing file: class_7_1227.csv\n",
            "Processing file: class_1_1228.csv\n",
            "Processing file: class_7_1229.csv\n",
            "Processing file: class_1_1230.csv\n",
            "Processing file: class_11_1231.csv\n",
            "Processing file: class_7_1232.csv\n",
            "Processing file: class_3_1233.csv\n",
            "Processing file: class_10_1234.csv\n",
            "Processing file: class_9_1235.csv\n",
            "Processing file: class_5_1236.csv\n",
            "Processing file: class_1_1237.csv\n",
            "Processing file: class_3_1238.csv\n",
            "Processing file: class_10_1239.csv\n",
            "Processing file: class_1_1240.csv\n",
            "Processing file: class_2_1241.csv\n",
            "Processing file: class_3_1242.csv\n",
            "Processing file: class_3_1243.csv\n",
            "Processing file: class_10_1244.csv\n",
            "Processing file: class_11_1245.csv\n",
            "Processing file: class_8_1246.csv\n",
            "Processing file: class_3_1247.csv\n",
            "Processing file: class_7_1248.csv\n",
            "Processing file: class_9_1249.csv\n",
            "Processing file: class_5_1250.csv\n",
            "Processing file: class_5_1251.csv\n",
            "Processing file: class_10_1252.csv\n",
            "Processing file: class_8_1253.csv\n",
            "Processing file: class_11_1254.csv\n",
            "Processing file: class_4_1255.csv\n",
            "Processing file: class_10_1256.csv\n",
            "Processing file: class_4_1257.csv\n",
            "Processing file: class_5_1258.csv\n",
            "Processing file: class_6_1259.csv\n",
            "Processing file: class_3_1260.csv\n",
            "Processing file: class_2_1261.csv\n",
            "Processing file: class_4_1262.csv\n",
            "Processing file: class_4_1263.csv\n",
            "Processing file: class_10_1264.csv\n",
            "Processing file: class_7_1265.csv\n",
            "Processing file: class_9_1266.csv\n",
            "Processing file: class_7_1267.csv\n",
            "Processing file: class_2_1268.csv\n",
            "Processing file: class_5_1269.csv\n",
            "Processing file: class_11_1270.csv\n",
            "Processing file: class_4_1271.csv\n",
            "Processing file: class_8_1272.csv\n",
            "Processing file: class_2_1273.csv\n",
            "Processing file: class_9_1274.csv\n",
            "Processing file: class_2_1275.csv\n",
            "Processing file: class_10_1276.csv\n",
            "Processing file: class_4_1277.csv\n",
            "Processing file: class_7_1278.csv\n",
            "Processing file: class_3_1279.csv\n",
            "Processing file: class_1_1280.csv\n",
            "Processing file: class_6_1281.csv\n",
            "Processing file: class_6_1282.csv\n",
            "Processing file: class_1_1283.csv\n",
            "Processing file: class_0_1284.csv\n",
            "Processing file: class_8_1285.csv\n",
            "Processing file: class_3_1286.csv\n",
            "Processing file: class_8_1287.csv\n",
            "Processing file: class_7_1288.csv\n",
            "Processing file: class_1_1289.csv\n",
            "Processing file: class_7_1290.csv\n",
            "Processing file: class_11_1291.csv\n",
            "Processing file: class_4_1292.csv\n",
            "Processing file: class_5_1293.csv\n",
            "Processing file: class_9_1294.csv\n",
            "Processing file: class_8_1295.csv\n",
            "Processing file: class_10_1296.csv\n",
            "Processing file: class_2_1297.csv\n",
            "Processing file: class_11_1298.csv\n",
            "Processing file: class_10_1299.csv\n",
            "Processing file: class_7_1300.csv\n",
            "Processing file: class_8_1301.csv\n",
            "Processing file: class_10_1302.csv\n",
            "Processing file: class_1_1303.csv\n",
            "Processing file: class_3_1304.csv\n",
            "Processing file: class_3_1305.csv\n",
            "Processing file: class_11_1306.csv\n",
            "Processing file: class_6_1307.csv\n",
            "Processing file: class_9_1308.csv\n",
            "Processing file: class_7_1309.csv\n",
            "Processing file: class_6_1310.csv\n",
            "Processing file: class_11_1311.csv\n",
            "Processing file: class_1_1312.csv\n",
            "Processing file: class_1_1313.csv\n",
            "Processing file: class_2_1314.csv\n",
            "Processing file: class_11_1315.csv\n",
            "Processing file: class_11_1316.csv\n",
            "Processing file: class_7_1317.csv\n",
            "Processing file: class_9_1318.csv\n",
            "Processing file: class_7_1319.csv\n",
            "Processing file: class_1_1320.csv\n",
            "Processing file: class_2_1321.csv\n",
            "Processing file: class_2_1322.csv\n",
            "Processing file: class_0_1323.csv\n",
            "Processing file: class_11_1324.csv\n",
            "Processing file: class_9_1325.csv\n",
            "Processing file: class_3_1326.csv\n",
            "Processing file: class_9_1327.csv\n",
            "Processing file: class_2_1328.csv\n",
            "Processing file: class_10_1329.csv\n",
            "Processing file: class_8_1330.csv\n",
            "Processing file: class_2_1331.csv\n",
            "Processing file: class_2_1332.csv\n",
            "Processing file: class_11_1333.csv\n",
            "Processing file: class_8_1334.csv\n",
            "Processing file: class_4_1335.csv\n",
            "Processing file: class_1_1336.csv\n",
            "Processing file: class_1_1337.csv\n",
            "Processing file: class_4_1338.csv\n",
            "Processing file: class_11_1339.csv\n",
            "Processing file: class_10_1340.csv\n",
            "Processing file: class_11_1341.csv\n",
            "Processing file: class_5_1342.csv\n",
            "Processing file: class_2_1343.csv\n",
            "Processing file: class_8_1344.csv\n",
            "Processing file: class_3_1345.csv\n",
            "Processing file: class_2_1346.csv\n",
            "Processing file: class_5_1347.csv\n",
            "Processing file: class_8_1348.csv\n",
            "Processing file: class_9_1349.csv\n",
            "Processing file: class_1_1350.csv\n",
            "Processing file: class_4_1351.csv\n",
            "Processing file: class_4_1352.csv\n",
            "Processing file: class_11_1353.csv\n",
            "Processing file: class_0_1354.csv\n",
            "Processing file: class_8_1355.csv\n",
            "Processing file: class_1_1356.csv\n",
            "Processing file: class_1_1357.csv\n",
            "Processing file: class_8_1358.csv\n",
            "Processing file: class_1_1359.csv\n",
            "Processing file: class_9_1360.csv\n",
            "Processing file: class_11_1361.csv\n",
            "Processing file: class_5_1362.csv\n",
            "Processing file: class_11_1363.csv\n",
            "Processing file: class_10_1364.csv\n",
            "Processing file: class_10_1365.csv\n",
            "Processing file: class_7_1366.csv\n",
            "Processing file: class_0_1367.csv\n",
            "Processing file: class_2_1368.csv\n",
            "Processing file: class_4_1369.csv\n",
            "Processing file: class_11_1370.csv\n",
            "Processing file: class_5_1371.csv\n",
            "Processing file: class_1_1372.csv\n",
            "Processing file: class_3_1373.csv\n",
            "Processing file: class_4_1374.csv\n",
            "Processing file: class_10_1375.csv\n",
            "Processing file: class_10_1376.csv\n",
            "Processing file: class_0_1377.csv\n",
            "Processing file: class_6_1378.csv\n",
            "Processing file: class_3_1379.csv\n",
            "Processing file: class_1_1380.csv\n",
            "Processing file: class_11_1381.csv\n",
            "Processing file: class_0_1382.csv\n",
            "Processing file: class_3_1383.csv\n",
            "Processing file: class_8_1384.csv\n",
            "Processing file: class_1_1385.csv\n",
            "Processing file: class_3_1386.csv\n",
            "Processing file: class_3_1387.csv\n",
            "Processing file: class_1_1388.csv\n",
            "Processing file: class_10_1389.csv\n",
            "Processing file: class_11_1390.csv\n",
            "Processing file: class_6_1391.csv\n",
            "Processing file: class_11_1392.csv\n",
            "Processing file: class_8_1393.csv\n",
            "Processing file: class_7_1394.csv\n",
            "Processing file: class_2_1395.csv\n",
            "Processing file: class_9_1396.csv\n",
            "Processing file: class_8_1397.csv\n",
            "Processing file: class_10_1398.csv\n",
            "Processing file: class_5_1399.csv\n",
            "Processing file: class_11_0.csv\n",
            "Processing file: class_7_1.csv\n",
            "Processing file: class_3_2.csv\n",
            "Processing file: class_6_3.csv\n",
            "Processing file: class_4_4.csv\n",
            "Processing file: class_7_5.csv\n",
            "Processing file: class_7_6.csv\n",
            "Processing file: class_10_7.csv\n",
            "Processing file: class_2_8.csv\n",
            "Processing file: class_0_9.csv\n",
            "Processing file: class_10_10.csv\n",
            "Processing file: class_9_11.csv\n",
            "Processing file: class_2_12.csv\n",
            "Processing file: class_6_13.csv\n",
            "Processing file: class_6_14.csv\n",
            "Processing file: class_3_15.csv\n",
            "Processing file: class_8_16.csv\n",
            "Processing file: class_3_17.csv\n",
            "Processing file: class_9_18.csv\n",
            "Processing file: class_8_19.csv\n",
            "Processing file: class_7_20.csv\n",
            "Processing file: class_0_21.csv\n",
            "Processing file: class_4_22.csv\n",
            "Processing file: class_8_23.csv\n",
            "Processing file: class_2_24.csv\n",
            "Processing file: class_11_25.csv\n",
            "Processing file: class_6_26.csv\n",
            "Processing file: class_2_27.csv\n",
            "Processing file: class_0_28.csv\n",
            "Processing file: class_11_29.csv\n",
            "Processing file: class_5_30.csv\n",
            "Processing file: class_9_31.csv\n",
            "Processing file: class_1_32.csv\n",
            "Processing file: class_2_33.csv\n",
            "Processing file: class_7_34.csv\n",
            "Processing file: class_11_35.csv\n",
            "Processing file: class_0_36.csv\n",
            "Processing file: class_7_37.csv\n",
            "Processing file: class_0_38.csv\n",
            "Processing file: class_6_39.csv\n",
            "Processing file: class_11_40.csv\n",
            "Processing file: class_0_41.csv\n",
            "Processing file: class_11_42.csv\n",
            "Processing file: class_2_43.csv\n",
            "Processing file: class_11_44.csv\n",
            "Processing file: class_6_45.csv\n",
            "Processing file: class_5_46.csv\n",
            "Processing file: class_6_47.csv\n",
            "Processing file: class_10_48.csv\n",
            "Processing file: class_0_49.csv\n",
            "Processing file: class_2_50.csv\n",
            "Processing file: class_2_51.csv\n",
            "Processing file: class_1_52.csv\n",
            "Processing file: class_10_53.csv\n",
            "Processing file: class_10_54.csv\n",
            "Processing file: class_7_55.csv\n",
            "Processing file: class_9_56.csv\n",
            "Processing file: class_5_57.csv\n",
            "Processing file: class_1_58.csv\n",
            "Processing file: class_6_59.csv\n",
            "Processing file: class_3_60.csv\n",
            "Processing file: class_0_61.csv\n",
            "Processing file: class_5_62.csv\n",
            "Processing file: class_3_63.csv\n",
            "Processing file: class_8_64.csv\n",
            "Processing file: class_5_65.csv\n",
            "Processing file: class_0_66.csv\n",
            "Processing file: class_2_67.csv\n",
            "Processing file: class_9_68.csv\n",
            "Processing file: class_2_69.csv\n",
            "Processing file: class_5_70.csv\n",
            "Processing file: class_3_71.csv\n",
            "Processing file: class_2_72.csv\n",
            "Processing file: class_0_73.csv\n",
            "Processing file: class_7_74.csv\n",
            "Processing file: class_3_75.csv\n",
            "Processing file: class_10_76.csv\n",
            "Processing file: class_6_77.csv\n",
            "Processing file: class_10_78.csv\n",
            "Processing file: class_5_79.csv\n",
            "Processing file: class_2_80.csv\n",
            "Processing file: class_8_81.csv\n",
            "Processing file: class_5_82.csv\n",
            "Processing file: class_6_83.csv\n",
            "Processing file: class_11_84.csv\n",
            "Processing file: class_3_85.csv\n",
            "Processing file: class_10_86.csv\n",
            "Processing file: class_11_87.csv\n",
            "Processing file: class_5_88.csv\n",
            "Processing file: class_7_89.csv\n",
            "Processing file: class_5_90.csv\n",
            "Processing file: class_5_91.csv\n",
            "Processing file: class_11_92.csv\n",
            "Processing file: class_5_93.csv\n",
            "Processing file: class_3_94.csv\n",
            "Processing file: class_2_95.csv\n",
            "Processing file: class_2_96.csv\n",
            "Processing file: class_11_97.csv\n",
            "Processing file: class_10_98.csv\n",
            "Processing file: class_3_99.csv\n",
            "Processing file: class_11_100.csv\n",
            "Processing file: class_10_101.csv\n",
            "Processing file: class_0_102.csv\n",
            "Processing file: class_11_103.csv\n",
            "Processing file: class_10_104.csv\n",
            "Processing file: class_10_105.csv\n",
            "Processing file: class_11_106.csv\n",
            "Processing file: class_9_107.csv\n",
            "Processing file: class_0_108.csv\n",
            "Processing file: class_1_109.csv\n",
            "Processing file: class_4_110.csv\n",
            "Processing file: class_10_111.csv\n",
            "Processing file: class_0_112.csv\n",
            "Processing file: class_10_113.csv\n",
            "Processing file: class_8_114.csv\n",
            "Processing file: class_6_115.csv\n",
            "Processing file: class_5_116.csv\n",
            "Processing file: class_6_117.csv\n",
            "Processing file: class_7_118.csv\n",
            "Processing file: class_2_119.csv\n",
            "Processing file: class_10_120.csv\n",
            "Processing file: class_5_121.csv\n",
            "Processing file: class_2_122.csv\n",
            "Processing file: class_9_123.csv\n",
            "Processing file: class_8_124.csv\n",
            "Processing file: class_0_125.csv\n",
            "Processing file: class_8_126.csv\n",
            "Processing file: class_2_127.csv\n",
            "Processing file: class_2_128.csv\n",
            "Processing file: class_0_129.csv\n",
            "Processing file: class_1_130.csv\n",
            "Processing file: class_3_131.csv\n",
            "Processing file: class_0_132.csv\n",
            "Processing file: class_8_133.csv\n",
            "Processing file: class_2_134.csv\n",
            "Processing file: class_1_135.csv\n",
            "Processing file: class_4_136.csv\n",
            "Processing file: class_3_137.csv\n",
            "Processing file: class_8_138.csv\n",
            "Processing file: class_8_139.csv\n",
            "Processing file: class_3_140.csv\n",
            "Processing file: class_1_141.csv\n",
            "Processing file: class_10_142.csv\n",
            "Processing file: class_5_143.csv\n",
            "Processing file: class_9_144.csv\n",
            "Processing file: class_1_145.csv\n",
            "Processing file: class_8_146.csv\n",
            "Processing file: class_10_147.csv\n",
            "Processing file: class_2_148.csv\n",
            "Processing file: class_10_149.csv\n",
            "Processing file: class_7_150.csv\n",
            "Processing file: class_7_151.csv\n",
            "Processing file: class_5_152.csv\n",
            "Processing file: class_10_153.csv\n",
            "Processing file: class_1_154.csv\n",
            "Processing file: class_5_155.csv\n",
            "Processing file: class_3_156.csv\n",
            "Processing file: class_9_157.csv\n",
            "Processing file: class_7_158.csv\n",
            "Processing file: class_8_159.csv\n",
            "Processing file: class_11_160.csv\n",
            "Processing file: class_1_161.csv\n",
            "Processing file: class_8_162.csv\n",
            "Processing file: class_9_163.csv\n",
            "Processing file: class_3_164.csv\n",
            "Processing file: class_8_165.csv\n",
            "Processing file: class_0_166.csv\n",
            "Processing file: class_6_167.csv\n",
            "Processing file: class_10_168.csv\n",
            "Processing file: class_6_169.csv\n",
            "Processing file: class_9_170.csv\n",
            "Processing file: class_8_171.csv\n",
            "Processing file: class_8_172.csv\n",
            "Processing file: class_5_173.csv\n",
            "Processing file: class_1_174.csv\n",
            "Processing file: class_3_175.csv\n",
            "Processing file: class_6_176.csv\n",
            "Processing file: class_0_177.csv\n",
            "Processing file: class_6_178.csv\n",
            "Processing file: class_3_179.csv\n",
            "Processing file: class_5_180.csv\n",
            "Processing file: class_0_181.csv\n",
            "Processing file: class_10_182.csv\n",
            "Processing file: class_5_183.csv\n",
            "Processing file: class_1_184.csv\n",
            "Processing file: class_10_185.csv\n",
            "Processing file: class_7_186.csv\n",
            "Processing file: class_6_187.csv\n",
            "Processing file: class_7_188.csv\n",
            "Processing file: class_5_189.csv\n",
            "Processing file: class_8_190.csv\n",
            "Processing file: class_8_191.csv\n",
            "Processing file: class_9_192.csv\n",
            "Processing file: class_0_193.csv\n",
            "Processing file: class_2_194.csv\n",
            "Processing file: class_3_195.csv\n",
            "Processing file: class_6_196.csv\n",
            "Processing file: class_6_197.csv\n",
            "Processing file: class_11_198.csv\n",
            "Processing file: class_2_199.csv\n",
            "Processing file: class_5_200.csv\n",
            "Processing file: class_3_201.csv\n",
            "Processing file: class_5_202.csv\n",
            "Processing file: class_7_203.csv\n",
            "Processing file: class_9_204.csv\n",
            "Processing file: class_5_205.csv\n",
            "Processing file: class_2_206.csv\n",
            "Processing file: class_4_207.csv\n",
            "Processing file: class_10_208.csv\n",
            "Processing file: class_2_209.csv\n",
            "Processing file: class_11_210.csv\n",
            "Processing file: class_0_211.csv\n",
            "Processing file: class_9_212.csv\n",
            "Processing file: class_4_213.csv\n",
            "Processing file: class_1_214.csv\n",
            "Processing file: class_11_215.csv\n",
            "Processing file: class_7_216.csv\n",
            "Processing file: class_8_217.csv\n",
            "Processing file: class_6_218.csv\n",
            "Processing file: class_0_219.csv\n",
            "Processing file: class_0_220.csv\n",
            "Processing file: class_9_221.csv\n",
            "Processing file: class_1_222.csv\n",
            "Processing file: class_3_223.csv\n",
            "Processing file: class_3_224.csv\n",
            "Processing file: class_9_225.csv\n",
            "Processing file: class_1_226.csv\n",
            "Processing file: class_11_227.csv\n",
            "Processing file: class_0_228.csv\n",
            "Processing file: class_1_229.csv\n",
            "Processing file: class_8_230.csv\n",
            "Processing file: class_4_231.csv\n",
            "Processing file: class_6_232.csv\n",
            "Processing file: class_5_233.csv\n",
            "Processing file: class_3_234.csv\n",
            "Processing file: class_10_235.csv\n",
            "Processing file: class_5_236.csv\n",
            "Processing file: class_11_237.csv\n",
            "Processing file: class_0_238.csv\n",
            "Processing file: class_5_239.csv\n",
            "Processing file: class_4_240.csv\n",
            "Processing file: class_11_241.csv\n",
            "Processing file: class_9_242.csv\n",
            "Processing file: class_7_243.csv\n",
            "Processing file: class_0_244.csv\n",
            "Processing file: class_10_245.csv\n",
            "Processing file: class_2_246.csv\n",
            "Processing file: class_4_247.csv\n",
            "Processing file: class_5_248.csv\n",
            "Processing file: class_6_249.csv\n",
            "Processing file: class_11_250.csv\n",
            "Processing file: class_10_251.csv\n",
            "Processing file: class_10_252.csv\n",
            "Processing file: class_9_253.csv\n",
            "Processing file: class_5_254.csv\n",
            "Processing file: class_6_255.csv\n",
            "Processing file: class_7_256.csv\n",
            "Processing file: class_9_257.csv\n",
            "Processing file: class_11_258.csv\n",
            "Processing file: class_11_259.csv\n",
            "Processing file: class_4_260.csv\n",
            "Processing file: class_11_261.csv\n",
            "Processing file: class_10_262.csv\n",
            "Processing file: class_1_263.csv\n",
            "Processing file: class_2_264.csv\n",
            "Processing file: class_9_265.csv\n",
            "Processing file: class_0_266.csv\n",
            "Processing file: class_3_267.csv\n",
            "Processing file: class_3_268.csv\n",
            "Processing file: class_6_269.csv\n",
            "Processing file: class_0_270.csv\n",
            "Processing file: class_2_271.csv\n",
            "Processing file: class_6_272.csv\n",
            "Processing file: class_11_273.csv\n",
            "Processing file: class_8_274.csv\n",
            "Processing file: class_4_275.csv\n",
            "Processing file: class_8_276.csv\n",
            "Processing file: class_1_277.csv\n",
            "Processing file: class_4_278.csv\n",
            "Processing file: class_0_279.csv\n",
            "Processing file: class_0_280.csv\n",
            "Processing file: class_10_281.csv\n",
            "Processing file: class_2_282.csv\n",
            "Processing file: class_6_283.csv\n",
            "Processing file: class_0_284.csv\n",
            "Processing file: class_6_285.csv\n",
            "Processing file: class_11_286.csv\n",
            "Processing file: class_9_287.csv\n",
            "Processing file: class_4_288.csv\n",
            "Processing file: class_9_289.csv\n",
            "Processing file: class_5_290.csv\n",
            "Processing file: class_7_291.csv\n",
            "Processing file: class_5_292.csv\n",
            "Processing file: class_6_293.csv\n",
            "Processing file: class_3_294.csv\n",
            "Processing file: class_2_295.csv\n",
            "Processing file: class_3_296.csv\n",
            "Processing file: class_2_297.csv\n",
            "Processing file: class_5_298.csv\n",
            "Processing file: class_4_299.csv\n",
            "Processing file: class_10_300.csv\n",
            "Processing file: class_2_301.csv\n",
            "Processing file: class_4_302.csv\n",
            "Processing file: class_8_303.csv\n",
            "Processing file: class_0_304.csv\n",
            "Processing file: class_5_305.csv\n",
            "Processing file: class_2_306.csv\n",
            "Processing file: class_4_307.csv\n",
            "Processing file: class_9_308.csv\n",
            "Processing file: class_11_309.csv\n",
            "Processing file: class_11_310.csv\n",
            "Processing file: class_0_311.csv\n",
            "Processing file: class_5_312.csv\n",
            "Processing file: class_10_313.csv\n",
            "Processing file: class_5_314.csv\n",
            "Processing file: class_6_315.csv\n",
            "Processing file: class_3_316.csv\n",
            "Processing file: class_1_317.csv\n",
            "Processing file: class_4_318.csv\n",
            "Processing file: class_9_319.csv\n",
            "Processing file: class_8_320.csv\n",
            "Processing file: class_9_321.csv\n",
            "Processing file: class_6_322.csv\n",
            "Processing file: class_5_323.csv\n",
            "Processing file: class_7_324.csv\n",
            "Processing file: class_5_325.csv\n",
            "Processing file: class_3_326.csv\n",
            "Processing file: class_1_327.csv\n",
            "Processing file: class_10_328.csv\n",
            "Processing file: class_10_329.csv\n",
            "Processing file: class_7_330.csv\n",
            "Processing file: class_11_331.csv\n",
            "Processing file: class_5_332.csv\n",
            "Processing file: class_7_333.csv\n",
            "Processing file: class_4_334.csv\n",
            "Processing file: class_6_335.csv\n",
            "Processing file: class_3_336.csv\n",
            "Processing file: class_4_337.csv\n",
            "Processing file: class_1_338.csv\n",
            "Processing file: class_10_339.csv\n",
            "Processing file: class_0_340.csv\n",
            "Processing file: class_4_341.csv\n",
            "Processing file: class_7_342.csv\n",
            "Processing file: class_11_343.csv\n",
            "Processing file: class_6_344.csv\n",
            "Processing file: class_2_345.csv\n",
            "Processing file: class_7_346.csv\n",
            "Processing file: class_5_347.csv\n",
            "Processing file: class_5_348.csv\n",
            "Processing file: class_0_349.csv\n",
            "Processing file: class_10_350.csv\n",
            "Processing file: class_3_351.csv\n",
            "Processing file: class_9_352.csv\n",
            "Processing file: class_0_353.csv\n",
            "Processing file: class_0_354.csv\n",
            "Processing file: class_8_355.csv\n",
            "Processing file: class_8_356.csv\n",
            "Processing file: class_4_357.csv\n",
            "Processing file: class_4_358.csv\n",
            "Processing file: class_2_359.csv\n",
            "Processing file: class_11_360.csv\n",
            "Processing file: class_3_361.csv\n",
            "Processing file: class_0_362.csv\n",
            "Processing file: class_3_363.csv\n",
            "Processing file: class_8_364.csv\n",
            "Processing file: class_9_365.csv\n",
            "Processing file: class_11_366.csv\n",
            "Processing file: class_5_367.csv\n",
            "Processing file: class_5_368.csv\n",
            "Processing file: class_0_369.csv\n",
            "Processing file: class_6_370.csv\n",
            "Processing file: class_10_371.csv\n",
            "Processing file: class_5_372.csv\n",
            "Processing file: class_11_373.csv\n",
            "Processing file: class_9_374.csv\n",
            "Processing file: class_4_375.csv\n",
            "Processing file: class_2_376.csv\n",
            "Processing file: class_8_377.csv\n",
            "Processing file: class_6_378.csv\n",
            "Processing file: class_9_379.csv\n",
            "Processing file: class_6_380.csv\n",
            "Processing file: class_0_381.csv\n",
            "Processing file: class_2_382.csv\n",
            "Processing file: class_6_383.csv\n",
            "Processing file: class_5_384.csv\n",
            "Processing file: class_2_385.csv\n",
            "Processing file: class_7_386.csv\n",
            "Processing file: class_5_387.csv\n",
            "Processing file: class_4_388.csv\n",
            "Processing file: class_10_389.csv\n",
            "Processing file: class_8_390.csv\n",
            "Processing file: class_8_391.csv\n",
            "Processing file: class_6_392.csv\n",
            "Processing file: class_5_393.csv\n",
            "Processing file: class_7_394.csv\n",
            "Processing file: class_11_395.csv\n",
            "Processing file: class_2_396.csv\n",
            "Processing file: class_2_397.csv\n",
            "Processing file: class_6_398.csv\n",
            "Processing file: class_3_399.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "day1_df.to_csv(\"/content/drive/MyDrive/scattering_coeff_day10.csv\")"
      ],
      "metadata": {
        "id": "V_aR_4PIrNGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "days_10_scatter = pd.read_csv(\"/content/drive/MyDrive/scattering_coeff_day10.csv\")"
      ],
      "metadata": {
        "id": "XpdV6QrlrW04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrft_yUCrdxQ"
      },
      "outputs": [],
      "source": [
        "x= days_10_scatter.iloc[:,:-1].copy()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "#class_labels = ['class0', 'class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'class10', 'class11']\n",
        "#label_encoder = LabelEncoder()\n",
        "#label_encoder.fit(class_labels)\n",
        "\n",
        "# Convert class labels to numeric levels using transform method of LabelEncoder\n",
        "#labels = label_encoder.transform(data_4days['Class'].values) #change\n",
        "\n",
        "y = days_10_scatter.iloc[:,-1:].values\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(x)\n",
        "x = sc.transform(x)\n",
        "x = pd.DataFrame(x)\n",
        "x.columns = days_10_scatter.iloc[:,:-1].columns.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2a089d-5675-49d7-832e-d30e0eded1d8",
        "id": "AVNLn2KMrdxU"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1680, 1265), (1680, 1), (720, 1265), (720, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTBXI96zrdxV"
      },
      "outputs": [],
      "source": [
        "#normalize the data\n",
        "X_train = TimeSeriesScalerMinMax().fit_transform(X_train)\n",
        "X_test = TimeSeriesScalerMinMax().fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWHOc6b7rdxV"
      },
      "outputs": [],
      "source": [
        "# Convert the data to torch tensors\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "y_test = torch.from_numpy(y_test).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9056a92d-834a-44ad-8376-26a06b16c47d",
        "id": "KhUrCyJErdxW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in y_train: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "Unique values in y_test: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
          ]
        }
      ],
      "source": [
        "print(\"Unique values in y_train:\", torch.unique(y_train))\n",
        "print(\"Unique values in y_test:\", torch.unique(y_test))\n",
        "\n",
        "#start class from 0\n",
        "#y_train = y_train - 1\n",
        "#y_test = y_test - 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNbkM6tTrdxW"
      },
      "outputs": [],
      "source": [
        "#Datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "#Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae97584-0b43-4445-e191-bd769b6be009",
        "id": "3u5TaRMNrdxW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in y_train: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "Unique values in y_test: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
          ]
        }
      ],
      "source": [
        "print(\"Unique values in y_train:\", torch.unique(y_train))\n",
        "print(\"Unique values in y_test:\", torch.unique(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rakQ-yeQrdxX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382b22c0-306b-4d3d-ac8d-ba09b94a43d2",
        "id": "1m3ji4VprdxX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2IygWZvrdxX"
      },
      "outputs": [],
      "source": [
        "# model 1: CNN + LSTM\n",
        "# model 2: LSTM + CNN\n",
        "# model 3: CNN LSTM parallel\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #cnn takes input of shape (batch_size, channels, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        out = self.cnn(x)\n",
        "        # lstm takes input of shape (batch_size, seq_len, input_size)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out, _ = self.lstm(out)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_9WLX8rrdxY"
      },
      "outputs": [],
      "source": [
        "class LSTM_CNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM_CNN, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=hidden_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            #flatten\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.cnn(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6A9L7gHrdxY"
      },
      "outputs": [],
      "source": [
        "# model 3: CNN LSTM parallel\n",
        "class ParallelCNNLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(ParallelCNNLSTMModel, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(out_features=128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc_lstm = nn.Linear(hidden_size, 128)\n",
        "        self.fc = nn.Linear(128*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #cnn takes input of shape (batch_size, channels, seq_len)\n",
        "        x_cnn = x.permute(0, 2, 1)\n",
        "        out_cnn = self.cnn(x_cnn)\n",
        "        # lstm takes input of shape (batch_size, seq_len, input_size)\n",
        "        out_lstm, _ = self.lstm(x)\n",
        "        out_lstm = self.fc_lstm(out_lstm[:, -1, :])\n",
        "        out = torch.cat([out_cnn, out_lstm], dim=1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcDTaZWSrdxZ"
      },
      "outputs": [],
      "source": [
        "input_size = X_train.shape[-1]\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "cnn_lstm = CNN_LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "lstm_cnn = LSTM_CNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "cnn_lstm_parallel = ParallelCNNLSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ceac9eb-d497-48c7-a288-0689088c34cb",
        "id": "fH5SCyIirdxZ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model:  CNN_LSTM\n",
            "Epoch [1/150], Step [10/27], Loss: 0.0410, Accuracy: 98.91%\n",
            "Epoch [1/150], Step [20/27], Loss: 0.1254, Accuracy: 95.78%\n",
            "Epoch [2/150], Step [10/27], Loss: 0.1363, Accuracy: 96.72%\n",
            "Epoch [2/150], Step [20/27], Loss: 0.0721, Accuracy: 97.03%\n",
            "Epoch [3/150], Step [10/27], Loss: 0.0702, Accuracy: 97.34%\n",
            "Epoch [3/150], Step [20/27], Loss: 0.0837, Accuracy: 97.50%\n",
            "Epoch [4/150], Step [10/27], Loss: 0.0531, Accuracy: 98.28%\n",
            "Epoch [4/150], Step [20/27], Loss: 0.0315, Accuracy: 99.06%\n",
            "Epoch [5/150], Step [10/27], Loss: 0.0190, Accuracy: 99.38%\n",
            "Epoch [5/150], Step [20/27], Loss: 0.0192, Accuracy: 99.84%\n",
            "Epoch [6/150], Step [10/27], Loss: 0.0125, Accuracy: 99.69%\n",
            "Epoch [6/150], Step [20/27], Loss: 0.0103, Accuracy: 100.00%\n",
            "Epoch [7/150], Step [10/27], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [7/150], Step [20/27], Loss: 0.0053, Accuracy: 100.00%\n",
            "Epoch [8/150], Step [10/27], Loss: 0.0032, Accuracy: 100.00%\n",
            "Epoch [8/150], Step [20/27], Loss: 0.0038, Accuracy: 100.00%\n",
            "Epoch [9/150], Step [10/27], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [9/150], Step [20/27], Loss: 0.0030, Accuracy: 100.00%\n",
            "Epoch [10/150], Step [10/27], Loss: 0.0024, Accuracy: 100.00%\n",
            "Epoch [10/150], Step [20/27], Loss: 0.0022, Accuracy: 100.00%\n",
            "Epoch [11/150], Step [10/27], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [11/150], Step [20/27], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [12/150], Step [10/27], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [12/150], Step [20/27], Loss: 0.0021, Accuracy: 100.00%\n",
            "Epoch [13/150], Step [10/27], Loss: 0.0020, Accuracy: 100.00%\n",
            "Epoch [13/150], Step [20/27], Loss: 0.0018, Accuracy: 100.00%\n",
            "Epoch [14/150], Step [10/27], Loss: 0.0019, Accuracy: 100.00%\n",
            "Epoch [14/150], Step [20/27], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [15/150], Step [10/27], Loss: 0.0016, Accuracy: 100.00%\n",
            "Epoch [15/150], Step [20/27], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [16/150], Step [10/27], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [16/150], Step [20/27], Loss: 0.0017, Accuracy: 100.00%\n",
            "Epoch [17/150], Step [10/27], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [17/150], Step [20/27], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [18/150], Step [10/27], Loss: 0.0013, Accuracy: 100.00%\n",
            "Epoch [18/150], Step [20/27], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [19/150], Step [10/27], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [19/150], Step [20/27], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [20/150], Step [10/27], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [20/150], Step [20/27], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [21/150], Step [10/27], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [21/150], Step [20/27], Loss: 0.0012, Accuracy: 100.00%\n",
            "Epoch [22/150], Step [10/27], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [22/150], Step [20/27], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [23/150], Step [10/27], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [23/150], Step [20/27], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [24/150], Step [10/27], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [24/150], Step [20/27], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [25/150], Step [10/27], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [25/150], Step [20/27], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [26/150], Step [10/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [26/150], Step [20/27], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [27/150], Step [10/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [27/150], Step [20/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [28/150], Step [10/27], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [28/150], Step [20/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [29/150], Step [10/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [29/150], Step [20/27], Loss: 0.0009, Accuracy: 100.00%\n",
            "Epoch [30/150], Step [10/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [30/150], Step [20/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [31/150], Step [10/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [31/150], Step [20/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [32/150], Step [10/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [32/150], Step [20/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [33/150], Step [10/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [33/150], Step [20/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [34/150], Step [10/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [34/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [35/150], Step [10/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [35/150], Step [20/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [36/150], Step [10/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [36/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [37/150], Step [10/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [37/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [38/150], Step [10/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [38/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [39/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [39/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [40/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [40/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [41/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [41/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [42/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [42/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [43/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [43/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [44/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [44/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [45/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [45/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [46/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [46/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [47/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [47/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [48/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [48/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [49/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [49/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [50/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [50/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [51/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [51/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [52/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [52/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [53/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [53/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [54/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [54/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [55/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [55/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [56/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [56/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [57/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [57/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [58/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [58/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [59/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [59/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [60/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [60/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [61/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [61/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [62/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [62/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [63/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [63/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [64/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [64/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [65/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [65/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [66/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [66/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [67/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [67/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [68/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [68/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [69/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [69/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [70/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [70/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [71/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [71/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [72/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [72/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [73/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [73/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [74/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [74/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [75/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [75/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [76/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [76/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [77/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [77/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [78/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [78/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [79/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [79/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [80/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [80/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [81/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [81/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [82/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [82/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [83/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [83/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [84/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [84/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [85/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [85/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [86/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [86/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [87/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [87/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [88/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [88/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [89/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [89/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [90/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [90/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [91/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [91/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [92/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [92/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [93/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [93/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [94/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [94/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [95/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [95/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [96/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [96/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [97/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [97/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [98/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [98/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [99/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [99/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [100/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [100/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [101/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [101/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [102/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [102/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [103/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [103/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [104/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [104/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [105/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [105/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [106/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [106/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [107/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [107/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [108/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [108/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [109/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [109/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [110/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [110/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [111/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [111/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [112/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [112/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [113/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [113/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [114/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [114/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [115/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [115/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [116/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [116/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [117/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [117/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [118/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [118/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [119/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [119/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [120/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [120/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [121/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [121/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [122/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [122/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [123/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [123/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [124/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [124/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [125/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [125/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [126/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [126/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [127/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [127/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [128/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [128/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [129/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [130/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [131/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [131/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [132/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [132/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [133/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [133/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [134/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [134/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [135/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [135/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [136/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [136/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [137/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [137/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [138/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [138/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [139/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [140/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [140/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [141/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [141/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [142/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [142/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [143/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [143/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [144/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [144/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [145/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [145/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [146/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [146/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [147/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [147/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [148/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [148/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [149/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [149/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [150/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [150/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Training completed for model:  CNN_LSTM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(models: List, train_loader: DataLoader, epochs: int):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss_history = {model.__class__.__name__: [] for model in models}\n",
        "    accuracy_history = {model.__class__.__name__: [] for model in models}\n",
        "\n",
        "    for model in models:\n",
        "        print(\"Training model: \", model.__class__.__name__)\n",
        "        model.train()\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            for i, (x, y) in enumerate(train_loader):\n",
        "                x = x.to(device)\n",
        "                y = y.squeeze().to(device)  # Remove the extra dimension from the target tensor\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x)\n",
        "                loss = criterion(y_pred, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Compute accuracy\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                correct_predictions += (predicted_labels == y).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    avg_loss = total_loss / 10\n",
        "                    accuracy = correct_predictions / total_samples * 100\n",
        "                    print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], '\n",
        "                          f'Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "                    total_loss = 0.0\n",
        "                    correct_predictions = 0\n",
        "                    total_samples = 0\n",
        "\n",
        "            # Store loss and accuracy values for plotting\n",
        "            loss_history[model.__class__.__name__].append(avg_loss)\n",
        "            accuracy_history[model.__class__.__name__].append(accuracy)\n",
        "\n",
        "        print(\"Training completed for model: \", model.__class__.__name__)\n",
        "\n",
        "    # Plot loss and accuracy for each model\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for model in models:\n",
        "        plt.subplot(1, 2, 1)  # Loss plot\n",
        "        plt.plot(range(1, epochs + 1), loss_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)  # Accuracy plot\n",
        "        plt.plot(range(1, epochs + 1), accuracy_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# train\n",
        "models = [cnn_lstm]#, cnn_lstm_parallel]\n",
        "num_epochs = 150\n",
        "train(models, train_loader, epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VtrgvOBXrdxZ",
        "outputId": "95778557-5254-42cc-9325-cd10c0828e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model:  ParallelCNNLSTMModel\n",
            "Epoch [1/150], Step [10/27], Loss: 2.1474, Accuracy: 28.91%\n",
            "Epoch [1/150], Step [20/27], Loss: 0.8817, Accuracy: 72.19%\n",
            "Epoch [2/150], Step [10/27], Loss: 0.5292, Accuracy: 79.84%\n",
            "Epoch [2/150], Step [20/27], Loss: 0.4319, Accuracy: 85.62%\n",
            "Epoch [3/150], Step [10/27], Loss: 0.3506, Accuracy: 88.59%\n",
            "Epoch [3/150], Step [20/27], Loss: 0.2859, Accuracy: 89.69%\n",
            "Epoch [4/150], Step [10/27], Loss: 0.2937, Accuracy: 89.22%\n",
            "Epoch [4/150], Step [20/27], Loss: 0.2830, Accuracy: 90.31%\n",
            "Epoch [5/150], Step [10/27], Loss: 0.2249, Accuracy: 91.56%\n",
            "Epoch [5/150], Step [20/27], Loss: 0.2519, Accuracy: 90.47%\n",
            "Epoch [6/150], Step [10/27], Loss: 0.1799, Accuracy: 94.06%\n",
            "Epoch [6/150], Step [20/27], Loss: 0.1731, Accuracy: 94.06%\n",
            "Epoch [7/150], Step [10/27], Loss: 0.1863, Accuracy: 92.81%\n",
            "Epoch [7/150], Step [20/27], Loss: 0.1806, Accuracy: 92.81%\n",
            "Epoch [8/150], Step [10/27], Loss: 0.2136, Accuracy: 93.12%\n",
            "Epoch [8/150], Step [20/27], Loss: 0.1593, Accuracy: 94.38%\n",
            "Epoch [9/150], Step [10/27], Loss: 0.1367, Accuracy: 94.84%\n",
            "Epoch [9/150], Step [20/27], Loss: 0.1728, Accuracy: 93.75%\n",
            "Epoch [10/150], Step [10/27], Loss: 0.1283, Accuracy: 94.69%\n",
            "Epoch [10/150], Step [20/27], Loss: 0.1141, Accuracy: 95.47%\n",
            "Epoch [11/150], Step [10/27], Loss: 0.1248, Accuracy: 95.31%\n",
            "Epoch [11/150], Step [20/27], Loss: 0.1343, Accuracy: 94.69%\n",
            "Epoch [12/150], Step [10/27], Loss: 0.0793, Accuracy: 97.81%\n",
            "Epoch [12/150], Step [20/27], Loss: 0.1257, Accuracy: 95.00%\n",
            "Epoch [13/150], Step [10/27], Loss: 0.1323, Accuracy: 95.94%\n",
            "Epoch [13/150], Step [20/27], Loss: 0.1325, Accuracy: 95.31%\n",
            "Epoch [14/150], Step [10/27], Loss: 0.0568, Accuracy: 97.50%\n",
            "Epoch [14/150], Step [20/27], Loss: 0.0991, Accuracy: 96.41%\n",
            "Epoch [15/150], Step [10/27], Loss: 0.0669, Accuracy: 97.81%\n",
            "Epoch [15/150], Step [20/27], Loss: 0.0659, Accuracy: 97.66%\n",
            "Epoch [16/150], Step [10/27], Loss: 0.0513, Accuracy: 98.44%\n",
            "Epoch [16/150], Step [20/27], Loss: 0.0677, Accuracy: 97.50%\n",
            "Epoch [17/150], Step [10/27], Loss: 0.0691, Accuracy: 97.34%\n",
            "Epoch [17/150], Step [20/27], Loss: 0.0466, Accuracy: 98.59%\n",
            "Epoch [18/150], Step [10/27], Loss: 0.0495, Accuracy: 98.28%\n",
            "Epoch [18/150], Step [20/27], Loss: 0.0386, Accuracy: 99.38%\n",
            "Epoch [19/150], Step [10/27], Loss: 0.0365, Accuracy: 98.91%\n",
            "Epoch [19/150], Step [20/27], Loss: 0.0332, Accuracy: 98.91%\n",
            "Epoch [20/150], Step [10/27], Loss: 0.0185, Accuracy: 99.84%\n",
            "Epoch [20/150], Step [20/27], Loss: 0.0219, Accuracy: 99.53%\n",
            "Epoch [21/150], Step [10/27], Loss: 0.0595, Accuracy: 97.97%\n",
            "Epoch [21/150], Step [20/27], Loss: 0.0695, Accuracy: 97.19%\n",
            "Epoch [22/150], Step [10/27], Loss: 0.0565, Accuracy: 97.97%\n",
            "Epoch [22/150], Step [20/27], Loss: 0.0652, Accuracy: 97.81%\n",
            "Epoch [23/150], Step [10/27], Loss: 0.1017, Accuracy: 96.56%\n",
            "Epoch [23/150], Step [20/27], Loss: 0.0879, Accuracy: 96.72%\n",
            "Epoch [24/150], Step [10/27], Loss: 0.0353, Accuracy: 98.75%\n",
            "Epoch [24/150], Step [20/27], Loss: 0.0347, Accuracy: 98.75%\n",
            "Epoch [25/150], Step [10/27], Loss: 0.0725, Accuracy: 97.19%\n",
            "Epoch [25/150], Step [20/27], Loss: 0.0459, Accuracy: 98.59%\n",
            "Epoch [26/150], Step [10/27], Loss: 0.0371, Accuracy: 98.91%\n",
            "Epoch [26/150], Step [20/27], Loss: 0.0214, Accuracy: 99.53%\n",
            "Epoch [27/150], Step [10/27], Loss: 0.0111, Accuracy: 99.84%\n",
            "Epoch [27/150], Step [20/27], Loss: 0.0087, Accuracy: 100.00%\n",
            "Epoch [28/150], Step [10/27], Loss: 0.0094, Accuracy: 99.69%\n",
            "Epoch [28/150], Step [20/27], Loss: 0.0098, Accuracy: 100.00%\n",
            "Epoch [29/150], Step [10/27], Loss: 0.0035, Accuracy: 100.00%\n",
            "Epoch [29/150], Step [20/27], Loss: 0.0057, Accuracy: 100.00%\n",
            "Epoch [30/150], Step [10/27], Loss: 0.0037, Accuracy: 100.00%\n",
            "Epoch [30/150], Step [20/27], Loss: 0.0083, Accuracy: 99.84%\n",
            "Epoch [31/150], Step [10/27], Loss: 0.0043, Accuracy: 100.00%\n",
            "Epoch [31/150], Step [20/27], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [32/150], Step [10/27], Loss: 0.0055, Accuracy: 99.84%\n",
            "Epoch [32/150], Step [20/27], Loss: 0.0057, Accuracy: 99.84%\n",
            "Epoch [33/150], Step [10/27], Loss: 0.0039, Accuracy: 100.00%\n",
            "Epoch [33/150], Step [20/27], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [34/150], Step [10/27], Loss: 0.0023, Accuracy: 100.00%\n",
            "Epoch [34/150], Step [20/27], Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch [35/150], Step [10/27], Loss: 0.0015, Accuracy: 100.00%\n",
            "Epoch [35/150], Step [20/27], Loss: 0.0011, Accuracy: 100.00%\n",
            "Epoch [36/150], Step [10/27], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [36/150], Step [20/27], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [37/150], Step [10/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [37/150], Step [20/27], Loss: 0.0010, Accuracy: 100.00%\n",
            "Epoch [38/150], Step [10/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [38/150], Step [20/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [39/150], Step [10/27], Loss: 0.0008, Accuracy: 100.00%\n",
            "Epoch [39/150], Step [20/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [40/150], Step [10/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [40/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [41/150], Step [10/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [41/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [42/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [42/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [43/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [43/150], Step [20/27], Loss: 0.0006, Accuracy: 100.00%\n",
            "Epoch [44/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [44/150], Step [20/27], Loss: 0.0007, Accuracy: 100.00%\n",
            "Epoch [45/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [45/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [46/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [46/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [47/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [47/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [48/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [48/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [49/150], Step [10/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [49/150], Step [20/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [50/150], Step [10/27], Loss: 0.0005, Accuracy: 100.00%\n",
            "Epoch [50/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [51/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [51/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [52/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [52/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [53/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [53/150], Step [20/27], Loss: 0.0004, Accuracy: 100.00%\n",
            "Epoch [54/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [54/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [55/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [55/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [56/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [56/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [57/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [57/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [58/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [58/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [59/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [59/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [60/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [60/150], Step [20/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [61/150], Step [10/27], Loss: 0.0003, Accuracy: 100.00%\n",
            "Epoch [61/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [62/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [62/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [63/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [63/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [64/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [64/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [65/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [65/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [66/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [66/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [67/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [67/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [68/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [68/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [69/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [69/150], Step [20/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [70/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [70/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [71/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [71/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [72/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [72/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [73/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [73/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [74/150], Step [10/27], Loss: 0.0002, Accuracy: 100.00%\n",
            "Epoch [74/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [75/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [75/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [76/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [76/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [77/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [77/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [78/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [78/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [79/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [79/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [80/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [80/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [81/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [81/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [82/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [82/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [83/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [83/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [84/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [84/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [85/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [85/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [86/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [86/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [87/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [87/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [88/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [88/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [89/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [89/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [90/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [90/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [91/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [91/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [92/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [92/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [93/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [93/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [94/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [94/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [95/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [95/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [96/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [96/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [97/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [97/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [98/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [98/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [99/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [99/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [100/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [100/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [101/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [101/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [102/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [102/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [103/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [103/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [104/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [104/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [105/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [105/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [106/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [106/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [107/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [107/150], Step [20/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [108/150], Step [10/27], Loss: 0.0001, Accuracy: 100.00%\n",
            "Epoch [108/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [109/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [109/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [110/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [110/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [111/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [111/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [112/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [112/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [113/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [113/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [114/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [114/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [115/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [115/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [116/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [116/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [117/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [117/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [118/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [118/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [119/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [119/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [120/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [120/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [121/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [121/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [122/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [122/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [123/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [123/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [124/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [124/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [125/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [125/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [126/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [126/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [127/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [127/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [128/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [128/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [129/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [129/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [130/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [130/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [131/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [131/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [132/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [132/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [133/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [133/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [134/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [134/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [135/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [135/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [136/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [136/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [137/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [137/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [138/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [138/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [139/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [139/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [140/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [140/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [141/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [141/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [142/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [142/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [143/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [143/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [144/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [144/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [145/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [145/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [146/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [146/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [147/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [147/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [148/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [148/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [149/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [149/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [150/150], Step [10/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch [150/150], Step [20/27], Loss: 0.0000, Accuracy: 100.00%\n",
            "Training completed for model:  ParallelCNNLSTMModel\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(models: List, train_loader: DataLoader, epochs: int):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss_history = {model.__class__.__name__: [] for model in models}\n",
        "    accuracy_history = {model.__class__.__name__: [] for model in models}\n",
        "\n",
        "    for model in models:\n",
        "        print(\"Training model: \", model.__class__.__name__)\n",
        "        model.train()\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            for i, (x, y) in enumerate(train_loader):\n",
        "                x = x.to(device)\n",
        "                y = y.squeeze().to(device)  # Remove the extra dimension from the target tensor\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x)\n",
        "                loss = criterion(y_pred, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Compute accuracy\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                correct_predictions += (predicted_labels == y).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    avg_loss = total_loss / 10\n",
        "                    accuracy = correct_predictions / total_samples * 100\n",
        "                    print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], '\n",
        "                          f'Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "                    total_loss = 0.0\n",
        "                    correct_predictions = 0\n",
        "                    total_samples = 0\n",
        "\n",
        "            # Store loss and accuracy values for plotting\n",
        "            loss_history[model.__class__.__name__].append(avg_loss)\n",
        "            accuracy_history[model.__class__.__name__].append(accuracy)\n",
        "\n",
        "        print(\"Training completed for model: \", model.__class__.__name__)\n",
        "\n",
        "    # Plot loss and accuracy for each model\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    for model in models:\n",
        "        plt.subplot(1, 2, 1)  # Loss plot\n",
        "        plt.plot(range(1, epochs + 1), loss_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)  # Accuracy plot\n",
        "        plt.plot(range(1, epochs + 1), accuracy_history[model.__class__.__name__], label=model.__class__.__name__)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# train\n",
        "models = [cnn_lstm_parallel]\n",
        "num_epochs = 150\n",
        "train(models, train_loader, epochs=num_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekQSO6xwrdxZ"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BchsHx0urdxa"
      },
      "outputs": [],
      "source": [
        "models = [cnn_lstm, cnn_lstm_parallel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "T54QBNl3rdxa",
        "outputId": "42600c17-a4e2-4163-8502-b005d003388b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing model:  CNN_LSTM\n",
            "Test Accuracy: 85.14%\n",
            "Testing model:  ParallelCNNLSTMModel\n",
            "Test Accuracy: 91.11%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def test(models: List, test_loader: DataLoader):\n",
        "    accuracy_history = {model.__class__.__name__: [] for model in models}\n",
        "\n",
        "    for model in models:\n",
        "        print(\"Testing model: \", model.__class__.__name__)\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation during inference\n",
        "            for x, y in test_loader:\n",
        "                x = x.to(device)\n",
        "                y = y.squeeze().to(device)  # Remove the extra dimension from the target tensor\n",
        "                y_pred = model(x)\n",
        "                _, predicted_labels = torch.max(y_pred, 1)\n",
        "                correct_predictions += (predicted_labels == y).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "\n",
        "        accuracy = correct_predictions / total_samples * 100\n",
        "        print(\"Test Accuracy: {:.2f}%\".format(accuracy))\n",
        "        accuracy_history[model.__class__.__name__].append(accuracy)\n",
        "\n",
        "    # Plot accuracy for each model\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    models_names = [model.__class__.__name__ for model in models]\n",
        "    x_points = np.arange(len(models_names))\n",
        "    plt.bar(x_points, [accuracy_history[model_name][0] for model_name in models_names])\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.xticks(x_points, models_names)\n",
        "    plt.show()\n",
        "\n",
        "# Test the models\n",
        "test(models, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BoUqFlkrdxa"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "model_filename = 'cnn_lstm_scatter.pkl'\n",
        "with open(model_filename, 'wb') as file:\n",
        "    pickle.dump(cnn_lstm, file)\n",
        "\n",
        "import pickle\n",
        "model_filename = 'cnn_lstm_parallel_scatter.pkl'\n",
        "with open(model_filename, 'wb') as file:\n",
        "    pickle.dump(cnn_lstm_parallel, file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}